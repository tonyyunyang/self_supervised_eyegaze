{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': {'test_set': 'Reading', 'test_mode': 'One_out', 'window_size': 900, 'overlap': 0.944, 'feat_dim': None, 'pretrain_model': None, 'finetune_model': None, 'batch_size': 128, 'freeze': False}, 'kdd_pretrain': {'epoch': 5, 'lr': 0.0001, 'optimizer': 'RAdam', 'weight_decay': None, 'harden': False}, 'kdd_finetune': {'epoch': 5, 'lr': 0.0001, 'optimizer': 'RAdam', 'weight_decay': None}, 'limu_pretrain': {'epoch': 10, 'lr': 0.001, 'optimizer': 'Adam', 'weight_decay': None, 'harden': False}, 'limu_finetune': {'epoch': 10, 'lr': 0.001, 'optimizer': 'Adam', 'weight_decay': None, 'classifier': 'gru'}, 'limu_mask': {'mask_ratio': 0.15, 'mask_alpha': 6, 'max_gram': 10, 'mask_prob': 0.8, 'replace_prob': 0.0}, 'kdd_model': {'d_hidden': 64, 'd_ff': 256, 'n_heads': 8, 'n_layers': 3, 'dropout': 0.1, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'projection': 'linear'}, 'limu_model': {'d_hidden': 24, 'd_ff': 72, 'n_heads': 4, 'n_layers': 4, 'emb_norm': False}, 'limu_classifier': {'gru_v1': {'rnn_layers': [2, 1], 'rnn_io': [[72, 20], [20, 10]], 'linear_io': [[10, 6]], 'activ': False, 'dropout': False}}}\n",
      "The step size of each sample is 15, this is determined via the overlap\n",
      "Class: Magazine -> Encoded Value: 0\n",
      "Class: Manga -> Encoded Value: 1\n",
      "Class: Newspaper -> Encoded Value: 2\n",
      "Class: Novel -> Encoded Value: 3\n",
      "Class: Scipaper -> Encoded Value: 4\n",
      "Class: Textbook -> Encoded Value: 5\n",
      "The number of classes is 6, the feat_dim is 2\n",
      "Pretrain samples amount: 85968\n",
      "Finetune training samples amount: 752\n",
      "Finetune validation samples amount: 323\n",
      "Final testing samples amount: 9671\n",
      "Label 5: 126 samples\n",
      "Label 3: 119 samples\n",
      "Label 1: 121 samples\n",
      "Label 0: 118 samples\n",
      "Label 2: 141 samples\n",
      "Label 4: 127 samples\n",
      "Model:\n",
      "TSTransformerEncoder(\n",
      "  (project_inp): Linear(in_features=2, out_features=64, bias=True)\n",
      "  (pos_enc): LearnablePositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Total number of parameters: 159874\n",
      "Trainable parameters: 159874\n",
      "=============================================================\n",
      "=====================Training via cuda===================\n",
      "=============================================================\n",
      "Epoch 1/700, Train Loss: 132530381.5075, Time: 25.841562271118164\n",
      "Epoch 2/700, Train Loss: 132107694.7523, Time: 26.40795350074768\n",
      "Epoch 3/700, Train Loss: 131448295.9390, Time: 26.227282762527466\n",
      "Epoch 4/700, Train Loss: 130466592.6253, Time: 26.844122648239136\n",
      "Epoch 5/700, Train Loss: 129187568.0268, Time: 26.17584753036499\n",
      "Epoch 6/700, Train Loss: 127610554.7471, Time: 26.44481348991394\n",
      "Epoch 7/700, Train Loss: 125801040.0923, Time: 26.403234720230103\n",
      "Epoch 8/700, Train Loss: 123760554.7143, Time: 26.58102774620056\n",
      "Epoch 9/700, Train Loss: 121505534.4515, Time: 26.478681087493896\n",
      "Epoch 10/700, Train Loss: 119099479.7990, Time: 26.408017873764038\n",
      "Epoch 11/700, Train Loss: 116581857.2447, Time: 26.178868532180786\n",
      "Epoch 12/700, Train Loss: 114004042.3391, Time: 26.217020273208618\n",
      "Epoch 13/700, Train Loss: 111304405.6907, Time: 26.98628306388855\n",
      "Epoch 14/700, Train Loss: 108449442.1262, Time: 26.305434703826904\n",
      "Epoch 15/700, Train Loss: 105483104.5658, Time: 26.483858108520508\n",
      "Epoch 16/700, Train Loss: 102438551.0426, Time: 26.50765037536621\n",
      "Epoch 17/700, Train Loss: 99278180.6991, Time: 26.424092769622803\n",
      "Epoch 18/700, Train Loss: 96088409.8835, Time: 26.60362148284912\n",
      "Epoch 19/700, Train Loss: 92896086.6853, Time: 26.379220962524414\n",
      "Epoch 20/700, Train Loss: 89755125.3988, Time: 26.56853413581848\n",
      "Epoch 21/700, Train Loss: 86413754.4850, Time: 26.21912384033203\n",
      "Epoch 22/700, Train Loss: 83017499.8072, Time: 26.276600122451782\n",
      "Epoch 23/700, Train Loss: 79742950.6883, Time: 26.336795330047607\n",
      "Epoch 24/700, Train Loss: 76377374.3979, Time: 26.047884941101074\n",
      "Epoch 25/700, Train Loss: 73067116.4891, Time: 26.17555832862854\n",
      "Epoch 26/700, Train Loss: 69740367.9375, Time: 26.248087406158447\n",
      "Epoch 27/700, Train Loss: 66372223.4997, Time: 26.66615104675293\n",
      "Epoch 28/700, Train Loss: 63068363.1729, Time: 25.991835117340088\n",
      "Epoch 29/700, Train Loss: 59709950.8029, Time: 26.097126245498657\n",
      "Epoch 30/700, Train Loss: 56422596.8360, Time: 26.366908073425293\n",
      "Epoch 31/700, Train Loss: 53224893.5701, Time: 26.260289192199707\n",
      "Epoch 32/700, Train Loss: 50034472.1772, Time: 26.488155603408813\n",
      "Epoch 33/700, Train Loss: 46863535.0739, Time: 26.315645456314087\n",
      "Epoch 34/700, Train Loss: 43651302.3845, Time: 26.41163158416748\n",
      "Epoch 35/700, Train Loss: 40704347.2414, Time: 26.291146755218506\n",
      "Epoch 36/700, Train Loss: 37807933.6832, Time: 26.879326820373535\n",
      "Epoch 37/700, Train Loss: 34976460.3670, Time: 26.54928207397461\n",
      "Epoch 38/700, Train Loss: 32085855.7231, Time: 26.58450698852539\n",
      "Epoch 39/700, Train Loss: 29417754.4642, Time: 26.351800680160522\n",
      "Epoch 40/700, Train Loss: 26754302.8744, Time: 26.467047691345215\n",
      "Epoch 41/700, Train Loss: 24280085.8128, Time: 26.968121767044067\n",
      "Epoch 42/700, Train Loss: 21490800.1831, Time: 26.34375548362732\n",
      "Epoch 43/700, Train Loss: 19027276.6693, Time: 26.26833415031433\n",
      "Epoch 44/700, Train Loss: 16603793.0973, Time: 26.075825929641724\n",
      "Epoch 45/700, Train Loss: 14473711.8958, Time: 26.567500591278076\n",
      "Epoch 46/700, Train Loss: 12540919.9122, Time: 26.71298313140869\n",
      "Epoch 47/700, Train Loss: 10805856.4928, Time: 26.351264238357544\n",
      "Epoch 48/700, Train Loss: 9281912.5494, Time: 26.4592547416687\n",
      "Epoch 49/700, Train Loss: 7948638.9101, Time: 26.600107192993164\n",
      "Epoch 50/700, Train Loss: 6844786.9875, Time: 26.61977243423462\n",
      "Epoch 51/700, Train Loss: 5895495.9605, Time: 26.340387105941772\n",
      "Epoch 52/700, Train Loss: 5120616.5494, Time: 25.907485961914062\n",
      "Epoch 53/700, Train Loss: 4529859.5135, Time: 26.191916465759277\n",
      "Epoch 54/700, Train Loss: 3837078.6931, Time: 26.54004740715027\n",
      "Epoch 55/700, Train Loss: 2856009.4662, Time: 26.783867359161377\n",
      "Epoch 56/700, Train Loss: 2209624.3452, Time: 26.560805559158325\n",
      "Epoch 57/700, Train Loss: 1772919.8141, Time: 26.543413162231445\n",
      "Epoch 58/700, Train Loss: 1460612.8548, Time: 26.707220315933228\n",
      "Epoch 59/700, Train Loss: 1238313.1346, Time: 26.732269048690796\n",
      "Epoch 60/700, Train Loss: 1077579.4356, Time: 26.323887825012207\n",
      "Epoch 61/700, Train Loss: 958131.8879, Time: 26.291164875030518\n",
      "Epoch 62/700, Train Loss: 862536.9625, Time: 26.372347354888916\n",
      "Epoch 63/700, Train Loss: 783985.2151, Time: 26.46374225616455\n",
      "Epoch 64/700, Train Loss: 724338.8615, Time: 27.06738519668579\n",
      "Epoch 65/700, Train Loss: 676382.6816, Time: 26.690699815750122\n",
      "Epoch 66/700, Train Loss: 635641.0424, Time: 26.329481840133667\n",
      "Epoch 67/700, Train Loss: 605443.3942, Time: 26.691560745239258\n",
      "Epoch 68/700, Train Loss: 579278.9602, Time: 26.699807167053223\n",
      "Epoch 69/700, Train Loss: 558358.1994, Time: 26.056997776031494\n",
      "Epoch 70/700, Train Loss: 540597.4314, Time: 26.37092113494873\n",
      "Epoch 71/700, Train Loss: 524061.3269, Time: 26.427823543548584\n",
      "Epoch 72/700, Train Loss: 513588.2841, Time: 26.5122652053833\n",
      "Epoch 73/700, Train Loss: 499052.3080, Time: 26.509531497955322\n",
      "Epoch 74/700, Train Loss: 491872.8656, Time: 26.26231551170349\n",
      "Epoch 75/700, Train Loss: 485398.5107, Time: 26.071608066558838\n",
      "Epoch 76/700, Train Loss: 475590.4143, Time: 26.348851919174194\n",
      "Epoch 77/700, Train Loss: 468509.9958, Time: 26.70751166343689\n",
      "Epoch 78/700, Train Loss: 463899.6765, Time: 26.147732973098755\n",
      "Epoch 79/700, Train Loss: 463372.0858, Time: 26.395907878875732\n",
      "Epoch 80/700, Train Loss: 454713.3476, Time: 26.101872205734253\n",
      "Epoch 81/700, Train Loss: 451996.5402, Time: 26.58560609817505\n",
      "Epoch 82/700, Train Loss: 449874.2433, Time: 26.684152603149414\n",
      "Epoch 83/700, Train Loss: 444259.9903, Time: 26.536055088043213\n",
      "Epoch 84/700, Train Loss: 439089.9039, Time: 26.403154373168945\n",
      "Epoch 85/700, Train Loss: 439800.3344, Time: 26.39080023765564\n",
      "Epoch 86/700, Train Loss: 437397.6943, Time: 26.368891954421997\n",
      "Epoch 87/700, Train Loss: 435595.3815, Time: 26.144129276275635\n",
      "Epoch 88/700, Train Loss: 435796.6267, Time: 26.571659088134766\n",
      "Epoch 89/700, Train Loss: 429988.3814, Time: 26.319239139556885\n",
      "Epoch 90/700, Train Loss: 429809.1354, Time: 26.252261638641357\n",
      "Epoch 91/700, Train Loss: 428392.3347, Time: 26.379759311676025\n",
      "Epoch 92/700, Train Loss: 424382.1623, Time: 26.147549152374268\n",
      "Epoch 93/700, Train Loss: 421625.6283, Time: 26.19238257408142\n",
      "Epoch 94/700, Train Loss: 420852.7578, Time: 25.97131061553955\n",
      "Epoch 95/700, Train Loss: 420826.3740, Time: 26.225041389465332\n",
      "Epoch 96/700, Train Loss: 419969.6665, Time: 26.488587141036987\n",
      "Epoch 97/700, Train Loss: 417297.0352, Time: 26.43365716934204\n",
      "Epoch 98/700, Train Loss: 416942.4039, Time: 26.073290824890137\n",
      "Epoch 99/700, Train Loss: 415464.4383, Time: 26.295363664627075\n",
      "Epoch 100/700, Train Loss: 416163.1010, Time: 26.82815933227539\n",
      "Epoch 101/700, Train Loss: 412025.6370, Time: 26.3441219329834\n",
      "Epoch 102/700, Train Loss: 413209.4110, Time: 26.70670461654663\n",
      "Epoch 103/700, Train Loss: 414078.2838, Time: 26.54114556312561\n",
      "Epoch 104/700, Train Loss: 410523.2233, Time: 26.358698844909668\n",
      "Epoch 105/700, Train Loss: 407936.2084, Time: 26.668389081954956\n",
      "Epoch 106/700, Train Loss: 406653.9182, Time: 26.51572060585022\n",
      "Epoch 107/700, Train Loss: 407755.7064, Time: 26.475921392440796\n",
      "Epoch 108/700, Train Loss: 410416.9356, Time: 26.50745153427124\n",
      "Epoch 109/700, Train Loss: 403747.4706, Time: 26.660639762878418\n",
      "Epoch 110/700, Train Loss: 404007.6242, Time: 26.247538328170776\n",
      "Epoch 111/700, Train Loss: 400264.9014, Time: 26.371845245361328\n",
      "Epoch 112/700, Train Loss: 401017.8762, Time: 26.43597102165222\n",
      "Epoch 113/700, Train Loss: 403351.4896, Time: 26.567482709884644\n",
      "Epoch 114/700, Train Loss: 402944.2982, Time: 26.895014762878418\n",
      "Epoch 115/700, Train Loss: 403117.7396, Time: 26.500591278076172\n",
      "Epoch 116/700, Train Loss: 397843.6708, Time: 26.324549436569214\n",
      "Epoch 117/700, Train Loss: 397503.0382, Time: 26.383495330810547\n",
      "Epoch 118/700, Train Loss: 396029.6077, Time: 26.383495330810547\n",
      "Epoch 119/700, Train Loss: 395627.4731, Time: 26.839732885360718\n",
      "Epoch 120/700, Train Loss: 394279.1217, Time: 26.336885690689087\n",
      "Epoch 121/700, Train Loss: 394040.1424, Time: 26.40310549736023\n",
      "Epoch 122/700, Train Loss: 395137.4879, Time: 26.450578212738037\n",
      "Epoch 123/700, Train Loss: 395005.9600, Time: 26.773086309432983\n",
      "Epoch 124/700, Train Loss: 394265.6900, Time: 26.768074989318848\n",
      "Epoch 125/700, Train Loss: 394367.0520, Time: 26.424067974090576\n",
      "Epoch 126/700, Train Loss: 390277.5964, Time: 26.44322657585144\n",
      "Epoch 127/700, Train Loss: 392706.6907, Time: 26.196418523788452\n",
      "Epoch 128/700, Train Loss: 390318.7005, Time: 26.91671872138977\n",
      "Epoch 129/700, Train Loss: 389613.0817, Time: 26.45068883895874\n",
      "Epoch 130/700, Train Loss: 389882.4876, Time: 26.283792972564697\n",
      "Epoch 131/700, Train Loss: 391500.0509, Time: 26.436509370803833\n",
      "Epoch 132/700, Train Loss: 390254.2731, Time: 26.63962483406067\n",
      "Epoch 133/700, Train Loss: 388561.3603, Time: 26.618788957595825\n",
      "Epoch 134/700, Train Loss: 389360.7212, Time: 26.40893816947937\n",
      "Epoch 135/700, Train Loss: 386362.6515, Time: 26.18795108795166\n",
      "Epoch 136/700, Train Loss: 388698.0656, Time: 26.41082453727722\n",
      "Epoch 137/700, Train Loss: 386195.1537, Time: 26.852237939834595\n",
      "Epoch 138/700, Train Loss: 385831.2372, Time: 26.20024609565735\n",
      "Epoch 139/700, Train Loss: 387113.2086, Time: 26.152199745178223\n",
      "Epoch 140/700, Train Loss: 385722.7859, Time: 26.41577935218811\n",
      "Epoch 141/700, Train Loss: 387403.4542, Time: 26.432328701019287\n",
      "Epoch 142/700, Train Loss: 385033.3235, Time: 26.523390293121338\n",
      "Epoch 143/700, Train Loss: 381797.7036, Time: 26.31761074066162\n",
      "Epoch 144/700, Train Loss: 386193.5925, Time: 26.465678453445435\n",
      "Epoch 145/700, Train Loss: 383236.1323, Time: 26.036157608032227\n",
      "Epoch 146/700, Train Loss: 382036.7140, Time: 26.55062222480774\n",
      "Epoch 147/700, Train Loss: 382864.1157, Time: 26.56501841545105\n",
      "Epoch 148/700, Train Loss: 381312.1846, Time: 26.424460887908936\n",
      "Epoch 149/700, Train Loss: 381019.4790, Time: 26.639789819717407\n",
      "Epoch 150/700, Train Loss: 379169.0802, Time: 26.403876781463623\n",
      "Epoch 151/700, Train Loss: 382643.9582, Time: 26.30302333831787\n",
      "Epoch 152/700, Train Loss: 381242.7483, Time: 26.524635553359985\n",
      "Epoch 153/700, Train Loss: 380568.0378, Time: 26.32412815093994\n",
      "Epoch 154/700, Train Loss: 377972.1850, Time: 26.367960453033447\n",
      "Epoch 155/700, Train Loss: 379713.6729, Time: 26.76713728904724\n",
      "Epoch 156/700, Train Loss: 378264.4449, Time: 26.416500568389893\n",
      "Epoch 157/700, Train Loss: 379854.3208, Time: 26.479736804962158\n",
      "Epoch 158/700, Train Loss: 377679.8202, Time: 26.51150393486023\n",
      "Epoch 159/700, Train Loss: 379136.5073, Time: 26.409690618515015\n",
      "Epoch 160/700, Train Loss: 378537.6961, Time: 26.39026975631714\n",
      "Epoch 161/700, Train Loss: 381290.2539, Time: 25.919039964675903\n",
      "Epoch 162/700, Train Loss: 378640.4475, Time: 25.595229387283325\n",
      "Epoch 163/700, Train Loss: 376556.1966, Time: 26.141408443450928\n",
      "Epoch 164/700, Train Loss: 377416.8014, Time: 26.175581693649292\n",
      "Epoch 165/700, Train Loss: 378613.3518, Time: 25.978946447372437\n",
      "Epoch 166/700, Train Loss: 375237.7334, Time: 25.804681062698364\n",
      "Epoch 167/700, Train Loss: 375407.6529, Time: 26.27924108505249\n",
      "Epoch 168/700, Train Loss: 374292.1177, Time: 25.752412796020508\n",
      "Epoch 169/700, Train Loss: 376317.2298, Time: 26.052322149276733\n",
      "Epoch 170/700, Train Loss: 375142.1595, Time: 26.83156943321228\n",
      "Epoch 171/700, Train Loss: 372913.9828, Time: 26.600446224212646\n",
      "Epoch 172/700, Train Loss: 377084.5749, Time: 26.799323081970215\n",
      "Epoch 173/700, Train Loss: 374222.7973, Time: 26.598469734191895\n",
      "Epoch 174/700, Train Loss: 374875.1458, Time: 26.625596046447754\n",
      "Epoch 175/700, Train Loss: 374934.5668, Time: 26.464329957962036\n",
      "Epoch 176/700, Train Loss: 372933.7340, Time: 26.063870191574097\n",
      "Epoch 177/700, Train Loss: 371177.7904, Time: 26.65075421333313\n",
      "Epoch 178/700, Train Loss: 375335.1003, Time: 26.660779237747192\n",
      "Epoch 179/700, Train Loss: 372964.8055, Time: 26.303727865219116\n",
      "Epoch 180/700, Train Loss: 372535.8030, Time: 26.372596740722656\n",
      "Epoch 181/700, Train Loss: 372042.5002, Time: 26.515948057174683\n",
      "Epoch 182/700, Train Loss: 370855.3027, Time: 26.411444902420044\n",
      "Epoch 183/700, Train Loss: 374182.6301, Time: 26.327932596206665\n",
      "Epoch 184/700, Train Loss: 371187.5041, Time: 26.439762830734253\n",
      "Epoch 185/700, Train Loss: 374722.2935, Time: 26.291123390197754\n",
      "Epoch 186/700, Train Loss: 371795.2783, Time: 26.15229558944702\n",
      "Epoch 187/700, Train Loss: 370258.9868, Time: 26.5722815990448\n",
      "Epoch 188/700, Train Loss: 370746.7920, Time: 26.660162448883057\n",
      "Epoch 189/700, Train Loss: 371085.3103, Time: 26.090545415878296\n",
      "Epoch 190/700, Train Loss: 370407.3546, Time: 26.32075524330139\n",
      "Epoch 191/700, Train Loss: 372035.3356, Time: 26.424061059951782\n",
      "Epoch 192/700, Train Loss: 369574.8585, Time: 26.96005129814148\n",
      "Epoch 193/700, Train Loss: 369955.0458, Time: 26.624468564987183\n",
      "Epoch 194/700, Train Loss: 369224.1593, Time: 26.13571810722351\n",
      "Epoch 195/700, Train Loss: 369719.8060, Time: 26.47883939743042\n",
      "Epoch 196/700, Train Loss: 372106.1981, Time: 26.780776023864746\n",
      "Epoch 197/700, Train Loss: 370501.9383, Time: 26.426724672317505\n",
      "Epoch 198/700, Train Loss: 369989.0748, Time: 26.252978086471558\n",
      "Epoch 199/700, Train Loss: 368137.4577, Time: 25.998814344406128\n",
      "Epoch 200/700, Train Loss: 369918.2382, Time: 26.356411933898926\n",
      "Epoch 201/700, Train Loss: 368098.7295, Time: 26.580395936965942\n",
      "Epoch 202/700, Train Loss: 367435.1164, Time: 26.47567892074585\n",
      "Epoch 203/700, Train Loss: 367284.7765, Time: 26.299683332443237\n",
      "Epoch 204/700, Train Loss: 367499.7911, Time: 26.65583896636963\n",
      "Epoch 205/700, Train Loss: 367185.0405, Time: 26.443881511688232\n",
      "Epoch 206/700, Train Loss: 364386.7902, Time: 26.491963148117065\n",
      "Epoch 207/700, Train Loss: 367833.9499, Time: 26.294373273849487\n",
      "Epoch 208/700, Train Loss: 367198.4197, Time: 26.20941138267517\n",
      "Epoch 209/700, Train Loss: 367596.5513, Time: 26.159841775894165\n",
      "Epoch 210/700, Train Loss: 369578.1483, Time: 26.515799522399902\n",
      "Epoch 211/700, Train Loss: 369054.2147, Time: 26.448530673980713\n",
      "Epoch 212/700, Train Loss: 365593.4298, Time: 26.25602126121521\n",
      "Epoch 213/700, Train Loss: 367801.2289, Time: 26.327048301696777\n",
      "Epoch 214/700, Train Loss: 366927.0755, Time: 26.576172590255737\n",
      "Epoch 215/700, Train Loss: 367026.3785, Time: 26.20320177078247\n",
      "Epoch 216/700, Train Loss: 368259.7711, Time: 26.048451900482178\n",
      "Epoch 217/700, Train Loss: 365815.4075, Time: 26.111469984054565\n",
      "Epoch 218/700, Train Loss: 367056.4782, Time: 26.220160007476807\n",
      "Epoch 219/700, Train Loss: 365344.2270, Time: 26.68719792366028\n",
      "Epoch 220/700, Train Loss: 365879.5166, Time: 26.345655918121338\n",
      "Epoch 221/700, Train Loss: 365683.5401, Time: 26.39491081237793\n",
      "Epoch 222/700, Train Loss: 366174.1781, Time: 26.636047840118408\n",
      "Epoch 223/700, Train Loss: 364903.1352, Time: 26.64016342163086\n",
      "Epoch 224/700, Train Loss: 365418.5566, Time: 26.88381814956665\n",
      "Epoch 225/700, Train Loss: 365526.7998, Time: 26.599841594696045\n",
      "Epoch 226/700, Train Loss: 364204.3464, Time: 26.642885446548462\n",
      "Epoch 227/700, Train Loss: 364418.5447, Time: 26.18809747695923\n",
      "Epoch 228/700, Train Loss: 363130.3314, Time: 26.545093059539795\n",
      "Epoch 229/700, Train Loss: 365686.6696, Time: 26.354900121688843\n",
      "Epoch 230/700, Train Loss: 363064.3458, Time: 26.259352684020996\n",
      "Epoch 231/700, Train Loss: 361646.3488, Time: 26.66485571861267\n",
      "Epoch 232/700, Train Loss: 362744.2809, Time: 26.135849237442017\n",
      "Epoch 233/700, Train Loss: 363181.3857, Time: 26.38748526573181\n",
      "Epoch 234/700, Train Loss: 361631.9312, Time: 26.347946643829346\n",
      "Epoch 235/700, Train Loss: 363004.7864, Time: 26.403833150863647\n",
      "Epoch 236/700, Train Loss: 363108.2175, Time: 26.235796213150024\n",
      "Epoch 237/700, Train Loss: 360608.3569, Time: 26.522645235061646\n",
      "Epoch 238/700, Train Loss: 361920.7768, Time: 26.440953969955444\n",
      "Epoch 239/700, Train Loss: 361943.1690, Time: 26.379656553268433\n",
      "Epoch 240/700, Train Loss: 363316.6723, Time: 26.243189573287964\n",
      "Epoch 241/700, Train Loss: 361070.4466, Time: 26.45689082145691\n",
      "Epoch 242/700, Train Loss: 362207.8170, Time: 26.535115242004395\n",
      "Epoch 243/700, Train Loss: 362005.5634, Time: 26.008757829666138\n",
      "Epoch 244/700, Train Loss: 362936.0521, Time: 26.41525363922119\n",
      "Epoch 245/700, Train Loss: 360301.2490, Time: 26.580854415893555\n",
      "Epoch 246/700, Train Loss: 361702.9964, Time: 26.587365865707397\n",
      "Epoch 247/700, Train Loss: 358191.4776, Time: 26.57176399230957\n",
      "Epoch 248/700, Train Loss: 359195.0506, Time: 26.544058561325073\n",
      "Epoch 249/700, Train Loss: 359830.7049, Time: 26.784090518951416\n",
      "Epoch 250/700, Train Loss: 359013.5010, Time: 26.315887689590454\n",
      "Epoch 251/700, Train Loss: 361450.5038, Time: 26.579562664031982\n",
      "Epoch 252/700, Train Loss: 357532.1595, Time: 26.30379056930542\n",
      "Epoch 253/700, Train Loss: 358133.2233, Time: 26.483452320098877\n",
      "Epoch 254/700, Train Loss: 357117.0390, Time: 26.420239448547363\n",
      "Epoch 255/700, Train Loss: 358822.4128, Time: 27.104073762893677\n",
      "Epoch 256/700, Train Loss: 358239.0912, Time: 26.327511310577393\n",
      "Epoch 257/700, Train Loss: 355806.1831, Time: 26.575539350509644\n",
      "Epoch 258/700, Train Loss: 356535.2392, Time: 25.93315076828003\n",
      "Epoch 259/700, Train Loss: 357112.8794, Time: 26.407190322875977\n",
      "Epoch 260/700, Train Loss: 355744.4761, Time: 26.76784038543701\n",
      "Epoch 261/700, Train Loss: 358018.4346, Time: 26.50228524208069\n",
      "Epoch 262/700, Train Loss: 355581.9876, Time: 26.381312131881714\n",
      "Epoch 263/700, Train Loss: 355363.1020, Time: 26.347895622253418\n",
      "Epoch 264/700, Train Loss: 355918.1856, Time: 26.088302850723267\n",
      "Epoch 265/700, Train Loss: 357271.7681, Time: 26.66351079940796\n",
      "Epoch 266/700, Train Loss: 356862.7889, Time: 26.24387502670288\n",
      "Epoch 267/700, Train Loss: 355230.3074, Time: 26.31596612930298\n",
      "Epoch 268/700, Train Loss: 355793.2588, Time: 26.84013867378235\n",
      "Epoch 269/700, Train Loss: 355498.4666, Time: 26.67190170288086\n",
      "Epoch 270/700, Train Loss: 354180.7396, Time: 26.201968908309937\n",
      "Epoch 271/700, Train Loss: 354120.0449, Time: 26.173146963119507\n",
      "Epoch 272/700, Train Loss: 353635.6276, Time: 26.21627712249756\n",
      "Epoch 273/700, Train Loss: 353672.4094, Time: 26.32064461708069\n",
      "Epoch 274/700, Train Loss: 357120.4788, Time: 26.53432536125183\n",
      "Epoch 275/700, Train Loss: 352896.8276, Time: 26.264994621276855\n",
      "Epoch 276/700, Train Loss: 354198.7584, Time: 26.611553192138672\n",
      "Epoch 277/700, Train Loss: 353024.4671, Time: 26.403887271881104\n",
      "Epoch 278/700, Train Loss: 353104.6298, Time: 26.40351963043213\n",
      "Epoch 279/700, Train Loss: 352873.9867, Time: 26.368049144744873\n",
      "Epoch 280/700, Train Loss: 354948.7345, Time: 26.21272349357605\n",
      "Epoch 281/700, Train Loss: 353933.4975, Time: 26.559162139892578\n",
      "Epoch 282/700, Train Loss: 351481.4584, Time: 26.464290857315063\n",
      "Epoch 283/700, Train Loss: 354584.7457, Time: 26.598897218704224\n",
      "Epoch 284/700, Train Loss: 352330.7841, Time: 26.09958505630493\n",
      "Epoch 285/700, Train Loss: 353719.2171, Time: 26.42422103881836\n",
      "Epoch 286/700, Train Loss: 352645.8494, Time: 26.632431268692017\n",
      "Epoch 287/700, Train Loss: 353964.3463, Time: 27.320265531539917\n",
      "Epoch 288/700, Train Loss: 352337.6370, Time: 26.907472610473633\n",
      "Epoch 289/700, Train Loss: 353086.3232, Time: 26.547919988632202\n",
      "Epoch 290/700, Train Loss: 352645.3331, Time: 26.468173027038574\n",
      "Epoch 291/700, Train Loss: 351223.0975, Time: 26.395596981048584\n",
      "Epoch 292/700, Train Loss: 352197.6370, Time: 27.083813905715942\n",
      "Epoch 293/700, Train Loss: 350916.5804, Time: 26.355885982513428\n",
      "Epoch 294/700, Train Loss: 351822.9021, Time: 26.484415531158447\n",
      "Epoch 295/700, Train Loss: 351855.1630, Time: 26.379454851150513\n",
      "Epoch 296/700, Train Loss: 350769.9455, Time: 26.839820623397827\n",
      "Epoch 297/700, Train Loss: 350315.1971, Time: 26.47998332977295\n",
      "Epoch 298/700, Train Loss: 354893.9302, Time: 26.423328399658203\n",
      "Epoch 299/700, Train Loss: 351205.4062, Time: 26.34776473045349\n",
      "Epoch 300/700, Train Loss: 350384.3756, Time: 26.14463186264038\n",
      "Epoch 301/700, Train Loss: 351305.4785, Time: 27.099530935287476\n",
      "Epoch 302/700, Train Loss: 349688.2005, Time: 26.431955814361572\n",
      "Epoch 303/700, Train Loss: 351615.4901, Time: 26.280384063720703\n",
      "Epoch 304/700, Train Loss: 350693.0860, Time: 26.351324319839478\n",
      "Epoch 305/700, Train Loss: 350979.7681, Time: 26.259923696517944\n",
      "Epoch 306/700, Train Loss: 348730.3098, Time: 26.54775834083557\n",
      "Epoch 307/700, Train Loss: 350788.7446, Time: 26.319698810577393\n",
      "Epoch 308/700, Train Loss: 348549.6535, Time: 26.251537322998047\n",
      "Epoch 309/700, Train Loss: 351355.7228, Time: 26.156721115112305\n",
      "Epoch 310/700, Train Loss: 350039.8842, Time: 26.740119218826294\n",
      "Epoch 311/700, Train Loss: 349041.7570, Time: 26.19548225402832\n",
      "Epoch 312/700, Train Loss: 348599.0573, Time: 26.19974422454834\n",
      "Epoch 313/700, Train Loss: 348657.8237, Time: 26.57944917678833\n",
      "Epoch 314/700, Train Loss: 351158.1660, Time: 26.399893045425415\n",
      "Epoch 315/700, Train Loss: 348694.4421, Time: 26.95562243461609\n",
      "Epoch 316/700, Train Loss: 349314.0344, Time: 26.212632417678833\n",
      "Epoch 317/700, Train Loss: 348725.6454, Time: 26.423514127731323\n",
      "Epoch 318/700, Train Loss: 349124.4157, Time: 26.44042205810547\n",
      "Epoch 319/700, Train Loss: 348158.6026, Time: 26.51577591896057\n",
      "Epoch 320/700, Train Loss: 347907.2997, Time: 26.247910737991333\n",
      "Epoch 321/700, Train Loss: 348841.4044, Time: 26.319315195083618\n",
      "Epoch 322/700, Train Loss: 348161.6963, Time: 26.555310010910034\n",
      "Epoch 323/700, Train Loss: 349054.7412, Time: 26.124919176101685\n",
      "Epoch 324/700, Train Loss: 348600.1904, Time: 26.4880850315094\n",
      "Epoch 325/700, Train Loss: 348139.4196, Time: 26.19944930076599\n",
      "Epoch 326/700, Train Loss: 347219.1734, Time: 26.620333909988403\n",
      "Epoch 327/700, Train Loss: 347340.9685, Time: 26.410684823989868\n",
      "Epoch 328/700, Train Loss: 347591.7454, Time: 26.53675389289856\n",
      "Epoch 329/700, Train Loss: 347136.1598, Time: 26.355875968933105\n",
      "Epoch 330/700, Train Loss: 349266.7924, Time: 26.32431387901306\n",
      "Epoch 331/700, Train Loss: 345510.8274, Time: 26.271953582763672\n",
      "Epoch 332/700, Train Loss: 347634.3899, Time: 25.736098289489746\n",
      "Epoch 333/700, Train Loss: 355930.4934, Time: 26.215145111083984\n",
      "Epoch 334/700, Train Loss: 346200.8006, Time: 25.95559287071228\n",
      "Epoch 335/700, Train Loss: 347223.8789, Time: 26.095909595489502\n",
      "Epoch 336/700, Train Loss: 347782.7711, Time: 26.296367168426514\n",
      "Epoch 337/700, Train Loss: 347080.1731, Time: 26.352278232574463\n",
      "Epoch 338/700, Train Loss: 347173.0463, Time: 26.85089945793152\n",
      "Epoch 339/700, Train Loss: 346672.2879, Time: 25.828498363494873\n",
      "Epoch 340/700, Train Loss: 347018.7175, Time: 26.152307510375977\n",
      "Epoch 341/700, Train Loss: 345397.4687, Time: 26.202680826187134\n",
      "Epoch 342/700, Train Loss: 346134.9118, Time: 26.57163906097412\n",
      "Epoch 343/700, Train Loss: 346355.1168, Time: 26.228490352630615\n",
      "Epoch 344/700, Train Loss: 345501.4890, Time: 26.12360453605652\n",
      "Epoch 345/700, Train Loss: 346268.3880, Time: 26.287086486816406\n",
      "Epoch 346/700, Train Loss: 346181.4550, Time: 26.527847051620483\n",
      "Epoch 347/700, Train Loss: 346191.2635, Time: 26.70042395591736\n",
      "Epoch 348/700, Train Loss: 346339.0727, Time: 26.60322070121765\n",
      "Epoch 349/700, Train Loss: 345572.0067, Time: 26.34847617149353\n",
      "Epoch 350/700, Train Loss: 345282.4438, Time: 26.12372136116028\n",
      "Epoch 351/700, Train Loss: 345308.1031, Time: 26.66751766204834\n",
      "Epoch 352/700, Train Loss: 345465.7679, Time: 26.22443199157715\n",
      "Epoch 353/700, Train Loss: 345927.3564, Time: 25.886287689208984\n",
      "Epoch 354/700, Train Loss: 345002.9440, Time: 25.97769069671631\n",
      "Epoch 355/700, Train Loss: 344693.9258, Time: 26.37477970123291\n",
      "Epoch 356/700, Train Loss: 346892.9822, Time: 26.952431678771973\n",
      "Epoch 357/700, Train Loss: 345885.5281, Time: 26.295969486236572\n",
      "Epoch 358/700, Train Loss: 344387.7537, Time: 26.667499542236328\n",
      "Epoch 359/700, Train Loss: 344429.5270, Time: 26.184219360351562\n",
      "Epoch 360/700, Train Loss: 343938.2696, Time: 26.548336029052734\n",
      "Epoch 361/700, Train Loss: 346445.8574, Time: 26.10654091835022\n",
      "Epoch 362/700, Train Loss: 344217.0601, Time: 26.12782049179077\n",
      "Epoch 363/700, Train Loss: 344211.9992, Time: 26.42093062400818\n",
      "Epoch 364/700, Train Loss: 345632.3726, Time: 26.35475254058838\n",
      "Epoch 365/700, Train Loss: 344416.8681, Time: 26.740181922912598\n",
      "Epoch 366/700, Train Loss: 343918.2398, Time: 26.372077226638794\n",
      "Epoch 367/700, Train Loss: 344025.4896, Time: 26.136142253875732\n",
      "Epoch 368/700, Train Loss: 343530.5834, Time: 26.611419677734375\n",
      "Epoch 369/700, Train Loss: 343738.5466, Time: 26.067596912384033\n",
      "Epoch 370/700, Train Loss: 345145.1286, Time: 26.66875982284546\n",
      "Epoch 371/700, Train Loss: 345959.7218, Time: 26.211551427841187\n",
      "Epoch 372/700, Train Loss: 343711.6095, Time: 26.25544834136963\n",
      "Epoch 373/700, Train Loss: 342577.0541, Time: 26.36799454689026\n",
      "Epoch 374/700, Train Loss: 343301.8023, Time: 26.320323944091797\n",
      "Epoch 375/700, Train Loss: 344643.3468, Time: 26.459372758865356\n",
      "Epoch 376/700, Train Loss: 344435.9002, Time: 26.332183599472046\n",
      "Epoch 377/700, Train Loss: 343545.2004, Time: 26.208132028579712\n",
      "Epoch 378/700, Train Loss: 344602.4059, Time: 26.139363050460815\n",
      "Epoch 379/700, Train Loss: 341620.0507, Time: 26.444245100021362\n",
      "Epoch 380/700, Train Loss: 342954.7499, Time: 26.548259973526\n",
      "Epoch 381/700, Train Loss: 343323.0740, Time: 26.47208595275879\n",
      "Epoch 382/700, Train Loss: 342950.8512, Time: 26.45884370803833\n",
      "Epoch 383/700, Train Loss: 342506.5405, Time: 26.6755588054657\n",
      "Epoch 384/700, Train Loss: 343595.3075, Time: 26.98826313018799\n",
      "Epoch 385/700, Train Loss: 344906.0694, Time: 26.62831139564514\n",
      "Epoch 386/700, Train Loss: 343684.8344, Time: 26.171772718429565\n",
      "Epoch 387/700, Train Loss: 341147.7920, Time: 26.204759120941162\n",
      "Epoch 388/700, Train Loss: 343649.2276, Time: 26.695459842681885\n",
      "Epoch 389/700, Train Loss: 342235.1124, Time: 26.398108959197998\n",
      "Epoch 390/700, Train Loss: 342683.6396, Time: 26.20551872253418\n",
      "Epoch 391/700, Train Loss: 343400.8469, Time: 26.44841694831848\n",
      "Epoch 392/700, Train Loss: 342660.3887, Time: 26.611647129058838\n",
      "Epoch 393/700, Train Loss: 341799.0184, Time: 26.691697597503662\n",
      "Epoch 394/700, Train Loss: 342865.2663, Time: 26.291701555252075\n",
      "Epoch 395/700, Train Loss: 343245.5498, Time: 26.312830686569214\n",
      "Epoch 396/700, Train Loss: 343322.3745, Time: 26.40642523765564\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from modules.finetune_hyperparameters import KDD_Finetune_Hyperparameters\n",
    "from modules.kdd_model import kdd_model4pretrain, kdd_model4finetune\n",
    "from modules.pretrain_hyperparameters import KDD_Pretrain_Hyperparameters, KDD_NoMask_Pretrain_Hyperparameters\n",
    "from utils.finetune import finetune_kdd_model, eval_finetune_kdd_model\n",
    "from utils.load_data_from_file import load_mixed_data, prepare_mixed_data_loader, load_one_out_data, \\\n",
    "    prepare_one_out_data_loader, prepare_no_mask_one_out_data_loader\n",
    "from utils.pretrain import pretrain_kdd_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the config from JSON file first\n",
    "    with open(\"utils/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "    print(config)\n",
    "\n",
    "    # config[\"general\"][\"pretrain_model\"] = \"results/kdd_model/One_out/linear/pretrain/window_size_30sec/freeze_False_epoch_500_lr_0.0001_d_hidden_64_d_ff_256_n_heads_8_n_layer_1_pos_encode_learnable_activation_gelu_norm_BatchNorm\"\n",
    "\n",
    "    config[\"general\"][\"test_set\"] = \"Reading\" # Reading or Desktop\n",
    "\n",
    "    config[\"general\"][\"window_size\"] = 150\n",
    "    config[\"general\"][\"overlap\"] = 0.899\n",
    "    config[\"general\"][\"batch_size\"] = 128\n",
    "    config[\"kdd_pretrain\"][\"epoch\"] = 700\n",
    "    config[\"kdd_finetune\"][\"epoch\"] = 5\n",
    "\n",
    "    config[\"kdd_model\"][\"d_hidden\"] = 64\n",
    "    config[\"kdd_model\"][\"d_ff\"] = 256\n",
    "    config[\"kdd_model\"][\"n_heads\"] = 8\n",
    "    config[\"kdd_model\"][\"n_layers\"] = 3\n",
    "\n",
    "    # First load the data into dataloader according to chosen test_mode: Mixed or One_out\n",
    "    if config[\"general\"][\"test_mode\"] == \"Mixed\":\n",
    "        data, labels, encoder = load_mixed_data(window_size=config[\"general\"][\"window_size\"],\n",
    "                                                overlap=config[\"general\"][\"overlap\"],\n",
    "                                                data_set=config[\"general\"][\"test_set\"])\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        labels_dim = labels.shape\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}, the labels_dim is {labels_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_mixed_data_loader\n",
    "                               (data, labels, batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"]))\n",
    "\n",
    "    elif config[\"general\"][\"test_mode\"] == \"One_out\":\n",
    "        train_data, train_labels, test_data, test_labels, encoder = (load_one_out_data\n",
    "                                                                     (window_size=config[\"general\"][\"window_size\"],\n",
    "                                                                      overlap=config[\"general\"][\"overlap\"],\n",
    "                                                                      data_set=config[\"general\"][\"test_set\"]))\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = train_data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_no_mask_one_out_data_loader\n",
    "                               (train_data, train_labels, test_data, test_labels,\n",
    "                                batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"]))\n",
    "    else:\n",
    "        print(\"Either Mixed / One_out\")\n",
    "        sys.exit()\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # If the pretrain_model path is not provided, start with pretraining the model\n",
    "    if config[\"general\"][\"pretrain_model\"] is None:\n",
    "        hyperparameters = KDD_NoMask_Pretrain_Hyperparameters(config)\n",
    "        model = kdd_model4pretrain(config, feat_dim)\n",
    "        loss = hyperparameters.loss\n",
    "        optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                              weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "        pretrain_kdd_model(model, loss, optimizer, eyegaze_data_loader[0], config)\n",
    "\n",
    "    # If the pretrain_model path is provided, meaning that there is already a pretrained model, then directly finetune\n",
    "    # After pretrain, finetune will be performed automatically, because the pretrain_model will be filled\n",
    "    hyperparameters = KDD_Finetune_Hyperparameters(config)\n",
    "    model = kdd_model4finetune(config, feat_dim, num_classes)\n",
    "    loss = hyperparameters.loss\n",
    "    optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                          weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "    # eyegaze_data_loader[1] is the training set, and eyegaze_data_loader[2] is the validation set\n",
    "    finetune_kdd_model(model, loss, optimizer, eyegaze_data_loader[1], eyegaze_data_loader[2], config)\n",
    "\n",
    "    eval_finetune_kdd_model(model, eyegaze_data_loader[3], config, encoder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
