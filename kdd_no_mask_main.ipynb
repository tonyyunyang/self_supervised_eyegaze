{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': {'test_set': 'Reading', 'test_mode': 'One_out', 'window_size': 900, 'overlap': 0.944, 'feat_dim': None, 'pretrain_model': None, 'finetune_model': None, 'batch_size': 128, 'freeze': False}, 'kdd_pretrain': {'epoch': 5, 'lr': 0.0001, 'optimizer': 'RAdam', 'weight_decay': None, 'harden': False}, 'kdd_finetune': {'epoch': 5, 'lr': 0.0001, 'optimizer': 'RAdam', 'weight_decay': None}, 'limu_pretrain': {'epoch': 10, 'lr': 0.001, 'optimizer': 'Adam', 'weight_decay': None, 'harden': False}, 'limu_finetune': {'epoch': 10, 'lr': 0.001, 'optimizer': 'Adam', 'weight_decay': None, 'classifier': 'gru'}, 'limu_mask': {'mask_ratio': 0.15, 'mask_alpha': 6, 'max_gram': 10, 'mask_prob': 0.8, 'replace_prob': 0.0}, 'kdd_model': {'d_hidden': 64, 'd_ff': 256, 'n_heads': 8, 'n_layers': 3, 'dropout': 0.1, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'projection': 'linear'}, 'limu_model': {'d_hidden': 24, 'd_ff': 72, 'n_heads': 4, 'n_layers': 4, 'emb_norm': False}, 'limu_classifier': {'gru_v1': {'rnn_layers': [2, 1], 'rnn_io': [[72, 20], [20, 10]], 'linear_io': [[10, 6]], 'activ': False, 'dropout': False}}}\n",
      "The step size of each sample is 15, this is determined via the overlap\n",
      "Class: BROWSE -> Encoded Value: 0\n",
      "Class: PLAY -> Encoded Value: 1\n",
      "Class: READ -> Encoded Value: 2\n",
      "Class: SEARCH -> Encoded Value: 3\n",
      "Class: WATCH -> Encoded Value: 4\n",
      "Class: WRITE -> Encoded Value: 5\n",
      "The number of classes is 6, the feat_dim is 2\n",
      "Pretrain samples amount: 24821\n",
      "Finetune training samples amount: 248\n",
      "Finetune validation samples amount: 107\n",
      "Final testing samples amount: 3191\n",
      "Label 5: 48 samples\n",
      "Label 2: 32 samples\n",
      "Label 4: 36 samples\n",
      "Label 3: 44 samples\n",
      "Label 1: 48 samples\n",
      "Label 0: 40 samples\n",
      "Model:\n",
      "TSTransformerEncoder(\n",
      "  (project_inp): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (pos_enc): LearnablePositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
      "        (norm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Total number of parameters: 28722\n",
      "Trainable parameters: 28722\n",
      "=============================================================\n",
      "=====================Training via cuda===================\n",
      "=============================================================\n",
      "Epoch 1/700, Train Loss: 57.0546, Time: 12.468096256256104\n",
      "Epoch 2/700, Train Loss: 27.2144, Time: 11.697746515274048\n",
      "Epoch 3/700, Train Loss: 15.6002, Time: 11.831183910369873\n",
      "Epoch 4/700, Train Loss: 8.9057, Time: 11.646875381469727\n",
      "Epoch 5/700, Train Loss: 4.9716, Time: 11.75281310081482\n",
      "Epoch 6/700, Train Loss: 3.0097, Time: 11.927138805389404\n",
      "Epoch 7/700, Train Loss: 2.2758, Time: 11.78789472579956\n",
      "Epoch 8/700, Train Loss: 1.8159, Time: 11.825879573822021\n",
      "Epoch 9/700, Train Loss: 1.5139, Time: 12.038922548294067\n",
      "Epoch 10/700, Train Loss: 1.2987, Time: 12.046395778656006\n",
      "Epoch 11/700, Train Loss: 1.1448, Time: 12.116082191467285\n",
      "Epoch 12/700, Train Loss: 1.0250, Time: 11.91390061378479\n",
      "Epoch 13/700, Train Loss: 0.9341, Time: 11.816367626190186\n",
      "Epoch 14/700, Train Loss: 0.8764, Time: 11.792667627334595\n",
      "Epoch 15/700, Train Loss: 0.8178, Time: 11.791120767593384\n",
      "Epoch 16/700, Train Loss: 0.7802, Time: 11.906831741333008\n",
      "Epoch 17/700, Train Loss: 0.7421, Time: 11.868148565292358\n",
      "Epoch 18/700, Train Loss: 0.7265, Time: 11.790721654891968\n",
      "Epoch 19/700, Train Loss: 0.6962, Time: 12.063001871109009\n",
      "Epoch 20/700, Train Loss: 0.6833, Time: 11.859984159469604\n",
      "Epoch 21/700, Train Loss: 0.6436, Time: 12.048089742660522\n",
      "Epoch 22/700, Train Loss: 0.6452, Time: 11.857447147369385\n",
      "Epoch 23/700, Train Loss: 0.6053, Time: 11.838446855545044\n",
      "Epoch 24/700, Train Loss: 0.5920, Time: 11.868825197219849\n",
      "Epoch 25/700, Train Loss: 0.5731, Time: 11.832932949066162\n",
      "Epoch 26/700, Train Loss: 0.5652, Time: 11.885510921478271\n",
      "Epoch 27/700, Train Loss: 0.5524, Time: 11.806264877319336\n",
      "Epoch 28/700, Train Loss: 0.5349, Time: 11.91457724571228\n",
      "Epoch 29/700, Train Loss: 0.5208, Time: 11.837651014328003\n",
      "Epoch 30/700, Train Loss: 0.5201, Time: 12.028858184814453\n",
      "Epoch 31/700, Train Loss: 0.4930, Time: 11.909904718399048\n",
      "Epoch 32/700, Train Loss: 0.4877, Time: 11.891981363296509\n",
      "Epoch 33/700, Train Loss: 0.4735, Time: 11.835835933685303\n",
      "Epoch 34/700, Train Loss: 0.4680, Time: 11.790556192398071\n",
      "Epoch 35/700, Train Loss: 0.4545, Time: 11.783308744430542\n",
      "Epoch 36/700, Train Loss: 0.4609, Time: 11.872180938720703\n",
      "Epoch 37/700, Train Loss: 0.4411, Time: 11.795628309249878\n",
      "Epoch 38/700, Train Loss: 0.4350, Time: 11.756620645523071\n",
      "Epoch 39/700, Train Loss: 0.4344, Time: 11.794321537017822\n",
      "Epoch 40/700, Train Loss: 0.4181, Time: 11.776310205459595\n",
      "Epoch 41/700, Train Loss: 0.4168, Time: 11.92982006072998\n",
      "Epoch 42/700, Train Loss: 0.4219, Time: 11.736932516098022\n",
      "Epoch 43/700, Train Loss: 0.4008, Time: 11.75899338722229\n",
      "Epoch 44/700, Train Loss: 0.3900, Time: 11.762789249420166\n",
      "Epoch 45/700, Train Loss: 0.3899, Time: 11.762505769729614\n",
      "Epoch 46/700, Train Loss: 0.3849, Time: 11.847721338272095\n",
      "Epoch 47/700, Train Loss: 0.3685, Time: 11.772859573364258\n",
      "Epoch 48/700, Train Loss: 0.3717, Time: 11.77556586265564\n",
      "Epoch 49/700, Train Loss: 0.3661, Time: 11.881004333496094\n",
      "Epoch 50/700, Train Loss: 0.3593, Time: 11.92337155342102\n",
      "Epoch 51/700, Train Loss: 0.3552, Time: 12.030088424682617\n",
      "Epoch 52/700, Train Loss: 0.3425, Time: 11.813845872879028\n",
      "Epoch 53/700, Train Loss: 0.3490, Time: 11.7399160861969\n",
      "Epoch 54/700, Train Loss: 0.3469, Time: 11.770516395568848\n",
      "Epoch 55/700, Train Loss: 0.3445, Time: 11.783848285675049\n",
      "Epoch 56/700, Train Loss: 0.3326, Time: 11.83971881866455\n",
      "Epoch 57/700, Train Loss: 0.3294, Time: 11.778783559799194\n",
      "Epoch 58/700, Train Loss: 0.3240, Time: 11.785359859466553\n",
      "Epoch 59/700, Train Loss: 0.3184, Time: 11.730659484863281\n",
      "Epoch 60/700, Train Loss: 0.3177, Time: 11.725913286209106\n",
      "Epoch 61/700, Train Loss: 0.3220, Time: 11.940897941589355\n",
      "Epoch 62/700, Train Loss: 0.3072, Time: 11.871547222137451\n",
      "Epoch 63/700, Train Loss: 0.3083, Time: 11.765022039413452\n",
      "Epoch 64/700, Train Loss: 0.3028, Time: 11.85806393623352\n",
      "Epoch 65/700, Train Loss: 0.3001, Time: 11.764781951904297\n",
      "Epoch 66/700, Train Loss: 0.3017, Time: 11.913454294204712\n",
      "Epoch 67/700, Train Loss: 0.2973, Time: 11.735804319381714\n",
      "Epoch 68/700, Train Loss: 0.2984, Time: 11.73741626739502\n",
      "Epoch 69/700, Train Loss: 0.2965, Time: 11.798464298248291\n",
      "Epoch 70/700, Train Loss: 0.2954, Time: 11.920658349990845\n",
      "Epoch 71/700, Train Loss: 0.2743, Time: 11.829386949539185\n",
      "Epoch 72/700, Train Loss: 0.2807, Time: 11.835173606872559\n",
      "Epoch 73/700, Train Loss: 0.2807, Time: 11.736922264099121\n",
      "Epoch 74/700, Train Loss: 0.2865, Time: 11.79660177230835\n",
      "Epoch 75/700, Train Loss: 0.2844, Time: 11.890276908874512\n",
      "Epoch 76/700, Train Loss: 0.2698, Time: 11.90430736541748\n",
      "Epoch 77/700, Train Loss: 0.2755, Time: 11.895428657531738\n",
      "Epoch 78/700, Train Loss: 0.2720, Time: 11.731355428695679\n",
      "Epoch 79/700, Train Loss: 0.2709, Time: 11.74116826057434\n",
      "Epoch 80/700, Train Loss: 0.2765, Time: 11.959998369216919\n",
      "Epoch 81/700, Train Loss: 0.2744, Time: 11.834832906723022\n",
      "Epoch 82/700, Train Loss: 0.2670, Time: 11.756863832473755\n",
      "Epoch 83/700, Train Loss: 0.2652, Time: 11.774800777435303\n",
      "Epoch 84/700, Train Loss: 0.2632, Time: 11.923840522766113\n",
      "Epoch 85/700, Train Loss: 0.2597, Time: 11.825600147247314\n",
      "Epoch 86/700, Train Loss: 0.2681, Time: 11.864723682403564\n",
      "Epoch 87/700, Train Loss: 0.2560, Time: 11.80124807357788\n",
      "Epoch 88/700, Train Loss: 0.2581, Time: 11.763471841812134\n",
      "Epoch 89/700, Train Loss: 0.2593, Time: 11.878368616104126\n",
      "Epoch 90/700, Train Loss: 0.2609, Time: 11.780647039413452\n",
      "Epoch 91/700, Train Loss: 0.2543, Time: 11.876157999038696\n",
      "Epoch 92/700, Train Loss: 0.2524, Time: 11.94395637512207\n",
      "Epoch 93/700, Train Loss: 0.2443, Time: 11.757757663726807\n",
      "Epoch 94/700, Train Loss: 0.2582, Time: 11.754279851913452\n",
      "Epoch 95/700, Train Loss: 0.2502, Time: 11.80383014678955\n",
      "Epoch 96/700, Train Loss: 0.2435, Time: 11.819846391677856\n",
      "Epoch 97/700, Train Loss: 0.2487, Time: 11.843700647354126\n",
      "Epoch 98/700, Train Loss: 0.2436, Time: 11.881744384765625\n",
      "Epoch 99/700, Train Loss: 0.2381, Time: 11.738556385040283\n",
      "Epoch 100/700, Train Loss: 0.2443, Time: 11.750224828720093\n",
      "Epoch 101/700, Train Loss: 0.2468, Time: 11.932248592376709\n",
      "Epoch 102/700, Train Loss: 0.2386, Time: 11.907307147979736\n",
      "Epoch 103/700, Train Loss: 0.2376, Time: 11.853031158447266\n",
      "Epoch 104/700, Train Loss: 0.2458, Time: 11.779577255249023\n",
      "Epoch 105/700, Train Loss: 0.2314, Time: 11.775213479995728\n",
      "Epoch 106/700, Train Loss: 0.2339, Time: 11.750803232192993\n",
      "Epoch 107/700, Train Loss: 0.2443, Time: 11.83842921257019\n",
      "Epoch 108/700, Train Loss: 0.2292, Time: 11.768771409988403\n",
      "Epoch 109/700, Train Loss: 0.2328, Time: 11.828933477401733\n",
      "Epoch 110/700, Train Loss: 0.2392, Time: 11.772210836410522\n",
      "Epoch 111/700, Train Loss: 0.2295, Time: 11.788665771484375\n",
      "Epoch 112/700, Train Loss: 0.2368, Time: 11.748852968215942\n",
      "Epoch 113/700, Train Loss: 0.2380, Time: 11.791778802871704\n",
      "Epoch 114/700, Train Loss: 0.2296, Time: 11.715534210205078\n",
      "Epoch 115/700, Train Loss: 0.2298, Time: 11.737632274627686\n",
      "Epoch 116/700, Train Loss: 0.2357, Time: 11.70329999923706\n",
      "Epoch 117/700, Train Loss: 0.2320, Time: 11.830884218215942\n",
      "Epoch 118/700, Train Loss: 0.2228, Time: 11.714727878570557\n",
      "Epoch 119/700, Train Loss: 0.2285, Time: 11.735807180404663\n",
      "Epoch 120/700, Train Loss: 0.2303, Time: 11.69983983039856\n",
      "Epoch 121/700, Train Loss: 0.2234, Time: 11.752001762390137\n",
      "Epoch 122/700, Train Loss: 0.2284, Time: 11.940158128738403\n",
      "Epoch 123/700, Train Loss: 0.2229, Time: 11.834116697311401\n",
      "Epoch 124/700, Train Loss: 0.2285, Time: 11.76445198059082\n",
      "Epoch 125/700, Train Loss: 0.2277, Time: 11.819486141204834\n",
      "Epoch 126/700, Train Loss: 0.2241, Time: 11.937998294830322\n",
      "Epoch 127/700, Train Loss: 0.2215, Time: 11.835201978683472\n",
      "Epoch 128/700, Train Loss: 0.2280, Time: 11.844388246536255\n",
      "Epoch 129/700, Train Loss: 0.2218, Time: 11.714024305343628\n",
      "Epoch 130/700, Train Loss: 0.2226, Time: 11.69593596458435\n",
      "Epoch 131/700, Train Loss: 0.2248, Time: 11.696440935134888\n",
      "Epoch 132/700, Train Loss: 0.2138, Time: 11.678330183029175\n",
      "Epoch 133/700, Train Loss: 0.2184, Time: 11.753145217895508\n",
      "Epoch 134/700, Train Loss: 0.2117, Time: 11.760685443878174\n",
      "Epoch 135/700, Train Loss: 0.2185, Time: 11.798318862915039\n",
      "Epoch 136/700, Train Loss: 0.2194, Time: 11.81478476524353\n",
      "Epoch 137/700, Train Loss: 0.2185, Time: 11.809706449508667\n",
      "Epoch 138/700, Train Loss: 0.2192, Time: 11.969884157180786\n",
      "Epoch 139/700, Train Loss: 0.2142, Time: 11.770265817642212\n",
      "Epoch 140/700, Train Loss: 0.2169, Time: 11.787667036056519\n",
      "Epoch 141/700, Train Loss: 0.2152, Time: 11.979246139526367\n",
      "Epoch 142/700, Train Loss: 0.2145, Time: 11.702763319015503\n",
      "Epoch 143/700, Train Loss: 0.2136, Time: 11.729929208755493\n",
      "Epoch 144/700, Train Loss: 0.2195, Time: 11.746705532073975\n",
      "Epoch 145/700, Train Loss: 0.2141, Time: 11.79384469985962\n",
      "Epoch 146/700, Train Loss: 0.2149, Time: 11.86956787109375\n",
      "Epoch 147/700, Train Loss: 0.2121, Time: 11.8907310962677\n",
      "Epoch 148/700, Train Loss: 0.2142, Time: 11.83990216255188\n",
      "Epoch 149/700, Train Loss: 0.2123, Time: 11.878749132156372\n",
      "Epoch 150/700, Train Loss: 0.2125, Time: 11.857211828231812\n",
      "Epoch 151/700, Train Loss: 0.2084, Time: 11.755107402801514\n",
      "Epoch 152/700, Train Loss: 0.2117, Time: 11.743655681610107\n",
      "Epoch 153/700, Train Loss: 0.2094, Time: 11.833407640457153\n",
      "Epoch 154/700, Train Loss: 0.2094, Time: 11.854404211044312\n",
      "Epoch 155/700, Train Loss: 0.2077, Time: 11.7687668800354\n",
      "Epoch 156/700, Train Loss: 0.2058, Time: 11.744931936264038\n",
      "Epoch 157/700, Train Loss: 0.2006, Time: 11.749173164367676\n",
      "Epoch 158/700, Train Loss: 0.2065, Time: 11.978514194488525\n",
      "Epoch 159/700, Train Loss: 0.2070, Time: 11.818013429641724\n",
      "Epoch 160/700, Train Loss: 0.2089, Time: 11.864185571670532\n",
      "Epoch 161/700, Train Loss: 0.2068, Time: 11.767279624938965\n",
      "Epoch 162/700, Train Loss: 0.2076, Time: 11.767082691192627\n",
      "Epoch 163/700, Train Loss: 0.2076, Time: 11.803277492523193\n",
      "Epoch 164/700, Train Loss: 0.2091, Time: 11.876457929611206\n",
      "Epoch 165/700, Train Loss: 0.2028, Time: 11.854421377182007\n",
      "Epoch 166/700, Train Loss: 0.2051, Time: 11.862035274505615\n",
      "Epoch 167/700, Train Loss: 0.2009, Time: 11.792951345443726\n",
      "Epoch 168/700, Train Loss: 0.2106, Time: 11.85561752319336\n",
      "Epoch 169/700, Train Loss: 0.1984, Time: 11.753827333450317\n",
      "Epoch 170/700, Train Loss: 0.2086, Time: 11.799144744873047\n",
      "Epoch 171/700, Train Loss: 0.2037, Time: 11.7522554397583\n",
      "Epoch 172/700, Train Loss: 0.2042, Time: 11.987837553024292\n",
      "Epoch 173/700, Train Loss: 0.2045, Time: 12.050638914108276\n",
      "Epoch 174/700, Train Loss: 0.2040, Time: 11.918866157531738\n",
      "Epoch 175/700, Train Loss: 0.2041, Time: 11.851055383682251\n",
      "Epoch 176/700, Train Loss: 0.1959, Time: 11.792481660842896\n",
      "Epoch 177/700, Train Loss: 0.2047, Time: 11.725234270095825\n",
      "Epoch 178/700, Train Loss: 0.2010, Time: 11.812326669692993\n",
      "Epoch 179/700, Train Loss: 0.1995, Time: 11.755035400390625\n",
      "Epoch 180/700, Train Loss: 0.1968, Time: 11.778972864151001\n",
      "Epoch 181/700, Train Loss: 0.1963, Time: 11.755728006362915\n",
      "Epoch 182/700, Train Loss: 0.2027, Time: 11.738938808441162\n",
      "Epoch 183/700, Train Loss: 0.1932, Time: 11.832414865493774\n",
      "Epoch 184/700, Train Loss: 0.1990, Time: 11.857428312301636\n",
      "Epoch 185/700, Train Loss: 0.1989, Time: 11.820646286010742\n",
      "Epoch 186/700, Train Loss: 0.1978, Time: 11.81567645072937\n",
      "Epoch 187/700, Train Loss: 0.1979, Time: 11.909709215164185\n",
      "Epoch 188/700, Train Loss: 0.1957, Time: 11.881264686584473\n",
      "Epoch 189/700, Train Loss: 0.1943, Time: 11.91017198562622\n",
      "Epoch 190/700, Train Loss: 0.1904, Time: 11.85864782333374\n",
      "Epoch 191/700, Train Loss: 0.1961, Time: 11.718285322189331\n",
      "Epoch 192/700, Train Loss: 0.1941, Time: 11.898671388626099\n",
      "Epoch 193/700, Train Loss: 0.1995, Time: 11.850937604904175\n",
      "Epoch 194/700, Train Loss: 0.1968, Time: 11.86040711402893\n",
      "Epoch 195/700, Train Loss: 0.1956, Time: 11.752335548400879\n",
      "Epoch 196/700, Train Loss: 0.1901, Time: 11.742708921432495\n",
      "Epoch 197/700, Train Loss: 0.1944, Time: 11.796608686447144\n",
      "Epoch 198/700, Train Loss: 0.1957, Time: 11.911580085754395\n",
      "Epoch 199/700, Train Loss: 0.1870, Time: 11.93581748008728\n",
      "Epoch 200/700, Train Loss: 0.1926, Time: 11.913548946380615\n",
      "Epoch 201/700, Train Loss: 0.1925, Time: 12.012271881103516\n",
      "Epoch 202/700, Train Loss: 0.1976, Time: 11.85811448097229\n",
      "Epoch 203/700, Train Loss: 0.1956, Time: 11.793828964233398\n",
      "Epoch 204/700, Train Loss: 0.1928, Time: 11.76278042793274\n",
      "Epoch 205/700, Train Loss: 0.2011, Time: 11.754427433013916\n",
      "Epoch 206/700, Train Loss: 0.1939, Time: 11.808411836624146\n",
      "Epoch 207/700, Train Loss: 0.1925, Time: 11.863144159317017\n",
      "Epoch 208/700, Train Loss: 0.1954, Time: 11.960728645324707\n",
      "Epoch 209/700, Train Loss: 0.1924, Time: 11.768640518188477\n",
      "Epoch 210/700, Train Loss: 0.1969, Time: 11.7405104637146\n",
      "Epoch 211/700, Train Loss: 0.1935, Time: 11.756993532180786\n",
      "Epoch 212/700, Train Loss: 0.1930, Time: 11.722403287887573\n",
      "Epoch 213/700, Train Loss: 0.1904, Time: 11.732585668563843\n",
      "Epoch 214/700, Train Loss: 0.1924, Time: 11.852449178695679\n",
      "Epoch 215/700, Train Loss: 0.1963, Time: 11.736178874969482\n",
      "Epoch 216/700, Train Loss: 0.1896, Time: 11.756386756896973\n",
      "Epoch 217/700, Train Loss: 0.1829, Time: 11.864171981811523\n",
      "Epoch 218/700, Train Loss: 0.1861, Time: 11.812342166900635\n",
      "Epoch 219/700, Train Loss: 0.1889, Time: 11.831573724746704\n",
      "Epoch 220/700, Train Loss: 0.1861, Time: 11.71580719947815\n",
      "Epoch 221/700, Train Loss: 0.1833, Time: 11.73765754699707\n",
      "Epoch 222/700, Train Loss: 0.1836, Time: 11.755108118057251\n",
      "Epoch 223/700, Train Loss: 0.1877, Time: 11.743643999099731\n",
      "Epoch 224/700, Train Loss: 0.1884, Time: 11.766035556793213\n",
      "Epoch 225/700, Train Loss: 0.1907, Time: 11.759965658187866\n",
      "Epoch 226/700, Train Loss: 0.1876, Time: 11.736174583435059\n",
      "Epoch 227/700, Train Loss: 0.1825, Time: 11.747599363327026\n",
      "Epoch 228/700, Train Loss: 0.1860, Time: 11.726572036743164\n",
      "Epoch 229/700, Train Loss: 0.1913, Time: 11.79356861114502\n",
      "Epoch 230/700, Train Loss: 0.1845, Time: 11.766586780548096\n",
      "Epoch 231/700, Train Loss: 0.1827, Time: 11.746782302856445\n",
      "Epoch 232/700, Train Loss: 0.1881, Time: 11.754692316055298\n",
      "Epoch 233/700, Train Loss: 0.1868, Time: 11.837918758392334\n",
      "Epoch 234/700, Train Loss: 0.1848, Time: 11.91424298286438\n",
      "Epoch 235/700, Train Loss: 0.1908, Time: 11.897082805633545\n",
      "Epoch 236/700, Train Loss: 0.1893, Time: 11.804073810577393\n",
      "Epoch 237/700, Train Loss: 0.1864, Time: 11.79628586769104\n",
      "Epoch 238/700, Train Loss: 0.1858, Time: 11.877763986587524\n",
      "Epoch 239/700, Train Loss: 0.1842, Time: 11.836022853851318\n",
      "Epoch 240/700, Train Loss: 0.1852, Time: 11.827666997909546\n",
      "Epoch 241/700, Train Loss: 0.1793, Time: 11.72633695602417\n",
      "Epoch 242/700, Train Loss: 0.1898, Time: 11.785163640975952\n",
      "Epoch 243/700, Train Loss: 0.1815, Time: 11.940481901168823\n",
      "Epoch 244/700, Train Loss: 0.1795, Time: 11.80537223815918\n",
      "Epoch 245/700, Train Loss: 0.1786, Time: 11.958237409591675\n",
      "Epoch 246/700, Train Loss: 0.1862, Time: 11.75997519493103\n",
      "Epoch 247/700, Train Loss: 0.1789, Time: 11.775387287139893\n",
      "Epoch 248/700, Train Loss: 0.1841, Time: 11.755792379379272\n",
      "Epoch 249/700, Train Loss: 0.1796, Time: 11.804453611373901\n",
      "Epoch 250/700, Train Loss: 0.1757, Time: 11.748550176620483\n",
      "Epoch 251/700, Train Loss: 0.1824, Time: 11.79819917678833\n",
      "Epoch 252/700, Train Loss: 0.1790, Time: 11.760804653167725\n",
      "Epoch 253/700, Train Loss: 0.1822, Time: 11.768252611160278\n",
      "Epoch 254/700, Train Loss: 0.1791, Time: 11.960315942764282\n",
      "Epoch 255/700, Train Loss: 0.1803, Time: 11.796238899230957\n",
      "Epoch 256/700, Train Loss: 0.1817, Time: 11.739540338516235\n",
      "Epoch 257/700, Train Loss: 0.1893, Time: 11.788857221603394\n",
      "Epoch 258/700, Train Loss: 0.1854, Time: 11.737318515777588\n",
      "Epoch 259/700, Train Loss: 0.1818, Time: 11.819793701171875\n",
      "Epoch 260/700, Train Loss: 0.1800, Time: 11.772128105163574\n",
      "Epoch 261/700, Train Loss: 0.1709, Time: 11.743471145629883\n",
      "Epoch 262/700, Train Loss: 0.1832, Time: 11.73075819015503\n",
      "Epoch 263/700, Train Loss: 0.1855, Time: 11.734829902648926\n",
      "Epoch 264/700, Train Loss: 0.1836, Time: 11.949488401412964\n",
      "Epoch 265/700, Train Loss: 0.1799, Time: 11.73084044456482\n",
      "Epoch 266/700, Train Loss: 0.1786, Time: 11.746220827102661\n",
      "Epoch 267/700, Train Loss: 0.1794, Time: 11.759184122085571\n",
      "Epoch 268/700, Train Loss: 0.1758, Time: 11.799480676651001\n",
      "Epoch 269/700, Train Loss: 0.1818, Time: 11.933303356170654\n",
      "Epoch 270/700, Train Loss: 0.1808, Time: 11.779428958892822\n",
      "Epoch 271/700, Train Loss: 0.1752, Time: 11.792731523513794\n",
      "Epoch 272/700, Train Loss: 0.1790, Time: 11.758450031280518\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from modules.finetune_hyperparameters import KDD_Finetune_Hyperparameters\n",
    "from modules.kdd_model import kdd_model4pretrain, kdd_model4finetune\n",
    "from modules.pretrain_hyperparameters import KDD_Pretrain_Hyperparameters, KDD_NoMask_Pretrain_Hyperparameters\n",
    "from utils.finetune import finetune_kdd_model, eval_finetune_kdd_model\n",
    "from utils.load_data_from_file import load_mixed_data, prepare_mixed_data_loader, load_one_out_data, \\\n",
    "    prepare_one_out_data_loader, prepare_no_mask_one_out_data_loader\n",
    "from utils.pretrain import pretrain_kdd_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the config from JSON file first\n",
    "    with open(\"utils/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "    print(config)\n",
    "\n",
    "    # config[\"general\"][\"pretrain_model\"] = \"results/kdd_model/One_out/linear/pretrain/window_size_30sec/freeze_False_epoch_500_lr_0.0001_d_hidden_64_d_ff_256_n_heads_8_n_layer_1_pos_encode_learnable_activation_gelu_norm_BatchNorm\"\n",
    "\n",
    "    config[\"general\"][\"test_set\"] = \"Desktop\" # Reading or Desktop\n",
    "\n",
    "    config[\"general\"][\"window_size\"] = 150\n",
    "    config[\"general\"][\"overlap\"] = 0.899\n",
    "    config[\"general\"][\"batch_size\"] = 128\n",
    "    config[\"kdd_pretrain\"][\"epoch\"] = 700\n",
    "    config[\"kdd_finetune\"][\"epoch\"] = 1000\n",
    "\n",
    "    config[\"kdd_model\"][\"d_hidden\"] = 16\n",
    "    config[\"kdd_model\"][\"d_ff\"] = 64\n",
    "    config[\"kdd_model\"][\"n_heads\"] = 8\n",
    "    config[\"kdd_model\"][\"n_layers\"] = 8\n",
    "\n",
    "    # First load the data into dataloader according to chosen test_mode: Mixed or One_out\n",
    "    if config[\"general\"][\"test_mode\"] == \"Mixed\":\n",
    "        data, labels, encoder = load_mixed_data(window_size=config[\"general\"][\"window_size\"],\n",
    "                                                overlap=config[\"general\"][\"overlap\"],\n",
    "                                                data_set=config[\"general\"][\"test_set\"])\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        labels_dim = labels.shape\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}, the labels_dim is {labels_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_mixed_data_loader\n",
    "                               (data, labels, batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"]))\n",
    "\n",
    "    elif config[\"general\"][\"test_mode\"] == \"One_out\":\n",
    "        train_data, train_labels, test_data, test_labels, encoder = (load_one_out_data\n",
    "                                                                     (window_size=config[\"general\"][\"window_size\"],\n",
    "                                                                      overlap=config[\"general\"][\"overlap\"],\n",
    "                                                                      data_set=config[\"general\"][\"test_set\"]))\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = train_data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_no_mask_one_out_data_loader\n",
    "                               (train_data, train_labels, test_data, test_labels,\n",
    "                                batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"]))\n",
    "    else:\n",
    "        print(\"Either Mixed / One_out\")\n",
    "        sys.exit()\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # If the pretrain_model path is not provided, start with pretraining the model\n",
    "    if config[\"general\"][\"pretrain_model\"] is None:\n",
    "        hyperparameters = KDD_NoMask_Pretrain_Hyperparameters(config)\n",
    "        model = kdd_model4pretrain(config, feat_dim)\n",
    "        loss = hyperparameters.loss\n",
    "        optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                              weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "        pretrain_kdd_model(model, loss, optimizer, eyegaze_data_loader[0], config)\n",
    "\n",
    "    # If the pretrain_model path is provided, meaning that there is already a pretrained model, then directly finetune\n",
    "    # After pretrain, finetune will be performed automatically, because the pretrain_model will be filled\n",
    "    hyperparameters = KDD_Finetune_Hyperparameters(config)\n",
    "    model = kdd_model4finetune(config, feat_dim, num_classes)\n",
    "    loss = hyperparameters.loss\n",
    "    optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                          weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "    # eyegaze_data_loader[1] is the training set, and eyegaze_data_loader[2] is the validation set\n",
    "    finetune_kdd_model(model, loss, optimizer, eyegaze_data_loader[1], eyegaze_data_loader[2], config)\n",
    "\n",
    "    eval_finetune_kdd_model(model, eyegaze_data_loader[3], config, encoder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
