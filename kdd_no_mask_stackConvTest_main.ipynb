{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': {'test_set': 'Reading', 'test_mode': 'One_out', 'window_size': 900, 'overlap': 0.944, 'feat_dim': None, 'pretrain_model': None, 'finetune_model': None, 'batch_size': 128, 'freeze': False, 'stack_conv': False}, 'kdd_pretrain': {'epoch': 5, 'lr': 0.001, 'optimizer': 'RAdam', 'weight_decay': None, 'harden': False}, 'kdd_finetune': {'epoch': 5, 'lr': 0.0002, 'optimizer': 'RAdam', 'weight_decay': None}, 'limu_pretrain': {'epoch': 10, 'lr': 0.001, 'optimizer': 'Adam', 'weight_decay': None, 'harden': False}, 'limu_finetune': {'epoch': 10, 'lr': 0.001, 'optimizer': 'Adam', 'weight_decay': None, 'classifier': 'gru'}, 'limu_mask': {'mask_ratio': 0.15, 'mask_alpha': 6, 'max_gram': 10, 'mask_prob': 0.8, 'replace_prob': 0.0}, 'kdd_model': {'d_hidden': 64, 'd_ff': 256, 'n_heads': 8, 'n_layers': 3, 'dropout': 0.1, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'projection': 'convolution'}, 'limu_model': {'d_hidden': 24, 'd_ff': 72, 'n_heads': 4, 'n_layers': 4, 'emb_norm': False}, 'limu_classifier': {'gru_v1': {'rnn_layers': [2, 1], 'rnn_io': [[72, 20], [20, 10]], 'linear_io': [[10, 6]], 'activ': False, 'dropout': False}}, 'conv1d_5sec': {'first': {'kernel_size': 20, 'stride': 10, 'dilation': 1, 'padding': 0}}, 'conv1d_10sec': {'first': {'kernel_size': 30, 'stride': 15, 'dilation': 1, 'padding': 0}}, 'conv1d_15sec': {'first': {'kernel_size': 40, 'stride': 20, 'dilation': 1, 'padding': 0}}, 'conv1d_20sec': {'first': {'kernel_size': 30, 'stride': 15, 'dilation': 1, 'padding': 0}}, 'conv1d_30sec': {'first': {'kernel_size': 30, 'stride': 15, 'dilation': 1, 'padding': 0}}, 'conv1d_5sec_stack': {'first': {'kernel_size': 10, 'stride': 1, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 20, 'stride': 1, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 20, 'stride': 10, 'dilation': 1, 'padding': 0}}, 'conv1d_10sec_stack': {'first': {'kernel_size': 5, 'stride': 1, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 5, 'stride': 1, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 5, 'stride': 1, 'dilation': 1, 'padding': 0}}, 'conv1d_15sec_stack': {'first': {'kernel_size': 10, 'stride': 1, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 20, 'stride': 1, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 20, 'stride': 10, 'dilation': 1, 'padding': 0}}, 'conv1d_30sec_stack': {'first': {'kernel_size': 20, 'stride': 1, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 40, 'stride': 1, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 60, 'stride': 30, 'dilation': 1, 'padding': 0}}}\n",
      "The step size of each sample is 59, this is determined via the overlap\n",
      "Class: BROWSE -> Encoded Value: 0\n",
      "Class: PLAY -> Encoded Value: 1\n",
      "Class: READ -> Encoded Value: 2\n",
      "Class: SEARCH -> Encoded Value: 3\n",
      "Class: WATCH -> Encoded Value: 4\n",
      "Class: WRITE -> Encoded Value: 5\n",
      "The number of classes is 6, the feat_dim is 2\n",
      "Pretrain samples amount: 6216\n",
      "Finetune training samples amount: 124\n",
      "Finetune validation samples amount: 54\n",
      "Final testing samples amount: 710\n",
      "Label 0: 23 samples\n",
      "Label 3: 18 samples\n",
      "Label 5: 21 samples\n",
      "Label 2: 27 samples\n",
      "Label 4: 18 samples\n",
      "Label 1: 17 samples\n",
      "The first produces sequence length of 296\n",
      "The second produces sequence length of 292\n",
      "The thrid produces sequence length of 288\n",
      "The thrid produces sequence length of 292\n",
      "The second produces sequence length of 296\n",
      "The first produces sequence length of 300\n",
      "Model:\n",
      "TSTransformerEncoderStackTest(\n",
      "  (project_inp): Sequential(\n",
      "    (0): Conv1d(2, 16, kernel_size=(5,), stride=(1,))\n",
      "    (1): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n",
      "    (2): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n",
      "  )\n",
      "  (pos_enc): LearnablePositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=16, bias=True)\n",
      "        (norm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): ConvTransposeLinear(\n",
      "    (conv_transpose): Sequential(\n",
      "      (0): ConvTranspose1d(16, 2, kernel_size=(5,), stride=(1,))\n",
      "      (1): ConvTranspose1d(2, 2, kernel_size=(5,), stride=(1,))\n",
      "      (2): ConvTranspose1d(2, 2, kernel_size=(5,), stride=(1,))\n",
      "    )\n",
      "    (linear): Linear(in_features=600, out_features=600, bias=True)\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Total number of parameters: 411318\n",
      "Trainable parameters: 411318\n",
      "=============================================================\n",
      "=====================Training via cuda===================\n",
      "=============================================================\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 324.00 MiB (GPU 0; 5.77 GiB total capacity; 4.67 GiB already allocated; 31.00 MiB free; 4.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb#W0sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     eval_finetune_kdd_model(model, eyegaze_data_loader[\u001b[39m3\u001b[39m], config, encoder)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb#W0sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb#W0sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     main()\n",
      "\u001b[1;32m/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb#W0sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     loss \u001b[39m=\u001b[39m hyperparameters\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb#W0sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     optimizer \u001b[39m=\u001b[39m hyperparameters\u001b[39m.\u001b[39moptimizer(model\u001b[39m.\u001b[39mparameters(), hyperparameters\u001b[39m.\u001b[39mlr,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb#W0sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m                                           weight_decay\u001b[39m=\u001b[39mhyperparameters\u001b[39m.\u001b[39mweight_decay)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb#W0sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     pretrain_kdd_model(model, loss, optimizer, eyegaze_data_loader[\u001b[39m0\u001b[39;49m], config)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb#W0sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m# If the pretrain_model path is provided, meaning that there is already a pretrained model, then directly finetune\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb#W0sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m# After pretrain, finetune will be performed automatically, because the pretrain_model will be filled\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_stackConvTest_main.ipynb#W0sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m hyperparameters \u001b[39m=\u001b[39m KDD_Finetune_Hyperparameters(config)\n",
      "File \u001b[0;32m~/Desktop/self_supervised_eyegaze/utils/pretrain.py:108\u001b[0m, in \u001b[0;36mpretrain_kdd_model\u001b[0;34m(model, loss, optimizer, pretrain_data, config)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, config[\u001b[39m\"\u001b[39m\u001b[39mkdd_pretrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    106\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 108\u001b[0m     train_loss \u001b[39m=\u001b[39m pretrain_kdd_epoch(model, loss, optimizer, pretrain_data, config, device, epoch, l2_reg\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    109\u001b[0m     train_loss_list\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m    111\u001b[0m     \u001b[39m# Save the model if it has the best loss so far\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/self_supervised_eyegaze/utils/pretrain.py:144\u001b[0m, in \u001b[0;36mpretrain_kdd_epoch\u001b[0;34m(model, loss, optimizer, pretrain_data, config, device, epoch, l2_reg)\u001b[0m\n\u001b[1;32m    141\u001b[0m target_masks \u001b[39m=\u001b[39m target_masks\u001b[39m.\u001b[39mto(device)  \u001b[39m# 1s: mask and predict, 0s: unaffected input (ignore)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m padding_masks \u001b[39m=\u001b[39m padding_masks\u001b[39m.\u001b[39mto(device)  \u001b[39m# 0s: ignore\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m predictions \u001b[39m=\u001b[39m model(X\u001b[39m.\u001b[39;49mto(device))  \u001b[39m# (batch_size, padded_length, feat_dim)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39m# Cascade noise masks (batch_size, padded_length, feat_dim) and padding masks (batch_size, padded_length)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m target_masks \u001b[39m=\u001b[39m target_masks \u001b[39m*\u001b[39m padding_masks\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/self_supervised_eyegaze/modules/kdd_model.py:1127\u001b[0m, in \u001b[0;36mTSTransformerEncoderStackTest.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     sys\u001b[39m.\u001b[39mexit()\n\u001b[1;32m   1125\u001b[0m \u001b[39m# inp = self.pos_enc(inp)  # add positional encoding\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# NOTE: logic for padding masks is reversed to comply with definition in MultiHeadAttention, TransformerEncoderLayer\u001b[39;00m\n\u001b[0;32m-> 1127\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(inp)  \u001b[39m# (seq_length, batch_size, d_model)\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(output)  \u001b[39m# the output transformer encoder/decoder embeddings don't include non-linearity\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# (batch_size, seq_length, d_model)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 315\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    318\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/self_supervised_eyegaze/modules/kdd_model.py:566\u001b[0m, in \u001b[0;36mTransformerBatchNormEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src: Tensor, src_mask: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    554\u001b[0m             src_key_padding_mask: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, is_causal: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    555\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Pass the input through the encoder layer.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \n\u001b[1;32m    557\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m        see the docs in Transformer class.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m     src2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(src, src, src, attn_mask\u001b[39m=\u001b[39;49msrc_mask,\n\u001b[1;32m    567\u001b[0m                           key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    568\u001b[0m     src \u001b[39m=\u001b[39m src \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(src2)  \u001b[39m# (seq_len, batch_size, d_model)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m     src \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)  \u001b[39m# (batch_size, d_model, seq_len)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1192\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1203\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1206\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1210\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1211\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1212\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1213\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1214\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1215\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1217\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/functional.py:5338\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5336\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbaddbmm(attn_mask, q_scaled, k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   5337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 5338\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(q_scaled, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   5339\u001b[0m attn_output_weights \u001b[39m=\u001b[39m softmax(attn_output_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   5340\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 0; 5.77 GiB total capacity; 4.67 GiB already allocated; 31.00 MiB free; 4.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from modules.finetune_hyperparameters import KDD_Finetune_Hyperparameters\n",
    "from modules.kdd_model import kdd_model4finetune_stack_test, kdd_model4finetune_test, kdd_model4pretrain, kdd_model4finetune, kdd_model4pretrain_stack_test, kdd_model4pretrain_test\n",
    "from modules.pretrain_hyperparameters import KDD_Pretrain_Hyperparameters, KDD_NoMask_Pretrain_Hyperparameters\n",
    "from utils.finetune import finetune_kdd_model, eval_finetune_kdd_model\n",
    "from utils.load_data_from_file import load_mixed_data, prepare_mixed_data_loader, load_one_out_data, \\\n",
    "    prepare_one_out_data_loader, prepare_no_mask_one_out_data_loader\n",
    "from utils.pretrain import pretrain_kdd_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the config from JSON file first\n",
    "    with open(\"utils/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "    print(config)\n",
    "\n",
    "    # config[\"general\"][\"pretrain_model\"] = \"results/Desktop/kdd_model/One_out/convolution/pretrain/window_size_15sec/feat_dim_2/stack_convolution/freeze_False_epoch_2400_lr_0.001_d_hidden_16_d_ff_128_n_heads_4_n_layer_8_pos_encode_learnable_activation_gelu_norm_BatchNorm\"\n",
    "\n",
    "    config[\"general\"][\"test_set\"] = \"Desktop\" # Reading or Desktop or CosSin\n",
    "\n",
    "    config[\"general\"][\"window_size\"] = 300\n",
    "    config[\"general\"][\"overlap\"] = 0.8\n",
    "    config[\"general\"][\"batch_size\"] = 128\n",
    "    config[\"kdd_pretrain\"][\"epoch\"] = 3000\n",
    "    config[\"kdd_finetune\"][\"epoch\"] = 6000\n",
    "\n",
    "    config[\"kdd_model\"][\"d_hidden\"] = 16\n",
    "    config[\"kdd_model\"][\"d_ff\"] = 128\n",
    "    config[\"kdd_model\"][\"n_heads\"] = 8\n",
    "    config[\"kdd_model\"][\"n_layers\"] = 8\n",
    "    \n",
    "    config[\"kdd_model\"][\"projection\"] = \"convolution\"\n",
    "    config[\"general\"][\"stack_conv\"] = True\n",
    "    # config[\"general\"][\"freeze\"] = True\n",
    "\n",
    "    # First load the data into dataloader according to chosen test_mode: Mixed or One_out\n",
    "    if config[\"general\"][\"test_mode\"] == \"Mixed\":\n",
    "        data, labels, encoder = load_mixed_data(window_size=config[\"general\"][\"window_size\"],\n",
    "                                                overlap=config[\"general\"][\"overlap\"],\n",
    "                                                data_set=config[\"general\"][\"test_set\"])\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        labels_dim = labels.shape\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}, the labels_dim is {labels_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_mixed_data_loader\n",
    "                               (data, labels, batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"]))\n",
    "\n",
    "    elif config[\"general\"][\"test_mode\"] == \"One_out\":\n",
    "        train_data, train_labels, test_data, test_labels, encoder = (load_one_out_data\n",
    "                                                                     (window_size=config[\"general\"][\"window_size\"],\n",
    "                                                                      overlap=config[\"general\"][\"overlap\"],\n",
    "                                                                      data_set=config[\"general\"][\"test_set\"]))\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = train_data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_no_mask_one_out_data_loader\n",
    "                               (train_data, train_labels, test_data, test_labels,\n",
    "                                batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"],\n",
    "                                labeled_percentage=0.2))\n",
    "    else:\n",
    "        print(\"Either Mixed / One_out\")\n",
    "        sys.exit()\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # If the pretrain_model path is not provided, start with pretraining the model\n",
    "    if config[\"general\"][\"pretrain_model\"] is None:\n",
    "        hyperparameters = KDD_NoMask_Pretrain_Hyperparameters(config)\n",
    "        model = kdd_model4pretrain_stack_test(config, feat_dim)\n",
    "        loss = hyperparameters.loss\n",
    "        optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                              weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "        pretrain_kdd_model(model, loss, optimizer, eyegaze_data_loader[0], config)\n",
    "\n",
    "    # If the pretrain_model path is provided, meaning that there is already a pretrained model, then directly finetune\n",
    "    # After pretrain, finetune will be performed automatically, because the pretrain_model will be filled\n",
    "    hyperparameters = KDD_Finetune_Hyperparameters(config)\n",
    "    model = kdd_model4finetune_stack_test(config, feat_dim, num_classes)\n",
    "    loss = hyperparameters.loss\n",
    "    optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                          weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "    # eyegaze_data_loader[1] is the training set, and eyegaze_data_loader[2] is the validation set\n",
    "    finetune_kdd_model(model, loss, optimizer, eyegaze_data_loader[1], eyegaze_data_loader[2], config)\n",
    "\n",
    "    eval_finetune_kdd_model(model, eyegaze_data_loader[3], config, encoder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
