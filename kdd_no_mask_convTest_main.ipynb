{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': {'test_set': 'Reading', 'test_mode': 'One_out', 'window_size': 900, 'overlap': 0.944, 'feat_dim': None, 'pretrain_model': None, 'finetune_model': None, 'batch_size': 128, 'freeze': False, 'stack_conv': False}, 'kdd_pretrain': {'epoch': 5, 'lr': 0.001, 'optimizer': 'RAdam', 'weight_decay': None, 'harden': False}, 'kdd_finetune': {'epoch': 5, 'lr': 0.0002, 'optimizer': 'RAdam', 'weight_decay': None}, 'limu_pretrain': {'epoch': 10, 'lr': 0.001, 'optimizer': 'Adam', 'weight_decay': None, 'harden': False}, 'limu_finetune': {'epoch': 10, 'lr': 0.001, 'optimizer': 'Adam', 'weight_decay': None, 'classifier': 'gru'}, 'limu_mask': {'mask_ratio': 0.15, 'mask_alpha': 6, 'max_gram': 10, 'mask_prob': 0.8, 'replace_prob': 0.0}, 'kdd_model': {'d_hidden': 64, 'd_ff': 256, 'n_heads': 8, 'n_layers': 3, 'dropout': 0.1, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'projection': 'convolution'}, 'limu_model': {'d_hidden': 24, 'd_ff': 72, 'n_heads': 4, 'n_layers': 4, 'emb_norm': False}, 'limu_classifier': {'gru_v1': {'rnn_layers': [2, 1], 'rnn_io': [[72, 20], [20, 10]], 'linear_io': [[10, 6]], 'activ': False, 'dropout': False}}, 'conv1d_5sec': {'first': {'kernel_size': 20, 'stride': 10, 'dilation': 1, 'padding': 0}}, 'conv1d_10sec': {'first': {'kernel_size': 30, 'stride': 15, 'dilation': 1, 'padding': 0}}, 'conv1d_15sec': {'first': {'kernel_size': 40, 'stride': 20, 'dilation': 1, 'padding': 0}}, 'conv1d_30sec': {'first': {'kernel_size': 30, 'stride': 15, 'dilation': 1, 'padding': 0}}, 'conv1d_5sec_stack': {'first': {'kernel_size': 10, 'stride': 1, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 20, 'stride': 1, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 20, 'stride': 10, 'dilation': 1, 'padding': 0}}, 'conv1d_10sec_stack': {'first': {'kernel_size': 10, 'stride': 2, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 10, 'stride': 2, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 10, 'stride': 2, 'dilation': 1, 'padding': 0}}, 'conv1d_15sec_stack': {'first': {'kernel_size': 10, 'stride': 1, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 20, 'stride': 1, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 20, 'stride': 10, 'dilation': 1, 'padding': 0}}, 'conv1d_30sec_stack': {'first': {'kernel_size': 20, 'stride': 1, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 40, 'stride': 1, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 60, 'stride': 30, 'dilation': 1, 'padding': 0}}}\n",
      "The step size of each sample is 59, this is determined via the overlap\n",
      "Class: BROWSE -> Encoded Value: 0\n",
      "Class: PLAY -> Encoded Value: 1\n",
      "Class: READ -> Encoded Value: 2\n",
      "Class: SEARCH -> Encoded Value: 3\n",
      "Class: WATCH -> Encoded Value: 4\n",
      "Class: WRITE -> Encoded Value: 5\n",
      "The number of classes is 6, the feat_dim is 2\n",
      "Pretrain samples amount: 6216\n",
      "Finetune training samples amount: 124\n",
      "Finetune validation samples amount: 54\n",
      "Final testing samples amount: 710\n",
      "Label 3: 23 samples\n",
      "Label 0: 18 samples\n",
      "Label 4: 21 samples\n",
      "Label 1: 27 samples\n",
      "Label 2: 18 samples\n",
      "Label 5: 17 samples\n",
      "Model:\n",
      "TSTransformerEncoderTest(\n",
      "  (project_inp): Conv1d(2, 16, kernel_size=(30,), stride=(15,))\n",
      "  (pos_enc): FixedPositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=16, bias=True)\n",
      "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): ConvTransposeLinear(\n",
      "    (conv_transpose): ConvTranspose1d(16, 2, kernel_size=(30,), stride=(15,))\n",
      "    (linear): Linear(in_features=600, out_features=600, bias=True)\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Total number of parameters: 405674\n",
      "Trainable parameters: 405674\n",
      "=============================================================\n",
      "=====================Training via cuda===================\n",
      "=============================================================\n",
      "Epoch 1/3000, Train Loss: 89.8769, Time: 2.290574550628662\n",
      "Epoch 2/3000, Train Loss: 13.3372, Time: 1.0444071292877197\n",
      "Epoch 3/3000, Train Loss: 7.1919, Time: 1.0034635066986084\n",
      "Epoch 4/3000, Train Loss: 6.0055, Time: 1.0059866905212402\n",
      "Epoch 5/3000, Train Loss: 5.3353, Time: 1.0374224185943604\n",
      "Epoch 6/3000, Train Loss: 5.0086, Time: 1.0262513160705566\n",
      "Epoch 7/3000, Train Loss: 4.7735, Time: 1.34320068359375\n",
      "Epoch 8/3000, Train Loss: 4.5119, Time: 1.2395966053009033\n",
      "Epoch 9/3000, Train Loss: 4.1813, Time: 1.1868517398834229\n",
      "Epoch 10/3000, Train Loss: 3.8270, Time: 1.154954195022583\n",
      "Epoch 11/3000, Train Loss: 3.6251, Time: 1.1476342678070068\n",
      "Epoch 12/3000, Train Loss: 3.4636, Time: 1.1253080368041992\n",
      "Epoch 13/3000, Train Loss: 3.3951, Time: 1.1831648349761963\n",
      "Epoch 14/3000, Train Loss: 3.2843, Time: 1.1999683380126953\n",
      "Epoch 15/3000, Train Loss: 3.2659, Time: 1.143158197402954\n",
      "Epoch 16/3000, Train Loss: 3.1718, Time: 1.2112658023834229\n",
      "Epoch 17/3000, Train Loss: 3.0730, Time: 1.1737291812896729\n",
      "Epoch 18/3000, Train Loss: 2.9109, Time: 1.1695411205291748\n",
      "Epoch 19/3000, Train Loss: 2.7401, Time: 1.1490049362182617\n",
      "Epoch 20/3000, Train Loss: 2.5028, Time: 1.1538243293762207\n",
      "Epoch 21/3000, Train Loss: 2.3652, Time: 1.154184103012085\n",
      "Epoch 22/3000, Train Loss: 2.2076, Time: 1.1609013080596924\n",
      "Epoch 23/3000, Train Loss: 2.1243, Time: 1.1366376876831055\n",
      "Epoch 24/3000, Train Loss: 2.0913, Time: 1.1222918033599854\n",
      "Epoch 25/3000, Train Loss: 2.0219, Time: 1.1218488216400146\n",
      "Epoch 26/3000, Train Loss: 2.0497, Time: 1.1138439178466797\n",
      "Epoch 27/3000, Train Loss: 1.9696, Time: 1.1318013668060303\n",
      "Epoch 28/3000, Train Loss: 1.9235, Time: 1.150045394897461\n",
      "Epoch 29/3000, Train Loss: 1.9490, Time: 1.1371312141418457\n",
      "Epoch 30/3000, Train Loss: 1.8877, Time: 1.1580893993377686\n",
      "Epoch 31/3000, Train Loss: 1.8577, Time: 1.131697654724121\n",
      "Epoch 32/3000, Train Loss: 1.8061, Time: 1.1287147998809814\n",
      "Epoch 33/3000, Train Loss: 1.8324, Time: 1.112806797027588\n",
      "Epoch 34/3000, Train Loss: 1.7810, Time: 1.1182072162628174\n",
      "Epoch 35/3000, Train Loss: 1.7855, Time: 1.1198973655700684\n",
      "Epoch 36/3000, Train Loss: 1.7781, Time: 1.147522211074829\n",
      "Epoch 37/3000, Train Loss: 1.7400, Time: 1.1638998985290527\n",
      "Epoch 38/3000, Train Loss: 1.7240, Time: 1.156134843826294\n",
      "Epoch 39/3000, Train Loss: 1.7400, Time: 1.147690773010254\n",
      "Epoch 40/3000, Train Loss: 1.6977, Time: 1.1198029518127441\n",
      "Epoch 41/3000, Train Loss: 1.6742, Time: 1.1282563209533691\n",
      "Epoch 42/3000, Train Loss: 1.7097, Time: 1.1083910465240479\n",
      "Epoch 43/3000, Train Loss: 1.6801, Time: 1.1545522212982178\n",
      "Epoch 44/3000, Train Loss: 1.6966, Time: 1.1584322452545166\n",
      "Epoch 45/3000, Train Loss: 1.7400, Time: 1.1214430332183838\n",
      "Epoch 46/3000, Train Loss: 1.6824, Time: 1.1099863052368164\n",
      "Epoch 47/3000, Train Loss: 1.6603, Time: 1.1539199352264404\n",
      "Epoch 48/3000, Train Loss: 1.6586, Time: 1.1672096252441406\n",
      "Epoch 49/3000, Train Loss: 1.6164, Time: 1.0486204624176025\n",
      "Epoch 50/3000, Train Loss: 1.6458, Time: 1.044921875\n",
      "Epoch 51/3000, Train Loss: 1.5826, Time: 1.2072598934173584\n",
      "Epoch 52/3000, Train Loss: 1.5765, Time: 1.1005675792694092\n",
      "Epoch 53/3000, Train Loss: 1.5451, Time: 1.0269660949707031\n",
      "Epoch 54/3000, Train Loss: 1.4828, Time: 1.2121860980987549\n",
      "Epoch 55/3000, Train Loss: 1.5254, Time: 1.1012353897094727\n",
      "Epoch 56/3000, Train Loss: 1.5292, Time: 1.0103342533111572\n",
      "Epoch 57/3000, Train Loss: 1.5413, Time: 1.0633137226104736\n",
      "Epoch 58/3000, Train Loss: 1.4765, Time: 1.059217929840088\n",
      "Epoch 59/3000, Train Loss: 1.5173, Time: 1.092498779296875\n",
      "Epoch 60/3000, Train Loss: 1.4913, Time: 1.010101079940796\n",
      "Epoch 61/3000, Train Loss: 1.4419, Time: 1.004411220550537\n",
      "Epoch 62/3000, Train Loss: 1.4790, Time: 0.9977433681488037\n",
      "Epoch 63/3000, Train Loss: 1.4897, Time: 1.0005269050598145\n",
      "Epoch 64/3000, Train Loss: 1.4527, Time: 0.9931223392486572\n",
      "Epoch 65/3000, Train Loss: 1.4083, Time: 1.0030157566070557\n",
      "Epoch 66/3000, Train Loss: 1.4412, Time: 1.0825161933898926\n",
      "Epoch 67/3000, Train Loss: 1.4184, Time: 1.0457279682159424\n",
      "Epoch 68/3000, Train Loss: 1.4248, Time: 0.9784228801727295\n",
      "Epoch 69/3000, Train Loss: 1.3775, Time: 1.0601305961608887\n",
      "Epoch 70/3000, Train Loss: 1.4715, Time: 1.0509746074676514\n",
      "Epoch 71/3000, Train Loss: 1.4275, Time: 1.0043315887451172\n",
      "Epoch 72/3000, Train Loss: 1.4393, Time: 1.0379276275634766\n",
      "Epoch 73/3000, Train Loss: 1.4362, Time: 1.0255892276763916\n",
      "Epoch 74/3000, Train Loss: 1.4478, Time: 1.0243165493011475\n",
      "Epoch 75/3000, Train Loss: 1.4270, Time: 0.9922082424163818\n",
      "Epoch 76/3000, Train Loss: 1.5175, Time: 0.9931788444519043\n",
      "Epoch 77/3000, Train Loss: 1.4638, Time: 1.0147647857666016\n",
      "Epoch 78/3000, Train Loss: 1.4862, Time: 0.9911162853240967\n",
      "Epoch 79/3000, Train Loss: 1.4583, Time: 1.0411181449890137\n",
      "Epoch 80/3000, Train Loss: 1.4376, Time: 1.0043859481811523\n",
      "Epoch 81/3000, Train Loss: 1.6736, Time: 1.0262527465820312\n",
      "Epoch 82/3000, Train Loss: 1.3766, Time: 1.236417293548584\n",
      "Epoch 83/3000, Train Loss: 1.3572, Time: 1.2605102062225342\n",
      "Epoch 84/3000, Train Loss: 1.4437, Time: 1.2814559936523438\n",
      "Epoch 85/3000, Train Loss: 1.3670, Time: 1.239067792892456\n",
      "Epoch 86/3000, Train Loss: 1.3675, Time: 1.2364380359649658\n",
      "Epoch 87/3000, Train Loss: 1.4332, Time: 1.2399821281433105\n",
      "Epoch 88/3000, Train Loss: 1.4351, Time: 1.1800620555877686\n",
      "Epoch 89/3000, Train Loss: 1.3790, Time: 1.1385643482208252\n",
      "Epoch 90/3000, Train Loss: 1.3247, Time: 1.1411540508270264\n",
      "Epoch 91/3000, Train Loss: 1.3047, Time: 1.1372325420379639\n",
      "Epoch 92/3000, Train Loss: 1.3650, Time: 1.1376826763153076\n",
      "Epoch 93/3000, Train Loss: 1.3498, Time: 1.1686575412750244\n",
      "Epoch 94/3000, Train Loss: 1.4309, Time: 1.1704633235931396\n",
      "Epoch 95/3000, Train Loss: 1.4125, Time: 1.1299948692321777\n",
      "Epoch 96/3000, Train Loss: 1.2840, Time: 1.1245851516723633\n",
      "Epoch 97/3000, Train Loss: 1.3189, Time: 1.1122357845306396\n",
      "Epoch 98/3000, Train Loss: 1.3864, Time: 1.1117727756500244\n",
      "Epoch 99/3000, Train Loss: 1.3367, Time: 1.1488265991210938\n",
      "Epoch 100/3000, Train Loss: 1.2814, Time: 1.1659376621246338\n",
      "Epoch 101/3000, Train Loss: 1.3039, Time: 1.1809570789337158\n",
      "Epoch 102/3000, Train Loss: 1.3369, Time: 1.1316447257995605\n",
      "Epoch 103/3000, Train Loss: 1.3725, Time: 1.1261470317840576\n",
      "Epoch 104/3000, Train Loss: 1.3252, Time: 1.1093499660491943\n",
      "Epoch 105/3000, Train Loss: 1.3082, Time: 1.1117289066314697\n",
      "Epoch 106/3000, Train Loss: 1.3322, Time: 1.0771398544311523\n",
      "Epoch 107/3000, Train Loss: 1.3303, Time: 1.0557951927185059\n",
      "Epoch 108/3000, Train Loss: 1.3029, Time: 1.0931236743927002\n",
      "Epoch 109/3000, Train Loss: 1.3552, Time: 1.1180882453918457\n",
      "Epoch 110/3000, Train Loss: 1.3100, Time: 1.112999677658081\n",
      "Epoch 111/3000, Train Loss: 1.2782, Time: 1.1251914501190186\n",
      "Epoch 112/3000, Train Loss: 1.3414, Time: 1.1207776069641113\n",
      "Epoch 113/3000, Train Loss: 1.2948, Time: 1.1483213901519775\n",
      "Epoch 114/3000, Train Loss: 1.2829, Time: 1.1549475193023682\n",
      "Epoch 115/3000, Train Loss: 1.2813, Time: 1.121302843093872\n",
      "Epoch 116/3000, Train Loss: 1.2922, Time: 1.1166348457336426\n",
      "Epoch 117/3000, Train Loss: 1.2337, Time: 1.1244964599609375\n",
      "Epoch 118/3000, Train Loss: 1.2501, Time: 1.1173067092895508\n",
      "Epoch 119/3000, Train Loss: 1.2922, Time: 1.1331944465637207\n",
      "Epoch 120/3000, Train Loss: 1.2549, Time: 1.1304433345794678\n",
      "Epoch 121/3000, Train Loss: 1.2713, Time: 1.167938470840454\n",
      "Epoch 122/3000, Train Loss: 1.3505, Time: 1.1302292346954346\n",
      "Epoch 123/3000, Train Loss: 1.2872, Time: 1.1058056354522705\n",
      "Epoch 124/3000, Train Loss: 1.2622, Time: 1.2370026111602783\n",
      "Epoch 125/3000, Train Loss: 1.2822, Time: 1.133068323135376\n",
      "Epoch 126/3000, Train Loss: 1.2722, Time: 1.120025634765625\n",
      "Epoch 127/3000, Train Loss: 1.2473, Time: 1.1416280269622803\n",
      "Epoch 128/3000, Train Loss: 1.2737, Time: 1.1315233707427979\n",
      "Epoch 129/3000, Train Loss: 1.2480, Time: 1.1245675086975098\n",
      "Epoch 130/3000, Train Loss: 1.2455, Time: 1.1177339553833008\n",
      "Epoch 131/3000, Train Loss: 1.2410, Time: 1.1322083473205566\n",
      "Epoch 132/3000, Train Loss: 1.2976, Time: 1.098442792892456\n",
      "Epoch 133/3000, Train Loss: 1.2643, Time: 1.1668543815612793\n",
      "Epoch 134/3000, Train Loss: 1.3303, Time: 1.1588399410247803\n",
      "Epoch 135/3000, Train Loss: 1.2835, Time: 1.1626887321472168\n",
      "Epoch 136/3000, Train Loss: 1.2700, Time: 1.1247460842132568\n",
      "Epoch 137/3000, Train Loss: 1.1857, Time: 1.1267666816711426\n",
      "Epoch 138/3000, Train Loss: 1.2166, Time: 1.1342008113861084\n",
      "Epoch 139/3000, Train Loss: 1.6074, Time: 1.1175165176391602\n",
      "Epoch 140/3000, Train Loss: 1.3097, Time: 1.1331560611724854\n",
      "Epoch 141/3000, Train Loss: 1.2723, Time: 1.1355524063110352\n",
      "Epoch 142/3000, Train Loss: 1.2339, Time: 1.1401474475860596\n",
      "Epoch 143/3000, Train Loss: 1.2219, Time: 1.151139736175537\n",
      "Epoch 144/3000, Train Loss: 1.2380, Time: 1.126835584640503\n",
      "Epoch 145/3000, Train Loss: 1.1835, Time: 1.1337988376617432\n",
      "Epoch 146/3000, Train Loss: 1.3433, Time: 1.150329828262329\n",
      "Epoch 147/3000, Train Loss: 1.4467, Time: 1.1177630424499512\n",
      "Epoch 148/3000, Train Loss: 1.1755, Time: 1.1306560039520264\n",
      "Epoch 149/3000, Train Loss: 1.2017, Time: 1.161755084991455\n",
      "Epoch 150/3000, Train Loss: 1.1298, Time: 1.197514533996582\n",
      "Epoch 151/3000, Train Loss: 1.1967, Time: 1.1523065567016602\n",
      "Epoch 152/3000, Train Loss: 1.1942, Time: 1.1388516426086426\n",
      "Epoch 153/3000, Train Loss: 1.2049, Time: 1.1592698097229004\n",
      "Epoch 154/3000, Train Loss: 1.1471, Time: 1.109710454940796\n",
      "Epoch 155/3000, Train Loss: 1.1727, Time: 1.1022071838378906\n",
      "Epoch 156/3000, Train Loss: 1.1971, Time: 1.1153342723846436\n",
      "Epoch 157/3000, Train Loss: 1.2257, Time: 1.1303315162658691\n",
      "Epoch 158/3000, Train Loss: 1.2192, Time: 1.1059513092041016\n",
      "Epoch 159/3000, Train Loss: 1.1859, Time: 1.104271411895752\n",
      "Epoch 160/3000, Train Loss: 1.1648, Time: 1.1300389766693115\n",
      "Epoch 161/3000, Train Loss: 1.1587, Time: 1.107635259628296\n",
      "Epoch 162/3000, Train Loss: 1.2335, Time: 1.148514986038208\n",
      "Epoch 163/3000, Train Loss: 1.1791, Time: 1.1581203937530518\n",
      "Epoch 164/3000, Train Loss: 1.1658, Time: 1.1797733306884766\n",
      "Epoch 165/3000, Train Loss: 1.1676, Time: 1.1776447296142578\n",
      "Epoch 166/3000, Train Loss: 1.2541, Time: 1.1350698471069336\n",
      "Epoch 167/3000, Train Loss: 1.2670, Time: 1.1296908855438232\n",
      "Epoch 168/3000, Train Loss: 1.2711, Time: 1.132455587387085\n",
      "Epoch 169/3000, Train Loss: 1.1310, Time: 1.1321463584899902\n",
      "Epoch 170/3000, Train Loss: 1.1446, Time: 1.175973892211914\n",
      "Epoch 171/3000, Train Loss: 1.3249, Time: 1.1615760326385498\n",
      "Epoch 172/3000, Train Loss: 1.3039, Time: 1.1525278091430664\n",
      "Epoch 173/3000, Train Loss: 1.2819, Time: 1.1144778728485107\n",
      "Epoch 174/3000, Train Loss: 1.2698, Time: 0.9920680522918701\n",
      "Epoch 175/3000, Train Loss: 1.2594, Time: 0.9959814548492432\n",
      "Epoch 176/3000, Train Loss: 1.2453, Time: 1.3366382122039795\n",
      "Epoch 177/3000, Train Loss: 1.2326, Time: 1.2597863674163818\n",
      "Epoch 178/3000, Train Loss: 1.1634, Time: 1.058779001235962\n",
      "Epoch 179/3000, Train Loss: 1.2090, Time: 0.993889570236206\n",
      "Epoch 180/3000, Train Loss: 1.1656, Time: 0.9903848171234131\n",
      "Epoch 181/3000, Train Loss: 1.1950, Time: 1.0199823379516602\n",
      "Epoch 182/3000, Train Loss: 1.1818, Time: 0.9980440139770508\n",
      "Epoch 183/3000, Train Loss: 1.2247, Time: 1.0027148723602295\n",
      "Epoch 184/3000, Train Loss: 1.2262, Time: 1.0526916980743408\n",
      "Epoch 185/3000, Train Loss: 1.1812, Time: 1.1105010509490967\n",
      "Epoch 186/3000, Train Loss: 1.1796, Time: 1.0258538722991943\n",
      "Epoch 187/3000, Train Loss: 1.1910, Time: 1.0880401134490967\n",
      "Epoch 188/3000, Train Loss: 1.2127, Time: 1.1427042484283447\n",
      "Epoch 189/3000, Train Loss: 1.1472, Time: 1.0027170181274414\n",
      "Epoch 190/3000, Train Loss: 1.2432, Time: 1.0052940845489502\n",
      "Epoch 191/3000, Train Loss: 1.1218, Time: 1.011488437652588\n",
      "Epoch 192/3000, Train Loss: 1.4051, Time: 1.2238128185272217\n",
      "Epoch 193/3000, Train Loss: 1.1316, Time: 1.2892649173736572\n",
      "Epoch 194/3000, Train Loss: 1.1928, Time: 1.1364326477050781\n",
      "Epoch 195/3000, Train Loss: 1.1688, Time: 1.1909475326538086\n",
      "Epoch 196/3000, Train Loss: 1.2733, Time: 1.1294972896575928\n",
      "Epoch 197/3000, Train Loss: 1.2030, Time: 1.1427950859069824\n",
      "Epoch 198/3000, Train Loss: 1.1204, Time: 1.2417409420013428\n",
      "Epoch 199/3000, Train Loss: 1.0982, Time: 1.1411640644073486\n",
      "Epoch 200/3000, Train Loss: 1.0824, Time: 1.2303569316864014\n",
      "Epoch 201/3000, Train Loss: 1.1207, Time: 0.9896583557128906\n",
      "Epoch 202/3000, Train Loss: 1.1761, Time: 1.0525882244110107\n",
      "Epoch 203/3000, Train Loss: 1.1199, Time: 1.1605372428894043\n",
      "Epoch 204/3000, Train Loss: 1.1262, Time: 1.1871075630187988\n",
      "Epoch 205/3000, Train Loss: 1.1762, Time: 1.1080491542816162\n",
      "Epoch 206/3000, Train Loss: 1.1698, Time: 1.0071582794189453\n",
      "Epoch 207/3000, Train Loss: 1.0985, Time: 0.9917702674865723\n",
      "Epoch 208/3000, Train Loss: 1.1228, Time: 0.9939932823181152\n",
      "Epoch 209/3000, Train Loss: 1.2461, Time: 0.9991114139556885\n",
      "Epoch 210/3000, Train Loss: 1.1480, Time: 1.0317060947418213\n",
      "Epoch 211/3000, Train Loss: 1.2317, Time: 1.0403523445129395\n",
      "Epoch 212/3000, Train Loss: 1.1336, Time: 1.1023468971252441\n",
      "Epoch 213/3000, Train Loss: 1.1039, Time: 1.017853021621704\n",
      "Epoch 214/3000, Train Loss: 1.0913, Time: 0.9980447292327881\n",
      "Epoch 215/3000, Train Loss: 1.2096, Time: 0.99212646484375\n",
      "Epoch 216/3000, Train Loss: 1.1923, Time: 1.0028061866760254\n",
      "Epoch 217/3000, Train Loss: 1.1731, Time: 1.12424635887146\n",
      "Epoch 218/3000, Train Loss: 1.0859, Time: 1.0748555660247803\n",
      "Epoch 219/3000, Train Loss: 1.3308, Time: 1.0803167819976807\n",
      "Epoch 220/3000, Train Loss: 1.1506, Time: 1.0320343971252441\n",
      "Epoch 221/3000, Train Loss: 1.0840, Time: 1.0421645641326904\n",
      "Epoch 222/3000, Train Loss: 1.1470, Time: 1.1236140727996826\n",
      "Epoch 223/3000, Train Loss: 1.1066, Time: 1.1525359153747559\n",
      "Epoch 224/3000, Train Loss: 1.1254, Time: 1.1205904483795166\n",
      "Epoch 225/3000, Train Loss: 1.2866, Time: 1.1260192394256592\n",
      "Epoch 226/3000, Train Loss: 1.1114, Time: 1.168543815612793\n",
      "Epoch 227/3000, Train Loss: 1.1188, Time: 1.1427361965179443\n",
      "Epoch 228/3000, Train Loss: 1.4228, Time: 1.1244797706604004\n",
      "Epoch 229/3000, Train Loss: 1.2918, Time: 1.1472246646881104\n",
      "Epoch 230/3000, Train Loss: 1.1196, Time: 1.1279654502868652\n",
      "Epoch 231/3000, Train Loss: 1.1573, Time: 1.2472784519195557\n",
      "Epoch 232/3000, Train Loss: 1.0983, Time: 1.1521568298339844\n",
      "Epoch 233/3000, Train Loss: 1.1823, Time: 1.2898602485656738\n",
      "Epoch 234/3000, Train Loss: 1.1861, Time: 1.140883445739746\n",
      "Epoch 235/3000, Train Loss: 1.1234, Time: 1.1040613651275635\n",
      "Epoch 236/3000, Train Loss: 1.0957, Time: 1.1091101169586182\n",
      "Epoch 237/3000, Train Loss: 1.1183, Time: 1.112684726715088\n",
      "Epoch 238/3000, Train Loss: 1.3083, Time: 1.0138843059539795\n",
      "Epoch 239/3000, Train Loss: 1.0895, Time: 1.0229475498199463\n",
      "Epoch 240/3000, Train Loss: 1.1058, Time: 1.0462462902069092\n",
      "Epoch 241/3000, Train Loss: 1.4178, Time: 1.0107390880584717\n",
      "Epoch 242/3000, Train Loss: 1.1648, Time: 1.1039855480194092\n",
      "Epoch 243/3000, Train Loss: 1.1289, Time: 1.0003490447998047\n",
      "Epoch 244/3000, Train Loss: 1.0924, Time: 0.9761571884155273\n",
      "Epoch 245/3000, Train Loss: 1.1855, Time: 0.9739937782287598\n",
      "Epoch 246/3000, Train Loss: 1.1641, Time: 0.9841628074645996\n",
      "Epoch 247/3000, Train Loss: 1.1240, Time: 1.1361286640167236\n",
      "Epoch 248/3000, Train Loss: 1.1109, Time: 1.1086928844451904\n",
      "Epoch 249/3000, Train Loss: 1.1888, Time: 1.0001561641693115\n",
      "Epoch 250/3000, Train Loss: 1.1933, Time: 0.9825184345245361\n",
      "Epoch 251/3000, Train Loss: 1.1321, Time: 0.9790215492248535\n",
      "Epoch 252/3000, Train Loss: 1.0775, Time: 1.013639211654663\n",
      "Epoch 253/3000, Train Loss: 1.1131, Time: 0.9881420135498047\n",
      "Epoch 254/3000, Train Loss: 1.1769, Time: 0.9842321872711182\n",
      "Epoch 255/3000, Train Loss: 1.0682, Time: 1.0064737796783447\n",
      "Epoch 256/3000, Train Loss: 1.0542, Time: 1.0217580795288086\n",
      "Epoch 257/3000, Train Loss: 1.0690, Time: 1.0214009284973145\n",
      "Epoch 258/3000, Train Loss: 1.0038, Time: 1.0295708179473877\n",
      "Epoch 259/3000, Train Loss: 1.0493, Time: 1.0129780769348145\n",
      "Epoch 260/3000, Train Loss: 1.0694, Time: 1.0659313201904297\n",
      "Epoch 261/3000, Train Loss: 1.3618, Time: 0.990401029586792\n",
      "Epoch 262/3000, Train Loss: 1.4513, Time: 0.9919219017028809\n",
      "Epoch 263/3000, Train Loss: 1.0939, Time: 1.0913677215576172\n",
      "Epoch 264/3000, Train Loss: 1.0533, Time: 1.132176160812378\n",
      "Epoch 265/3000, Train Loss: 1.0322, Time: 1.1324334144592285\n",
      "Epoch 266/3000, Train Loss: 1.0105, Time: 1.0271172523498535\n",
      "Epoch 267/3000, Train Loss: 1.0015, Time: 1.0218791961669922\n",
      "Epoch 268/3000, Train Loss: 1.0781, Time: 0.9912879467010498\n",
      "Epoch 269/3000, Train Loss: 1.1272, Time: 0.9803025722503662\n",
      "Epoch 270/3000, Train Loss: 1.0522, Time: 0.986119270324707\n",
      "Epoch 271/3000, Train Loss: 0.9968, Time: 1.0380439758300781\n",
      "Epoch 272/3000, Train Loss: 1.0438, Time: 1.0975325107574463\n",
      "Epoch 273/3000, Train Loss: 1.1968, Time: 1.1112120151519775\n",
      "Epoch 274/3000, Train Loss: 1.1276, Time: 1.0103087425231934\n",
      "Epoch 275/3000, Train Loss: 1.1028, Time: 0.9898934364318848\n",
      "Epoch 276/3000, Train Loss: 1.2620, Time: 0.9887199401855469\n",
      "Epoch 277/3000, Train Loss: 1.0164, Time: 0.9879400730133057\n",
      "Epoch 278/3000, Train Loss: 1.0261, Time: 0.9931797981262207\n",
      "Epoch 279/3000, Train Loss: 1.1738, Time: 1.0057096481323242\n",
      "Epoch 280/3000, Train Loss: 0.9943, Time: 1.1394789218902588\n",
      "Epoch 281/3000, Train Loss: 1.0948, Time: 1.0922300815582275\n",
      "Epoch 282/3000, Train Loss: 1.0941, Time: 1.0133202075958252\n",
      "Epoch 283/3000, Train Loss: 0.9681, Time: 0.9949662685394287\n",
      "Epoch 284/3000, Train Loss: 1.0873, Time: 0.986149787902832\n",
      "Epoch 285/3000, Train Loss: 1.0066, Time: 0.9818484783172607\n",
      "Epoch 286/3000, Train Loss: 1.0418, Time: 0.9828283786773682\n",
      "Epoch 287/3000, Train Loss: 0.9754, Time: 0.9855055809020996\n",
      "Epoch 288/3000, Train Loss: 1.0302, Time: 1.095219373703003\n",
      "Epoch 289/3000, Train Loss: 1.0533, Time: 1.175140619277954\n",
      "Epoch 290/3000, Train Loss: 1.0003, Time: 1.101163387298584\n",
      "Epoch 291/3000, Train Loss: 1.0773, Time: 1.0213596820831299\n",
      "Epoch 292/3000, Train Loss: 1.0034, Time: 0.986903190612793\n",
      "Epoch 293/3000, Train Loss: 1.1750, Time: 0.9863629341125488\n",
      "Epoch 294/3000, Train Loss: 0.9632, Time: 1.0221550464630127\n",
      "Epoch 295/3000, Train Loss: 1.0245, Time: 1.0788164138793945\n",
      "Epoch 296/3000, Train Loss: 0.9577, Time: 1.116051197052002\n",
      "Epoch 297/3000, Train Loss: 0.9386, Time: 1.1218552589416504\n",
      "Epoch 298/3000, Train Loss: 1.0423, Time: 1.0582799911499023\n",
      "Epoch 299/3000, Train Loss: 1.0253, Time: 1.0159862041473389\n",
      "Epoch 300/3000, Train Loss: 1.0696, Time: 1.0274431705474854\n",
      "Epoch 301/3000, Train Loss: 0.9849, Time: 1.0283005237579346\n",
      "Epoch 302/3000, Train Loss: 0.9545, Time: 1.066521167755127\n",
      "Epoch 303/3000, Train Loss: 1.0053, Time: 1.0172955989837646\n",
      "Epoch 304/3000, Train Loss: 1.0550, Time: 0.9943351745605469\n",
      "Epoch 305/3000, Train Loss: 1.0498, Time: 0.9875431060791016\n",
      "Epoch 306/3000, Train Loss: 1.0471, Time: 0.9881806373596191\n",
      "Epoch 307/3000, Train Loss: 1.0205, Time: 1.0142974853515625\n",
      "Epoch 308/3000, Train Loss: 1.0087, Time: 1.0707571506500244\n",
      "Epoch 309/3000, Train Loss: 1.0578, Time: 1.0215733051300049\n",
      "Epoch 310/3000, Train Loss: 1.0142, Time: 1.0016520023345947\n",
      "Epoch 311/3000, Train Loss: 1.0177, Time: 1.0060832500457764\n",
      "Epoch 312/3000, Train Loss: 1.1083, Time: 1.0051994323730469\n",
      "Epoch 313/3000, Train Loss: 0.9967, Time: 0.9900658130645752\n",
      "Epoch 314/3000, Train Loss: 1.0056, Time: 1.022641897201538\n",
      "Epoch 315/3000, Train Loss: 0.9700, Time: 1.089237928390503\n",
      "Epoch 316/3000, Train Loss: 0.9792, Time: 1.055955171585083\n",
      "Epoch 317/3000, Train Loss: 0.9382, Time: 0.9945573806762695\n",
      "Epoch 318/3000, Train Loss: 0.9427, Time: 0.9958658218383789\n",
      "Epoch 319/3000, Train Loss: 0.9921, Time: 0.9923641681671143\n",
      "Epoch 320/3000, Train Loss: 0.9698, Time: 0.9961609840393066\n",
      "Epoch 321/3000, Train Loss: 0.9375, Time: 1.062985897064209\n",
      "Epoch 322/3000, Train Loss: 0.9359, Time: 1.0920982360839844\n",
      "Epoch 323/3000, Train Loss: 0.9662, Time: 1.0891900062561035\n",
      "Epoch 324/3000, Train Loss: 0.9479, Time: 0.9977712631225586\n",
      "Epoch 325/3000, Train Loss: 0.8996, Time: 1.005887508392334\n",
      "Epoch 326/3000, Train Loss: 0.8805, Time: 1.0074779987335205\n",
      "Epoch 327/3000, Train Loss: 1.0684, Time: 0.9915635585784912\n",
      "Epoch 328/3000, Train Loss: 1.4230, Time: 0.9850482940673828\n",
      "Epoch 329/3000, Train Loss: 1.0269, Time: 1.0113334655761719\n",
      "Epoch 330/3000, Train Loss: 1.0172, Time: 1.149179458618164\n",
      "Epoch 331/3000, Train Loss: 1.0041, Time: 1.0591349601745605\n",
      "Epoch 332/3000, Train Loss: 1.0594, Time: 0.9984254837036133\n",
      "Epoch 333/3000, Train Loss: 0.9581, Time: 0.9799540042877197\n",
      "Epoch 334/3000, Train Loss: 0.9672, Time: 0.9819703102111816\n",
      "Epoch 335/3000, Train Loss: 1.0776, Time: 0.9860103130340576\n",
      "Epoch 336/3000, Train Loss: 1.1188, Time: 1.0929901599884033\n",
      "Epoch 337/3000, Train Loss: 0.9223, Time: 1.163562297821045\n",
      "Epoch 338/3000, Train Loss: 1.1766, Time: 1.1519274711608887\n",
      "Epoch 339/3000, Train Loss: 1.2956, Time: 1.1464943885803223\n",
      "Epoch 340/3000, Train Loss: 0.9651, Time: 1.1098015308380127\n",
      "Epoch 341/3000, Train Loss: 0.9295, Time: 1.1120898723602295\n",
      "Epoch 342/3000, Train Loss: 0.9191, Time: 1.1284759044647217\n",
      "Epoch 343/3000, Train Loss: 1.0518, Time: 1.1364777088165283\n",
      "Epoch 344/3000, Train Loss: 0.9735, Time: 1.1335675716400146\n",
      "Epoch 345/3000, Train Loss: 0.9143, Time: 1.1104192733764648\n",
      "Epoch 346/3000, Train Loss: 1.0067, Time: 1.1950876712799072\n",
      "Epoch 347/3000, Train Loss: 0.9554, Time: 1.1807432174682617\n",
      "Epoch 348/3000, Train Loss: 1.0757, Time: 1.1010122299194336\n",
      "Epoch 349/3000, Train Loss: 1.1997, Time: 1.1197841167449951\n",
      "Epoch 350/3000, Train Loss: 1.2647, Time: 1.1306657791137695\n",
      "Epoch 351/3000, Train Loss: 1.0318, Time: 1.1130871772766113\n",
      "Epoch 352/3000, Train Loss: 1.0162, Time: 1.1049742698669434\n",
      "Epoch 353/3000, Train Loss: 0.9272, Time: 1.1021363735198975\n",
      "Epoch 354/3000, Train Loss: 0.9197, Time: 1.1092827320098877\n",
      "Epoch 355/3000, Train Loss: 0.9562, Time: 1.1066396236419678\n",
      "Epoch 356/3000, Train Loss: 1.0532, Time: 1.154259443283081\n",
      "Epoch 357/3000, Train Loss: 0.9766, Time: 1.1411371231079102\n",
      "Epoch 358/3000, Train Loss: 0.9485, Time: 1.1294550895690918\n",
      "Epoch 359/3000, Train Loss: 0.9806, Time: 1.1054797172546387\n",
      "Epoch 360/3000, Train Loss: 1.0325, Time: 1.1450679302215576\n",
      "Epoch 361/3000, Train Loss: 1.1605, Time: 1.1156589984893799\n",
      "Epoch 362/3000, Train Loss: 0.9624, Time: 1.1142449378967285\n",
      "Epoch 363/3000, Train Loss: 0.9693, Time: 1.1248762607574463\n",
      "Epoch 364/3000, Train Loss: 0.9209, Time: 1.1412925720214844\n",
      "Epoch 365/3000, Train Loss: 0.9317, Time: 1.1320104598999023\n",
      "Epoch 366/3000, Train Loss: 0.9049, Time: 1.1360828876495361\n",
      "Epoch 367/3000, Train Loss: 0.9204, Time: 1.0953502655029297\n",
      "Epoch 368/3000, Train Loss: 0.9277, Time: 1.0978772640228271\n",
      "Epoch 369/3000, Train Loss: 0.9833, Time: 1.0982253551483154\n",
      "Epoch 370/3000, Train Loss: 1.1007, Time: 1.1096282005310059\n",
      "Epoch 371/3000, Train Loss: 1.2084, Time: 1.132417917251587\n",
      "Epoch 372/3000, Train Loss: 1.0157, Time: 1.1429662704467773\n",
      "Epoch 373/3000, Train Loss: 1.0797, Time: 1.12394380569458\n",
      "Epoch 374/3000, Train Loss: 1.0198, Time: 1.1003663539886475\n",
      "Epoch 375/3000, Train Loss: 0.9541, Time: 1.0962445735931396\n",
      "Epoch 376/3000, Train Loss: 0.9510, Time: 1.1035473346710205\n",
      "Epoch 377/3000, Train Loss: 1.0274, Time: 1.1062145233154297\n",
      "Epoch 378/3000, Train Loss: 0.9715, Time: 1.1025607585906982\n",
      "Epoch 379/3000, Train Loss: 0.9976, Time: 1.1033191680908203\n",
      "Epoch 380/3000, Train Loss: 0.9124, Time: 1.106048345565796\n",
      "Epoch 381/3000, Train Loss: 1.1061, Time: 1.1041193008422852\n",
      "Epoch 382/3000, Train Loss: 0.9665, Time: 1.106339693069458\n",
      "Epoch 383/3000, Train Loss: 0.9416, Time: 1.101797342300415\n",
      "Epoch 384/3000, Train Loss: 0.9339, Time: 1.1071159839630127\n",
      "Epoch 385/3000, Train Loss: 1.0273, Time: 1.1389825344085693\n",
      "Epoch 386/3000, Train Loss: 1.0231, Time: 1.13492751121521\n",
      "Epoch 387/3000, Train Loss: 0.9255, Time: 1.1095812320709229\n",
      "Epoch 388/3000, Train Loss: 0.9279, Time: 1.1373393535614014\n",
      "Epoch 389/3000, Train Loss: 0.9651, Time: 1.098914384841919\n",
      "Epoch 390/3000, Train Loss: 1.0105, Time: 1.1311454772949219\n",
      "Epoch 391/3000, Train Loss: 0.9892, Time: 1.1606850624084473\n",
      "Epoch 392/3000, Train Loss: 0.9759, Time: 1.1149060726165771\n",
      "Epoch 393/3000, Train Loss: 0.9667, Time: 1.1088974475860596\n",
      "Epoch 394/3000, Train Loss: 0.9193, Time: 1.1028892993927002\n",
      "Epoch 395/3000, Train Loss: 1.0098, Time: 1.1130642890930176\n",
      "Epoch 396/3000, Train Loss: 0.9564, Time: 1.128046989440918\n",
      "Epoch 397/3000, Train Loss: 1.2334, Time: 1.1166718006134033\n",
      "Epoch 398/3000, Train Loss: 1.0146, Time: 1.108335256576538\n",
      "Epoch 399/3000, Train Loss: 1.0174, Time: 1.1138341426849365\n",
      "Epoch 400/3000, Train Loss: 1.0262, Time: 1.2219383716583252\n",
      "Epoch 401/3000, Train Loss: 0.9893, Time: 1.1810941696166992\n",
      "Epoch 402/3000, Train Loss: 0.9967, Time: 1.1076781749725342\n",
      "Epoch 403/3000, Train Loss: 0.9797, Time: 1.1050970554351807\n",
      "Epoch 404/3000, Train Loss: 0.9155, Time: 1.070892572402954\n",
      "Epoch 405/3000, Train Loss: 0.9158, Time: 0.9829878807067871\n",
      "Epoch 406/3000, Train Loss: 1.0451, Time: 0.9912183284759521\n",
      "Epoch 407/3000, Train Loss: 0.9404, Time: 0.9968206882476807\n",
      "Epoch 408/3000, Train Loss: 0.9373, Time: 0.9977920055389404\n",
      "Epoch 409/3000, Train Loss: 1.0594, Time: 0.9876165390014648\n",
      "Epoch 410/3000, Train Loss: 0.9866, Time: 0.9908919334411621\n",
      "Epoch 411/3000, Train Loss: 0.9337, Time: 0.9954173564910889\n",
      "Epoch 412/3000, Train Loss: 0.9292, Time: 0.9869246482849121\n",
      "Epoch 413/3000, Train Loss: 0.9176, Time: 0.9907741546630859\n",
      "Epoch 414/3000, Train Loss: 0.9525, Time: 1.0113060474395752\n",
      "Epoch 415/3000, Train Loss: 0.9278, Time: 1.008741855621338\n",
      "Epoch 416/3000, Train Loss: 0.8915, Time: 0.9968862533569336\n",
      "Epoch 417/3000, Train Loss: 0.9398, Time: 0.9871025085449219\n",
      "Epoch 418/3000, Train Loss: 0.8817, Time: 0.9997615814208984\n",
      "Epoch 419/3000, Train Loss: 0.9855, Time: 0.9822285175323486\n",
      "Epoch 420/3000, Train Loss: 0.8961, Time: 0.983781099319458\n",
      "Epoch 421/3000, Train Loss: 0.9255, Time: 0.9828364849090576\n",
      "Epoch 422/3000, Train Loss: 0.9673, Time: 0.9869325160980225\n",
      "Epoch 423/3000, Train Loss: 1.0186, Time: 1.0232453346252441\n",
      "Epoch 424/3000, Train Loss: 0.9382, Time: 1.0126328468322754\n",
      "Epoch 425/3000, Train Loss: 1.0470, Time: 1.0036003589630127\n",
      "Epoch 426/3000, Train Loss: 0.8874, Time: 0.9925847053527832\n",
      "Epoch 427/3000, Train Loss: 0.9391, Time: 1.006765365600586\n",
      "Epoch 428/3000, Train Loss: 0.9068, Time: 0.982764720916748\n",
      "Epoch 429/3000, Train Loss: 1.1878, Time: 0.9851546287536621\n",
      "Epoch 430/3000, Train Loss: 1.1698, Time: 0.982377290725708\n",
      "Epoch 431/3000, Train Loss: 0.9046, Time: 0.9970684051513672\n",
      "Epoch 432/3000, Train Loss: 0.9596, Time: 1.0110764503479004\n",
      "Epoch 433/3000, Train Loss: 0.9267, Time: 0.9976963996887207\n",
      "Epoch 434/3000, Train Loss: 1.0837, Time: 0.9835195541381836\n",
      "Epoch 435/3000, Train Loss: 0.9618, Time: 0.9831032752990723\n",
      "Epoch 436/3000, Train Loss: 0.9204, Time: 0.9849820137023926\n",
      "Epoch 437/3000, Train Loss: 0.8784, Time: 0.9924554824829102\n",
      "Epoch 438/3000, Train Loss: 0.9814, Time: 1.0325090885162354\n",
      "Epoch 439/3000, Train Loss: 0.9684, Time: 1.023838996887207\n",
      "Epoch 440/3000, Train Loss: 0.9564, Time: 1.0129568576812744\n",
      "Epoch 441/3000, Train Loss: 1.0969, Time: 1.0060772895812988\n",
      "Epoch 442/3000, Train Loss: 1.0679, Time: 0.9855992794036865\n",
      "Epoch 443/3000, Train Loss: 0.9185, Time: 0.9925642013549805\n",
      "Epoch 444/3000, Train Loss: 0.8947, Time: 0.9904274940490723\n",
      "Epoch 445/3000, Train Loss: 0.9030, Time: 1.023313045501709\n",
      "Epoch 446/3000, Train Loss: 0.8678, Time: 1.018674373626709\n",
      "Epoch 447/3000, Train Loss: 0.9578, Time: 0.994081974029541\n",
      "Epoch 448/3000, Train Loss: 0.9430, Time: 0.9903323650360107\n",
      "Epoch 449/3000, Train Loss: 0.9471, Time: 1.0072617530822754\n",
      "Epoch 450/3000, Train Loss: 0.9302, Time: 1.0202827453613281\n",
      "Epoch 451/3000, Train Loss: 0.9795, Time: 1.021622657775879\n",
      "Epoch 452/3000, Train Loss: 0.9782, Time: 0.9988584518432617\n",
      "Epoch 453/3000, Train Loss: 0.8683, Time: 0.9896509647369385\n",
      "Epoch 454/3000, Train Loss: 0.9231, Time: 0.986661434173584\n",
      "Epoch 455/3000, Train Loss: 0.9001, Time: 1.0027015209197998\n",
      "Epoch 456/3000, Train Loss: 0.8842, Time: 1.0240545272827148\n",
      "Epoch 457/3000, Train Loss: 0.9591, Time: 1.01570725440979\n",
      "Epoch 458/3000, Train Loss: 0.9620, Time: 1.0263183116912842\n",
      "Epoch 459/3000, Train Loss: 0.9567, Time: 1.0226902961730957\n",
      "Epoch 460/3000, Train Loss: 0.9244, Time: 1.0674266815185547\n",
      "Epoch 461/3000, Train Loss: 1.0107, Time: 1.0159072875976562\n",
      "Epoch 462/3000, Train Loss: 0.9457, Time: 1.0109186172485352\n",
      "Epoch 463/3000, Train Loss: 0.8762, Time: 0.9986433982849121\n",
      "Epoch 464/3000, Train Loss: 0.8723, Time: 0.992293119430542\n",
      "Epoch 465/3000, Train Loss: 0.9759, Time: 0.9920306205749512\n",
      "Epoch 466/3000, Train Loss: 0.9600, Time: 0.9963650703430176\n",
      "Epoch 467/3000, Train Loss: 1.0724, Time: 1.0086147785186768\n",
      "Epoch 468/3000, Train Loss: 0.9452, Time: 1.0268678665161133\n",
      "Epoch 469/3000, Train Loss: 1.0252, Time: 0.9917213916778564\n",
      "Epoch 470/3000, Train Loss: 1.0731, Time: 0.9859330654144287\n",
      "Epoch 471/3000, Train Loss: 0.9423, Time: 1.0163686275482178\n",
      "Epoch 472/3000, Train Loss: 0.9153, Time: 1.0028727054595947\n",
      "Epoch 473/3000, Train Loss: 0.9688, Time: 1.015852451324463\n",
      "Epoch 474/3000, Train Loss: 0.8885, Time: 1.0728659629821777\n",
      "Epoch 475/3000, Train Loss: 0.8918, Time: 1.008840799331665\n",
      "Epoch 476/3000, Train Loss: 0.8733, Time: 0.9863700866699219\n",
      "Epoch 477/3000, Train Loss: 0.9996, Time: 0.9719080924987793\n",
      "Epoch 478/3000, Train Loss: 0.9919, Time: 0.9800586700439453\n",
      "Epoch 479/3000, Train Loss: 0.9175, Time: 0.9792122840881348\n",
      "Epoch 480/3000, Train Loss: 1.1515, Time: 0.974707841873169\n",
      "Epoch 481/3000, Train Loss: 0.9764, Time: 0.988839864730835\n",
      "Epoch 482/3000, Train Loss: 0.8669, Time: 1.0115704536437988\n",
      "Epoch 483/3000, Train Loss: 0.9614, Time: 1.009155035018921\n",
      "Epoch 484/3000, Train Loss: 0.9900, Time: 0.9918930530548096\n",
      "Epoch 485/3000, Train Loss: 0.8558, Time: 0.9863958358764648\n",
      "Epoch 486/3000, Train Loss: 0.8781, Time: 0.9809410572052002\n",
      "Epoch 487/3000, Train Loss: 0.8572, Time: 0.9752287864685059\n",
      "Epoch 488/3000, Train Loss: 0.9097, Time: 0.9759073257446289\n",
      "Epoch 489/3000, Train Loss: 1.1580, Time: 0.9798707962036133\n",
      "Epoch 490/3000, Train Loss: 0.9756, Time: 0.9952337741851807\n",
      "Epoch 491/3000, Train Loss: 1.0542, Time: 1.0014517307281494\n",
      "Epoch 492/3000, Train Loss: 0.8729, Time: 1.004279613494873\n",
      "Epoch 493/3000, Train Loss: 0.9138, Time: 0.9802613258361816\n",
      "Epoch 494/3000, Train Loss: 1.0555, Time: 0.9832966327667236\n",
      "Epoch 495/3000, Train Loss: 0.9194, Time: 0.9807846546173096\n",
      "Epoch 496/3000, Train Loss: 0.9133, Time: 0.9790492057800293\n",
      "Epoch 497/3000, Train Loss: 0.9780, Time: 0.9778544902801514\n",
      "Epoch 498/3000, Train Loss: 0.8934, Time: 0.9926130771636963\n",
      "Epoch 499/3000, Train Loss: 0.8851, Time: 1.0233135223388672\n",
      "Epoch 500/3000, Train Loss: 0.9352, Time: 1.0211691856384277\n",
      "Epoch 501/3000, Train Loss: 0.9419, Time: 0.9810333251953125\n",
      "Epoch 502/3000, Train Loss: 0.8995, Time: 0.9829633235931396\n",
      "Epoch 503/3000, Train Loss: 0.8805, Time: 0.9809222221374512\n",
      "Epoch 504/3000, Train Loss: 0.9286, Time: 0.9767954349517822\n",
      "Epoch 505/3000, Train Loss: 0.9663, Time: 0.9747087955474854\n",
      "Epoch 506/3000, Train Loss: 0.8473, Time: 0.9841973781585693\n",
      "Epoch 507/3000, Train Loss: 0.9527, Time: 1.0084233283996582\n",
      "Epoch 508/3000, Train Loss: 0.9152, Time: 1.0034327507019043\n",
      "Epoch 509/3000, Train Loss: 0.9056, Time: 0.9885597229003906\n",
      "Epoch 510/3000, Train Loss: 0.9777, Time: 0.9886322021484375\n",
      "Epoch 511/3000, Train Loss: 0.9545, Time: 0.9888999462127686\n",
      "Epoch 512/3000, Train Loss: 0.9120, Time: 0.9894061088562012\n",
      "Epoch 513/3000, Train Loss: 0.9364, Time: 0.9862051010131836\n",
      "Epoch 514/3000, Train Loss: 0.9136, Time: 0.9884462356567383\n",
      "Epoch 515/3000, Train Loss: 0.8611, Time: 1.0070571899414062\n",
      "Epoch 516/3000, Train Loss: 0.9613, Time: 1.057004690170288\n",
      "Epoch 517/3000, Train Loss: 1.1687, Time: 1.038769245147705\n",
      "Epoch 518/3000, Train Loss: 1.0094, Time: 0.9971528053283691\n",
      "Epoch 519/3000, Train Loss: 0.8664, Time: 1.1104168891906738\n",
      "Epoch 520/3000, Train Loss: 0.8593, Time: 1.0496633052825928\n",
      "Epoch 521/3000, Train Loss: 0.9187, Time: 0.993586540222168\n",
      "Epoch 522/3000, Train Loss: 0.8746, Time: 0.9987256526947021\n",
      "Epoch 523/3000, Train Loss: 0.9196, Time: 1.0009212493896484\n",
      "Epoch 524/3000, Train Loss: 0.9546, Time: 1.0118744373321533\n",
      "Epoch 525/3000, Train Loss: 0.8924, Time: 1.0229549407958984\n",
      "Epoch 526/3000, Train Loss: 0.9112, Time: 1.0216410160064697\n",
      "Epoch 527/3000, Train Loss: 1.0208, Time: 0.9849627017974854\n",
      "Epoch 528/3000, Train Loss: 0.9260, Time: 0.9836373329162598\n",
      "Epoch 529/3000, Train Loss: 0.8928, Time: 0.981217622756958\n",
      "Epoch 530/3000, Train Loss: 0.9555, Time: 1.003511905670166\n",
      "Epoch 531/3000, Train Loss: 1.1085, Time: 0.97900390625\n",
      "Epoch 532/3000, Train Loss: 0.9366, Time: 0.9919469356536865\n",
      "Epoch 533/3000, Train Loss: 0.8372, Time: 1.0182650089263916\n",
      "Epoch 534/3000, Train Loss: 0.8590, Time: 1.019031286239624\n",
      "Epoch 535/3000, Train Loss: 1.0081, Time: 1.012772560119629\n",
      "Epoch 536/3000, Train Loss: 0.9421, Time: 0.9789495468139648\n",
      "Epoch 537/3000, Train Loss: 0.9067, Time: 0.9789495468139648\n",
      "Epoch 538/3000, Train Loss: 0.9637, Time: 0.986863374710083\n",
      "Epoch 539/3000, Train Loss: 0.9151, Time: 0.9909212589263916\n",
      "Epoch 540/3000, Train Loss: 0.8797, Time: 0.9897606372833252\n",
      "Epoch 541/3000, Train Loss: 0.8776, Time: 1.0172438621520996\n",
      "Epoch 542/3000, Train Loss: 0.9022, Time: 1.0150415897369385\n",
      "Epoch 543/3000, Train Loss: 0.8472, Time: 1.009758710861206\n",
      "Epoch 544/3000, Train Loss: 0.8767, Time: 1.000797986984253\n",
      "Epoch 545/3000, Train Loss: 0.8618, Time: 1.0001823902130127\n",
      "Epoch 546/3000, Train Loss: 0.8570, Time: 1.0295836925506592\n",
      "Epoch 547/3000, Train Loss: 0.9010, Time: 1.0138213634490967\n",
      "Epoch 548/3000, Train Loss: 0.9997, Time: 0.989415168762207\n",
      "Epoch 549/3000, Train Loss: 0.9758, Time: 0.987462043762207\n",
      "Epoch 550/3000, Train Loss: 1.0443, Time: 0.984804630279541\n",
      "Epoch 551/3000, Train Loss: 0.8817, Time: 0.9894039630889893\n",
      "Epoch 552/3000, Train Loss: 0.8926, Time: 0.9929900169372559\n",
      "Epoch 553/3000, Train Loss: 0.8646, Time: 1.0234382152557373\n",
      "Epoch 554/3000, Train Loss: 0.8513, Time: 1.0153048038482666\n",
      "Epoch 555/3000, Train Loss: 0.9419, Time: 1.0146698951721191\n",
      "Epoch 556/3000, Train Loss: 0.9907, Time: 0.9890482425689697\n",
      "Epoch 557/3000, Train Loss: 0.9719, Time: 0.9913866519927979\n",
      "Epoch 558/3000, Train Loss: 0.8736, Time: 0.9938263893127441\n",
      "Epoch 559/3000, Train Loss: 0.8976, Time: 0.9952592849731445\n",
      "Epoch 560/3000, Train Loss: 0.9010, Time: 0.9879026412963867\n",
      "Epoch 561/3000, Train Loss: 0.9807, Time: 0.994236946105957\n",
      "Epoch 562/3000, Train Loss: 0.8765, Time: 1.0186331272125244\n",
      "Epoch 563/3000, Train Loss: 0.8405, Time: 1.0126457214355469\n",
      "Epoch 564/3000, Train Loss: 1.2070, Time: 0.9929070472717285\n",
      "Epoch 565/3000, Train Loss: 1.0012, Time: 0.9998178482055664\n",
      "Epoch 566/3000, Train Loss: 0.8232, Time: 1.0111865997314453\n",
      "Epoch 567/3000, Train Loss: 0.8745, Time: 1.004326343536377\n",
      "Epoch 568/3000, Train Loss: 0.9098, Time: 1.0028867721557617\n",
      "Epoch 569/3000, Train Loss: 0.9526, Time: 1.027252435684204\n",
      "Epoch 570/3000, Train Loss: 0.8494, Time: 1.0281329154968262\n",
      "Epoch 571/3000, Train Loss: 0.8479, Time: 1.007028341293335\n",
      "Epoch 572/3000, Train Loss: 0.9525, Time: 1.0062201023101807\n",
      "Epoch 573/3000, Train Loss: 0.9202, Time: 1.0013923645019531\n",
      "Epoch 574/3000, Train Loss: 0.8551, Time: 1.0150935649871826\n",
      "Epoch 575/3000, Train Loss: 0.9289, Time: 1.0193266868591309\n",
      "Epoch 576/3000, Train Loss: 1.0682, Time: 1.0193943977355957\n",
      "Epoch 577/3000, Train Loss: 0.9055, Time: 1.0029287338256836\n",
      "Epoch 578/3000, Train Loss: 0.9103, Time: 1.1554572582244873\n",
      "Epoch 579/3000, Train Loss: 0.9365, Time: 1.0142879486083984\n",
      "Epoch 580/3000, Train Loss: 1.0666, Time: 1.0143141746520996\n",
      "Epoch 581/3000, Train Loss: 0.9199, Time: 1.0027356147766113\n",
      "Epoch 582/3000, Train Loss: 0.8770, Time: 0.9852719306945801\n",
      "Epoch 583/3000, Train Loss: 0.8776, Time: 0.987962007522583\n",
      "Epoch 584/3000, Train Loss: 0.9934, Time: 0.9908804893493652\n",
      "Epoch 585/3000, Train Loss: 0.9702, Time: 0.988959789276123\n",
      "Epoch 586/3000, Train Loss: 0.9394, Time: 1.0123298168182373\n",
      "Epoch 587/3000, Train Loss: 0.9135, Time: 1.0177643299102783\n",
      "Epoch 588/3000, Train Loss: 0.9379, Time: 1.006134271621704\n",
      "Epoch 589/3000, Train Loss: 0.9845, Time: 0.9883406162261963\n",
      "Epoch 590/3000, Train Loss: 0.8844, Time: 0.9945590496063232\n",
      "Epoch 591/3000, Train Loss: 0.9733, Time: 1.0336456298828125\n",
      "Epoch 592/3000, Train Loss: 0.8655, Time: 0.9998085498809814\n",
      "Epoch 593/3000, Train Loss: 0.8746, Time: 1.018622636795044\n",
      "Epoch 594/3000, Train Loss: 0.8662, Time: 1.0220844745635986\n",
      "Epoch 595/3000, Train Loss: 0.9437, Time: 1.101994276046753\n",
      "Epoch 596/3000, Train Loss: 0.8426, Time: 1.0503640174865723\n",
      "Epoch 597/3000, Train Loss: 0.8841, Time: 1.0344018936157227\n",
      "Epoch 598/3000, Train Loss: 0.8228, Time: 1.015479326248169\n",
      "Epoch 599/3000, Train Loss: 0.9448, Time: 0.9993019104003906\n",
      "Epoch 600/3000, Train Loss: 0.9210, Time: 1.028348684310913\n",
      "Epoch 601/3000, Train Loss: 0.9029, Time: 1.066633701324463\n",
      "Epoch 602/3000, Train Loss: 0.8959, Time: 1.0384445190429688\n",
      "Epoch 603/3000, Train Loss: 0.9719, Time: 1.0315451622009277\n",
      "Epoch 604/3000, Train Loss: 0.9088, Time: 1.0660245418548584\n",
      "Epoch 605/3000, Train Loss: 0.9315, Time: 1.040747880935669\n",
      "Epoch 606/3000, Train Loss: 0.9654, Time: 1.024383544921875\n",
      "Epoch 607/3000, Train Loss: 0.8573, Time: 0.9977543354034424\n",
      "Epoch 608/3000, Train Loss: 0.8886, Time: 0.9974915981292725\n",
      "Epoch 609/3000, Train Loss: 0.9658, Time: 0.9984400272369385\n",
      "Epoch 610/3000, Train Loss: 0.9888, Time: 1.0315396785736084\n",
      "Epoch 611/3000, Train Loss: 0.8814, Time: 1.018157720565796\n",
      "Epoch 612/3000, Train Loss: 0.8608, Time: 1.0121386051177979\n",
      "Epoch 613/3000, Train Loss: 0.8809, Time: 1.027695655822754\n",
      "Epoch 614/3000, Train Loss: 0.8410, Time: 0.9871475696563721\n",
      "Epoch 615/3000, Train Loss: 0.8502, Time: 1.0339312553405762\n",
      "Epoch 616/3000, Train Loss: 0.8330, Time: 1.1034247875213623\n",
      "Epoch 617/3000, Train Loss: 0.8569, Time: 1.1200776100158691\n",
      "Epoch 618/3000, Train Loss: 0.9014, Time: 1.1299307346343994\n",
      "Epoch 619/3000, Train Loss: 0.8758, Time: 1.1131665706634521\n",
      "Epoch 620/3000, Train Loss: 0.8610, Time: 1.1117615699768066\n",
      "Epoch 621/3000, Train Loss: 0.9626, Time: 1.1055707931518555\n",
      "Epoch 622/3000, Train Loss: 0.8851, Time: 1.1043314933776855\n",
      "Epoch 623/3000, Train Loss: 0.9876, Time: 1.13519287109375\n",
      "Epoch 624/3000, Train Loss: 0.9152, Time: 1.1319935321807861\n",
      "Epoch 625/3000, Train Loss: 0.8773, Time: 1.1303279399871826\n",
      "Epoch 626/3000, Train Loss: 0.9787, Time: 1.0996994972229004\n",
      "Epoch 627/3000, Train Loss: 0.8496, Time: 1.101177453994751\n",
      "Epoch 628/3000, Train Loss: 0.8946, Time: 1.1011416912078857\n",
      "Epoch 629/3000, Train Loss: 0.9050, Time: 1.096123218536377\n",
      "Epoch 630/3000, Train Loss: 0.8537, Time: 1.127220869064331\n",
      "Epoch 631/3000, Train Loss: 0.9680, Time: 1.1248455047607422\n",
      "Epoch 632/3000, Train Loss: 0.9289, Time: 1.1156165599822998\n",
      "Epoch 633/3000, Train Loss: 0.8514, Time: 1.1014325618743896\n",
      "Epoch 634/3000, Train Loss: 0.9349, Time: 1.1274659633636475\n",
      "Epoch 635/3000, Train Loss: 0.9322, Time: 1.2138397693634033\n",
      "Epoch 636/3000, Train Loss: 0.8187, Time: 1.1872851848602295\n",
      "Epoch 637/3000, Train Loss: 0.9443, Time: 1.1238608360290527\n",
      "Epoch 638/3000, Train Loss: 0.8516, Time: 1.0953631401062012\n",
      "Epoch 639/3000, Train Loss: 0.9230, Time: 1.099613904953003\n",
      "Epoch 640/3000, Train Loss: 0.8801, Time: 1.1027498245239258\n",
      "Epoch 641/3000, Train Loss: 0.8399, Time: 1.111541748046875\n",
      "Epoch 642/3000, Train Loss: 0.8774, Time: 1.1055212020874023\n",
      "Epoch 643/3000, Train Loss: 0.8923, Time: 1.1040682792663574\n",
      "Epoch 644/3000, Train Loss: 0.8823, Time: 1.1038386821746826\n",
      "Epoch 645/3000, Train Loss: 0.8440, Time: 1.0999658107757568\n",
      "Epoch 646/3000, Train Loss: 0.9267, Time: 1.130910873413086\n",
      "Epoch 647/3000, Train Loss: 0.9069, Time: 1.100435495376587\n",
      "Epoch 648/3000, Train Loss: 0.8252, Time: 1.1000559329986572\n",
      "Epoch 649/3000, Train Loss: 0.9530, Time: 1.0996439456939697\n",
      "Epoch 650/3000, Train Loss: 0.9530, Time: 1.1076478958129883\n",
      "Epoch 651/3000, Train Loss: 1.1071, Time: 1.1111958026885986\n",
      "Epoch 652/3000, Train Loss: 0.9036, Time: 1.108459711074829\n",
      "Epoch 653/3000, Train Loss: 0.8667, Time: 1.0982866287231445\n",
      "Epoch 654/3000, Train Loss: 0.9373, Time: 1.104733943939209\n",
      "Epoch 655/3000, Train Loss: 0.8572, Time: 1.1058719158172607\n",
      "Epoch 656/3000, Train Loss: 0.8641, Time: 1.1137728691101074\n",
      "Epoch 657/3000, Train Loss: 0.8645, Time: 1.1147775650024414\n",
      "Epoch 658/3000, Train Loss: 0.8852, Time: 1.11354398727417\n",
      "Epoch 659/3000, Train Loss: 0.8933, Time: 1.1287789344787598\n",
      "Epoch 660/3000, Train Loss: 0.8254, Time: 1.103140115737915\n",
      "Epoch 661/3000, Train Loss: 0.8395, Time: 1.103569507598877\n",
      "Epoch 662/3000, Train Loss: 0.8417, Time: 1.1116962432861328\n",
      "Epoch 663/3000, Train Loss: 0.8385, Time: 1.0986182689666748\n",
      "Epoch 664/3000, Train Loss: 0.8330, Time: 1.0991196632385254\n",
      "Epoch 665/3000, Train Loss: 0.9022, Time: 1.1041796207427979\n",
      "Epoch 666/3000, Train Loss: 0.9946, Time: 1.104935884475708\n",
      "Epoch 667/3000, Train Loss: 0.8785, Time: 1.116868019104004\n",
      "Epoch 668/3000, Train Loss: 0.9119, Time: 1.1159000396728516\n",
      "Epoch 669/3000, Train Loss: 0.9544, Time: 1.1075034141540527\n",
      "Epoch 670/3000, Train Loss: 0.9286, Time: 1.1058967113494873\n",
      "Epoch 671/3000, Train Loss: 0.9279, Time: 1.1238698959350586\n",
      "Epoch 672/3000, Train Loss: 1.0088, Time: 1.1102814674377441\n",
      "Epoch 673/3000, Train Loss: 0.8879, Time: 1.1091179847717285\n",
      "Epoch 674/3000, Train Loss: 0.8541, Time: 1.10561203956604\n",
      "Epoch 675/3000, Train Loss: 0.8850, Time: 1.112459421157837\n",
      "Epoch 676/3000, Train Loss: 0.8901, Time: 1.113921880722046\n",
      "Epoch 677/3000, Train Loss: 0.9715, Time: 1.1434693336486816\n",
      "Epoch 678/3000, Train Loss: 0.9131, Time: 1.1113224029541016\n",
      "Epoch 679/3000, Train Loss: 0.9695, Time: 1.15706205368042\n",
      "Epoch 680/3000, Train Loss: 0.8731, Time: 1.1036202907562256\n",
      "Epoch 681/3000, Train Loss: 0.8492, Time: 1.0985589027404785\n",
      "Epoch 682/3000, Train Loss: 0.8517, Time: 1.0572080612182617\n",
      "Epoch 683/3000, Train Loss: 0.8505, Time: 1.0054457187652588\n",
      "Epoch 684/3000, Train Loss: 0.8546, Time: 1.0097055435180664\n",
      "Epoch 685/3000, Train Loss: 1.0945, Time: 1.1031835079193115\n",
      "Epoch 686/3000, Train Loss: 0.9366, Time: 1.098651647567749\n",
      "Epoch 687/3000, Train Loss: 0.8715, Time: 1.130188226699829\n",
      "Epoch 688/3000, Train Loss: 0.9008, Time: 1.0984148979187012\n",
      "Epoch 689/3000, Train Loss: 0.8313, Time: 1.127500295639038\n",
      "Epoch 690/3000, Train Loss: 0.8843, Time: 1.1702687740325928\n",
      "Epoch 691/3000, Train Loss: 0.8431, Time: 1.1658201217651367\n",
      "Epoch 692/3000, Train Loss: 0.8456, Time: 1.1107029914855957\n",
      "Epoch 693/3000, Train Loss: 0.8375, Time: 1.1014902591705322\n",
      "Epoch 694/3000, Train Loss: 0.8569, Time: 1.1015808582305908\n",
      "Epoch 695/3000, Train Loss: 0.8588, Time: 1.135655164718628\n",
      "Epoch 696/3000, Train Loss: 0.9400, Time: 1.1048552989959717\n",
      "Epoch 697/3000, Train Loss: 0.8598, Time: 1.1544013023376465\n",
      "Epoch 698/3000, Train Loss: 0.8735, Time: 1.0987555980682373\n",
      "Epoch 699/3000, Train Loss: 0.9621, Time: 1.0966403484344482\n",
      "Epoch 700/3000, Train Loss: 0.8351, Time: 1.0949764251708984\n",
      "Epoch 701/3000, Train Loss: 0.9024, Time: 1.0979061126708984\n",
      "Epoch 702/3000, Train Loss: 1.0048, Time: 1.098048210144043\n",
      "Epoch 703/3000, Train Loss: 0.8787, Time: 1.0993008613586426\n",
      "Epoch 704/3000, Train Loss: 0.8894, Time: 1.099764108657837\n",
      "Epoch 705/3000, Train Loss: 0.8200, Time: 1.0946342945098877\n",
      "Epoch 706/3000, Train Loss: 0.9829, Time: 1.1070144176483154\n",
      "Epoch 707/3000, Train Loss: 0.9099, Time: 1.1018273830413818\n",
      "Epoch 708/3000, Train Loss: 0.8714, Time: 1.1332552433013916\n",
      "Epoch 709/3000, Train Loss: 0.8185, Time: 1.1045429706573486\n",
      "Epoch 710/3000, Train Loss: 0.8231, Time: 1.0952768325805664\n",
      "Epoch 711/3000, Train Loss: 0.8585, Time: 1.097513198852539\n",
      "Epoch 712/3000, Train Loss: 0.8433, Time: 1.102299690246582\n",
      "Epoch 713/3000, Train Loss: 0.8104, Time: 1.104733943939209\n",
      "Epoch 714/3000, Train Loss: 0.8002, Time: 1.1478257179260254\n",
      "Epoch 715/3000, Train Loss: 0.8139, Time: 1.100233554840088\n",
      "Epoch 716/3000, Train Loss: 1.1312, Time: 1.1035935878753662\n",
      "Epoch 717/3000, Train Loss: 0.8907, Time: 1.100475788116455\n",
      "Epoch 718/3000, Train Loss: 0.8933, Time: 1.0994112491607666\n",
      "Epoch 719/3000, Train Loss: 0.8542, Time: 1.0997257232666016\n",
      "Epoch 720/3000, Train Loss: 1.0149, Time: 1.1073439121246338\n",
      "Epoch 721/3000, Train Loss: 0.8794, Time: 1.1399140357971191\n",
      "Epoch 722/3000, Train Loss: 0.8501, Time: 1.124382734298706\n",
      "Epoch 723/3000, Train Loss: 0.9051, Time: 1.1247878074645996\n",
      "Epoch 724/3000, Train Loss: 0.7944, Time: 1.1609058380126953\n",
      "Epoch 725/3000, Train Loss: 0.8737, Time: 1.1364164352416992\n",
      "Epoch 726/3000, Train Loss: 0.8990, Time: 1.1077044010162354\n",
      "Epoch 727/3000, Train Loss: 0.8512, Time: 1.109196662902832\n",
      "Epoch 728/3000, Train Loss: 0.9160, Time: 1.1016592979431152\n",
      "Epoch 729/3000, Train Loss: 0.9450, Time: 1.1058664321899414\n",
      "Epoch 730/3000, Train Loss: 1.3034, Time: 1.1002116203308105\n",
      "Epoch 731/3000, Train Loss: 0.8521, Time: 1.1332051753997803\n",
      "Epoch 732/3000, Train Loss: 0.9076, Time: 1.103628158569336\n",
      "Epoch 733/3000, Train Loss: 1.1801, Time: 1.1344964504241943\n",
      "Epoch 734/3000, Train Loss: 0.8235, Time: 1.1007933616638184\n",
      "Epoch 735/3000, Train Loss: 0.8229, Time: 1.102635145187378\n",
      "Epoch 736/3000, Train Loss: 0.8306, Time: 1.1695287227630615\n",
      "Epoch 737/3000, Train Loss: 0.9374, Time: 1.1023941040039062\n",
      "Epoch 738/3000, Train Loss: 0.8484, Time: 1.1100356578826904\n",
      "Epoch 739/3000, Train Loss: 0.9327, Time: 1.1312274932861328\n",
      "Epoch 740/3000, Train Loss: 0.8306, Time: 1.1190986633300781\n",
      "Epoch 741/3000, Train Loss: 0.8096, Time: 1.1224689483642578\n",
      "Epoch 742/3000, Train Loss: 0.8128, Time: 1.1758506298065186\n",
      "Epoch 743/3000, Train Loss: 0.8924, Time: 1.2585475444793701\n",
      "Epoch 744/3000, Train Loss: 0.8352, Time: 1.1164195537567139\n",
      "Epoch 745/3000, Train Loss: 0.8753, Time: 1.1099045276641846\n",
      "Epoch 746/3000, Train Loss: 0.9219, Time: 1.1021668910980225\n",
      "Epoch 747/3000, Train Loss: 0.9497, Time: 1.0986371040344238\n",
      "Epoch 748/3000, Train Loss: 0.8212, Time: 1.1091711521148682\n",
      "Epoch 749/3000, Train Loss: 0.8406, Time: 1.1045360565185547\n",
      "Epoch 750/3000, Train Loss: 0.8230, Time: 1.1324024200439453\n",
      "Epoch 751/3000, Train Loss: 0.8361, Time: 1.114267110824585\n",
      "Epoch 752/3000, Train Loss: 0.8314, Time: 1.1116456985473633\n",
      "Epoch 753/3000, Train Loss: 0.8554, Time: 1.1061365604400635\n",
      "Epoch 754/3000, Train Loss: 0.8598, Time: 1.1311976909637451\n",
      "Epoch 755/3000, Train Loss: 0.8747, Time: 1.1471564769744873\n",
      "Epoch 756/3000, Train Loss: 0.8558, Time: 1.1171064376831055\n",
      "Epoch 757/3000, Train Loss: 0.8748, Time: 1.1016528606414795\n",
      "Epoch 758/3000, Train Loss: 0.8352, Time: 1.1073322296142578\n",
      "Epoch 759/3000, Train Loss: 0.8979, Time: 1.1285076141357422\n",
      "Epoch 760/3000, Train Loss: 0.8412, Time: 1.1118154525756836\n",
      "Epoch 761/3000, Train Loss: 0.8628, Time: 1.1590955257415771\n",
      "Epoch 762/3000, Train Loss: 0.8053, Time: 1.12260103225708\n",
      "Epoch 763/3000, Train Loss: 0.8409, Time: 1.105055809020996\n",
      "Epoch 764/3000, Train Loss: 0.8442, Time: 1.150869369506836\n",
      "Epoch 765/3000, Train Loss: 0.8380, Time: 1.1038970947265625\n",
      "Epoch 766/3000, Train Loss: 0.8351, Time: 1.110567331314087\n",
      "Epoch 767/3000, Train Loss: 0.8287, Time: 1.1104419231414795\n",
      "Epoch 768/3000, Train Loss: 0.8238, Time: 1.1008479595184326\n",
      "Epoch 769/3000, Train Loss: 0.8672, Time: 1.1053385734558105\n",
      "Epoch 770/3000, Train Loss: 0.8040, Time: 1.0996105670928955\n",
      "Epoch 771/3000, Train Loss: 0.8828, Time: 1.094632625579834\n",
      "Epoch 772/3000, Train Loss: 0.8754, Time: 1.0910289287567139\n",
      "Epoch 773/3000, Train Loss: 0.8721, Time: 1.1026768684387207\n",
      "Epoch 774/3000, Train Loss: 0.8975, Time: 1.1121852397918701\n",
      "Epoch 775/3000, Train Loss: 0.9041, Time: 1.1054177284240723\n",
      "Epoch 776/3000, Train Loss: 0.8822, Time: 1.0350263118743896\n",
      "Epoch 777/3000, Train Loss: 0.8822, Time: 1.011181354522705\n",
      "Epoch 778/3000, Train Loss: 0.9506, Time: 1.0579173564910889\n",
      "Epoch 779/3000, Train Loss: 0.8796, Time: 1.1012413501739502\n",
      "Epoch 780/3000, Train Loss: 0.9344, Time: 1.0994174480438232\n",
      "Epoch 781/3000, Train Loss: 0.8151, Time: 1.1288065910339355\n",
      "Epoch 782/3000, Train Loss: 0.8604, Time: 1.135389804840088\n",
      "Epoch 783/3000, Train Loss: 0.8255, Time: 1.1133675575256348\n",
      "Epoch 784/3000, Train Loss: 0.8603, Time: 1.1081640720367432\n",
      "Epoch 785/3000, Train Loss: 0.9185, Time: 1.1042704582214355\n",
      "Epoch 786/3000, Train Loss: 0.8943, Time: 1.1047966480255127\n",
      "Epoch 787/3000, Train Loss: 0.8445, Time: 1.1441938877105713\n",
      "Epoch 788/3000, Train Loss: 0.8378, Time: 1.0977647304534912\n",
      "Epoch 789/3000, Train Loss: 0.8778, Time: 1.1017017364501953\n",
      "Epoch 790/3000, Train Loss: 0.8835, Time: 1.1478595733642578\n",
      "Epoch 791/3000, Train Loss: 0.9212, Time: 1.1180872917175293\n",
      "Epoch 792/3000, Train Loss: 0.8152, Time: 1.1226849555969238\n",
      "Epoch 793/3000, Train Loss: 0.8444, Time: 1.1390490531921387\n",
      "Epoch 794/3000, Train Loss: 0.8568, Time: 1.1200144290924072\n",
      "Epoch 795/3000, Train Loss: 0.8756, Time: 1.1447324752807617\n",
      "Epoch 796/3000, Train Loss: 0.8547, Time: 1.1090078353881836\n",
      "Epoch 797/3000, Train Loss: 0.8274, Time: 1.2430918216705322\n",
      "Epoch 798/3000, Train Loss: 0.8313, Time: 1.183488130569458\n",
      "Epoch 799/3000, Train Loss: 0.8544, Time: 1.1315231323242188\n",
      "Epoch 800/3000, Train Loss: 0.8815, Time: 1.1124331951141357\n",
      "Epoch 801/3000, Train Loss: 0.7945, Time: 1.1079504489898682\n",
      "Epoch 802/3000, Train Loss: 0.9067, Time: 1.138301134109497\n",
      "Epoch 803/3000, Train Loss: 0.8251, Time: 1.1267638206481934\n",
      "Epoch 804/3000, Train Loss: 0.9225, Time: 1.140711784362793\n",
      "Epoch 805/3000, Train Loss: 0.8418, Time: 1.09895920753479\n",
      "Epoch 806/3000, Train Loss: 0.9219, Time: 1.1484973430633545\n",
      "Epoch 807/3000, Train Loss: 0.8367, Time: 1.102548360824585\n",
      "Epoch 808/3000, Train Loss: 0.9245, Time: 1.0999667644500732\n",
      "Epoch 809/3000, Train Loss: 0.8433, Time: 1.1038086414337158\n",
      "Epoch 810/3000, Train Loss: 0.8555, Time: 1.1046645641326904\n",
      "Epoch 811/3000, Train Loss: 0.8405, Time: 1.101478099822998\n",
      "Epoch 812/3000, Train Loss: 0.8924, Time: 1.0965750217437744\n",
      "Epoch 813/3000, Train Loss: 0.8926, Time: 1.1035807132720947\n",
      "Epoch 814/3000, Train Loss: 0.8390, Time: 1.1111347675323486\n",
      "Epoch 815/3000, Train Loss: 0.8246, Time: 1.1136553287506104\n",
      "Epoch 816/3000, Train Loss: 0.8226, Time: 1.112257957458496\n",
      "Epoch 817/3000, Train Loss: 0.9310, Time: 1.1291429996490479\n",
      "Epoch 818/3000, Train Loss: 0.9047, Time: 1.1057076454162598\n",
      "Epoch 819/3000, Train Loss: 0.8687, Time: 1.1130397319793701\n",
      "Epoch 820/3000, Train Loss: 0.8079, Time: 1.1119968891143799\n",
      "Epoch 821/3000, Train Loss: 0.8566, Time: 1.1081304550170898\n",
      "Epoch 822/3000, Train Loss: 0.8565, Time: 1.1094353199005127\n",
      "Epoch 823/3000, Train Loss: 0.8281, Time: 1.1054165363311768\n",
      "Epoch 824/3000, Train Loss: 0.8148, Time: 1.1128029823303223\n",
      "Epoch 825/3000, Train Loss: 0.8109, Time: 1.104881763458252\n",
      "Epoch 826/3000, Train Loss: 0.8334, Time: 1.1019542217254639\n",
      "Epoch 827/3000, Train Loss: 0.8187, Time: 1.1035404205322266\n",
      "Epoch 828/3000, Train Loss: 0.8303, Time: 1.1334242820739746\n",
      "Epoch 829/3000, Train Loss: 0.8597, Time: 1.109710454940796\n",
      "Epoch 830/3000, Train Loss: 0.9215, Time: 1.1131126880645752\n",
      "Epoch 831/3000, Train Loss: 0.8357, Time: 1.1108026504516602\n",
      "Epoch 832/3000, Train Loss: 0.8177, Time: 1.1149542331695557\n",
      "Epoch 833/3000, Train Loss: 0.8728, Time: 1.1450669765472412\n",
      "Epoch 834/3000, Train Loss: 0.8098, Time: 1.116438627243042\n",
      "Epoch 835/3000, Train Loss: 0.8498, Time: 1.1131782531738281\n",
      "Epoch 836/3000, Train Loss: 0.8878, Time: 1.1018681526184082\n",
      "Epoch 837/3000, Train Loss: 0.8994, Time: 1.1134707927703857\n",
      "Epoch 838/3000, Train Loss: 0.8834, Time: 1.106175422668457\n",
      "Epoch 839/3000, Train Loss: 0.8441, Time: 1.1043884754180908\n",
      "Epoch 840/3000, Train Loss: 0.9576, Time: 1.1466593742370605\n",
      "Epoch 841/3000, Train Loss: 0.9969, Time: 1.1090409755706787\n",
      "Epoch 842/3000, Train Loss: 0.8356, Time: 1.1229324340820312\n",
      "Epoch 843/3000, Train Loss: 0.9366, Time: 1.1060330867767334\n",
      "Epoch 844/3000, Train Loss: 0.9290, Time: 1.1077075004577637\n",
      "Epoch 845/3000, Train Loss: 0.8335, Time: 1.1025547981262207\n",
      "Epoch 846/3000, Train Loss: 0.8273, Time: 1.1368780136108398\n",
      "Epoch 847/3000, Train Loss: 0.8212, Time: 1.1006991863250732\n",
      "Epoch 848/3000, Train Loss: 0.8456, Time: 1.127251148223877\n",
      "Epoch 849/3000, Train Loss: 0.9130, Time: 1.11295747756958\n",
      "Epoch 850/3000, Train Loss: 0.8330, Time: 1.112128496170044\n",
      "Epoch 851/3000, Train Loss: 0.8213, Time: 1.165954828262329\n",
      "Epoch 852/3000, Train Loss: 0.8297, Time: 1.173388957977295\n",
      "Epoch 853/3000, Train Loss: 0.8256, Time: 1.1280591487884521\n",
      "Epoch 854/3000, Train Loss: 0.9437, Time: 1.0984196662902832\n",
      "Epoch 855/3000, Train Loss: 0.8519, Time: 1.1018397808074951\n",
      "Epoch 856/3000, Train Loss: 0.8294, Time: 1.1009280681610107\n",
      "Epoch 857/3000, Train Loss: 0.8225, Time: 1.107872724533081\n",
      "Epoch 858/3000, Train Loss: 0.9627, Time: 1.1087121963500977\n",
      "Epoch 859/3000, Train Loss: 1.1386, Time: 1.1022377014160156\n",
      "Epoch 860/3000, Train Loss: 0.9787, Time: 1.138296365737915\n",
      "Epoch 861/3000, Train Loss: 0.8596, Time: 1.099006175994873\n",
      "Epoch 862/3000, Train Loss: 0.8936, Time: 1.1088407039642334\n",
      "Epoch 863/3000, Train Loss: 0.8530, Time: 1.1022617816925049\n",
      "Epoch 864/3000, Train Loss: 0.8099, Time: 1.100332498550415\n",
      "Epoch 865/3000, Train Loss: 0.8435, Time: 1.1047143936157227\n",
      "Epoch 866/3000, Train Loss: 0.8826, Time: 1.1072642803192139\n",
      "Epoch 867/3000, Train Loss: 0.8056, Time: 1.1089580059051514\n",
      "Epoch 868/3000, Train Loss: 0.8677, Time: 1.1795141696929932\n",
      "Epoch 869/3000, Train Loss: 0.8140, Time: 1.139855146408081\n",
      "Epoch 870/3000, Train Loss: 0.9102, Time: 1.0166583061218262\n",
      "Epoch 871/3000, Train Loss: 0.8382, Time: 1.0077548027038574\n",
      "Epoch 872/3000, Train Loss: 0.8372, Time: 1.0664772987365723\n",
      "Epoch 873/3000, Train Loss: 0.8179, Time: 1.1020076274871826\n",
      "Epoch 874/3000, Train Loss: 0.8710, Time: 1.1321139335632324\n",
      "Epoch 875/3000, Train Loss: 0.8208, Time: 1.1329402923583984\n",
      "Epoch 876/3000, Train Loss: 0.8038, Time: 1.1368632316589355\n",
      "Epoch 877/3000, Train Loss: 0.8353, Time: 1.1135914325714111\n",
      "Epoch 878/3000, Train Loss: 0.8036, Time: 1.1056971549987793\n",
      "Epoch 879/3000, Train Loss: 0.8889, Time: 1.1050527095794678\n",
      "Epoch 880/3000, Train Loss: 0.8399, Time: 1.102707862854004\n",
      "Epoch 881/3000, Train Loss: 0.9839, Time: 1.156346082687378\n",
      "Epoch 882/3000, Train Loss: 1.0381, Time: 1.135171890258789\n",
      "Epoch 883/3000, Train Loss: 0.9175, Time: 1.1282129287719727\n",
      "Epoch 884/3000, Train Loss: 0.8051, Time: 1.1035082340240479\n",
      "Epoch 885/3000, Train Loss: 0.8669, Time: 1.1035497188568115\n",
      "Epoch 886/3000, Train Loss: 0.8202, Time: 1.1021137237548828\n",
      "Epoch 887/3000, Train Loss: 0.8203, Time: 1.1091370582580566\n",
      "Epoch 888/3000, Train Loss: 0.8923, Time: 1.1410000324249268\n",
      "Epoch 889/3000, Train Loss: 0.8417, Time: 1.1741175651550293\n",
      "Epoch 890/3000, Train Loss: 0.8298, Time: 1.165912389755249\n",
      "Epoch 891/3000, Train Loss: 0.8323, Time: 1.1382508277893066\n",
      "Epoch 892/3000, Train Loss: 0.9883, Time: 1.1052591800689697\n",
      "Epoch 893/3000, Train Loss: 0.8523, Time: 1.1052887439727783\n",
      "Epoch 894/3000, Train Loss: 0.8721, Time: 1.1037192344665527\n",
      "Epoch 895/3000, Train Loss: 0.8174, Time: 1.1037919521331787\n",
      "Epoch 896/3000, Train Loss: 0.8794, Time: 1.1567795276641846\n",
      "Epoch 897/3000, Train Loss: 0.8886, Time: 1.1596813201904297\n",
      "Epoch 898/3000, Train Loss: 0.8206, Time: 1.110875129699707\n",
      "Epoch 899/3000, Train Loss: 0.8149, Time: 1.1187677383422852\n",
      "Epoch 900/3000, Train Loss: 0.8113, Time: 1.1088955402374268\n",
      "Epoch 901/3000, Train Loss: 0.8057, Time: 1.1126925945281982\n",
      "Epoch 902/3000, Train Loss: 0.8450, Time: 1.1590988636016846\n",
      "Epoch 903/3000, Train Loss: 0.8429, Time: 1.1748201847076416\n",
      "Epoch 904/3000, Train Loss: 0.8341, Time: 1.2119317054748535\n",
      "Epoch 905/3000, Train Loss: 0.8449, Time: 1.213144063949585\n",
      "Epoch 906/3000, Train Loss: 1.0504, Time: 1.105989694595337\n",
      "Epoch 907/3000, Train Loss: 0.9222, Time: 1.1144778728485107\n",
      "Epoch 908/3000, Train Loss: 0.8347, Time: 1.1140460968017578\n",
      "Epoch 909/3000, Train Loss: 0.8206, Time: 1.1174988746643066\n",
      "Epoch 910/3000, Train Loss: 0.8760, Time: 1.1636996269226074\n",
      "Epoch 911/3000, Train Loss: 0.8387, Time: 1.1720802783966064\n",
      "Epoch 912/3000, Train Loss: 0.8151, Time: 1.1282322406768799\n",
      "Epoch 913/3000, Train Loss: 0.8185, Time: 1.1002538204193115\n",
      "Epoch 914/3000, Train Loss: 0.8198, Time: 1.106431007385254\n",
      "Epoch 915/3000, Train Loss: 0.8543, Time: 1.111802101135254\n",
      "Epoch 916/3000, Train Loss: 0.8789, Time: 1.1305475234985352\n",
      "Epoch 917/3000, Train Loss: 0.9804, Time: 1.1369366645812988\n",
      "Epoch 918/3000, Train Loss: 0.8696, Time: 1.1350071430206299\n",
      "Epoch 919/3000, Train Loss: 0.8211, Time: 1.1099534034729004\n",
      "Epoch 920/3000, Train Loss: 0.8227, Time: 1.106856346130371\n",
      "Epoch 921/3000, Train Loss: 0.8681, Time: 1.1113576889038086\n",
      "Epoch 922/3000, Train Loss: 0.8699, Time: 1.128288984298706\n",
      "Epoch 923/3000, Train Loss: 0.8286, Time: 1.1288669109344482\n",
      "Epoch 924/3000, Train Loss: 0.9046, Time: 1.1234421730041504\n",
      "Epoch 925/3000, Train Loss: 0.8104, Time: 1.1382098197937012\n",
      "Epoch 926/3000, Train Loss: 0.8219, Time: 1.1234936714172363\n",
      "Epoch 927/3000, Train Loss: 0.8123, Time: 1.0969767570495605\n",
      "Epoch 928/3000, Train Loss: 0.8500, Time: 1.1121046543121338\n",
      "Epoch 929/3000, Train Loss: 0.8904, Time: 1.137834072113037\n",
      "Epoch 930/3000, Train Loss: 0.8083, Time: 1.1293995380401611\n",
      "Epoch 931/3000, Train Loss: 0.8850, Time: 1.1199381351470947\n",
      "Epoch 932/3000, Train Loss: 0.8873, Time: 1.0992944240570068\n",
      "Epoch 933/3000, Train Loss: 0.8991, Time: 1.1003363132476807\n",
      "Epoch 934/3000, Train Loss: 0.9470, Time: 1.1137099266052246\n",
      "Epoch 935/3000, Train Loss: 0.8110, Time: 1.132539987564087\n",
      "Epoch 936/3000, Train Loss: 0.8463, Time: 1.1757848262786865\n",
      "Epoch 937/3000, Train Loss: 0.8137, Time: 1.1301538944244385\n",
      "Epoch 938/3000, Train Loss: 0.8036, Time: 1.1240425109863281\n",
      "Epoch 939/3000, Train Loss: 0.8149, Time: 1.0975332260131836\n",
      "Epoch 940/3000, Train Loss: 0.8269, Time: 1.105064868927002\n",
      "Epoch 941/3000, Train Loss: 0.8169, Time: 1.1511414051055908\n",
      "Epoch 942/3000, Train Loss: 0.8231, Time: 1.1363494396209717\n",
      "Epoch 943/3000, Train Loss: 0.8222, Time: 1.135918140411377\n",
      "Epoch 944/3000, Train Loss: 0.9352, Time: 1.102079153060913\n",
      "Epoch 945/3000, Train Loss: 0.8342, Time: 1.1039402484893799\n",
      "Epoch 946/3000, Train Loss: 0.8023, Time: 1.0942308902740479\n",
      "Epoch 947/3000, Train Loss: 0.7910, Time: 1.1148579120635986\n",
      "Epoch 948/3000, Train Loss: 0.9145, Time: 1.1303613185882568\n",
      "Epoch 949/3000, Train Loss: 0.8449, Time: 1.133044958114624\n",
      "Epoch 950/3000, Train Loss: 0.8444, Time: 1.1295950412750244\n",
      "Epoch 951/3000, Train Loss: 0.8977, Time: 1.1062211990356445\n",
      "Epoch 952/3000, Train Loss: 0.8296, Time: 1.1085598468780518\n",
      "Epoch 953/3000, Train Loss: 0.8924, Time: 1.1114490032196045\n",
      "Epoch 954/3000, Train Loss: 0.8443, Time: 1.1655588150024414\n",
      "Epoch 955/3000, Train Loss: 0.8208, Time: 1.152024507522583\n",
      "Epoch 956/3000, Train Loss: 0.8520, Time: 1.1358325481414795\n",
      "Epoch 957/3000, Train Loss: 0.8308, Time: 1.2616002559661865\n",
      "Epoch 958/3000, Train Loss: 0.8702, Time: 1.242534875869751\n",
      "Epoch 959/3000, Train Loss: 0.8021, Time: 1.0988590717315674\n",
      "Epoch 960/3000, Train Loss: 0.8357, Time: 1.096996784210205\n",
      "Epoch 961/3000, Train Loss: 0.8034, Time: 1.1145100593566895\n",
      "Epoch 962/3000, Train Loss: 0.8191, Time: 1.1309547424316406\n",
      "Epoch 963/3000, Train Loss: 0.8670, Time: 1.1356372833251953\n",
      "Epoch 964/3000, Train Loss: 0.8293, Time: 1.1031019687652588\n",
      "Epoch 965/3000, Train Loss: 0.8190, Time: 1.1012835502624512\n",
      "Epoch 966/3000, Train Loss: 0.8108, Time: 1.1006577014923096\n",
      "Epoch 967/3000, Train Loss: 0.8002, Time: 1.1048946380615234\n",
      "Epoch 968/3000, Train Loss: 0.8207, Time: 1.0411746501922607\n",
      "Epoch 969/3000, Train Loss: 0.7971, Time: 1.0556235313415527\n",
      "Epoch 970/3000, Train Loss: 0.8317, Time: 1.1108767986297607\n",
      "Epoch 971/3000, Train Loss: 0.8748, Time: 1.1098992824554443\n",
      "Epoch 972/3000, Train Loss: 0.8766, Time: 1.106139898300171\n",
      "Epoch 973/3000, Train Loss: 0.8172, Time: 1.1081626415252686\n",
      "Epoch 974/3000, Train Loss: 0.8402, Time: 1.1475584506988525\n",
      "Epoch 975/3000, Train Loss: 0.9022, Time: 1.1452021598815918\n",
      "Epoch 976/3000, Train Loss: 0.8791, Time: 1.1620924472808838\n",
      "Epoch 977/3000, Train Loss: 0.8067, Time: 1.1410505771636963\n",
      "Epoch 978/3000, Train Loss: 0.8080, Time: 1.1362853050231934\n",
      "Epoch 979/3000, Train Loss: 0.8588, Time: 1.138232946395874\n",
      "Epoch 980/3000, Train Loss: 0.8090, Time: 1.1338040828704834\n",
      "Epoch 981/3000, Train Loss: 0.8725, Time: 1.1333909034729004\n",
      "Epoch 982/3000, Train Loss: 0.8072, Time: 1.1283414363861084\n",
      "Epoch 983/3000, Train Loss: 0.8034, Time: 1.1075453758239746\n",
      "Epoch 984/3000, Train Loss: 0.8588, Time: 1.1080060005187988\n",
      "Epoch 985/3000, Train Loss: 0.9276, Time: 1.1212577819824219\n",
      "Epoch 986/3000, Train Loss: 0.8794, Time: 1.125675916671753\n",
      "Epoch 987/3000, Train Loss: 0.8176, Time: 1.1241958141326904\n",
      "Epoch 988/3000, Train Loss: 0.8204, Time: 1.096264123916626\n",
      "Epoch 989/3000, Train Loss: 0.9478, Time: 1.0977189540863037\n",
      "Epoch 990/3000, Train Loss: 0.8442, Time: 1.0956342220306396\n",
      "Epoch 991/3000, Train Loss: 0.8746, Time: 1.0952703952789307\n",
      "Epoch 992/3000, Train Loss: 0.9411, Time: 1.1234984397888184\n",
      "Epoch 993/3000, Train Loss: 0.8138, Time: 1.121474027633667\n",
      "Epoch 994/3000, Train Loss: 0.8113, Time: 1.0973536968231201\n",
      "Epoch 995/3000, Train Loss: 0.9878, Time: 1.0901539325714111\n",
      "Epoch 996/3000, Train Loss: 0.8416, Time: 1.0974559783935547\n",
      "Epoch 997/3000, Train Loss: 0.8145, Time: 1.0981380939483643\n",
      "Epoch 998/3000, Train Loss: 0.8048, Time: 1.1209547519683838\n",
      "Epoch 999/3000, Train Loss: 0.8146, Time: 1.1323838233947754\n",
      "Epoch 1000/3000, Train Loss: 0.8365, Time: 1.1233131885528564\n",
      "Epoch 1001/3000, Train Loss: 0.8439, Time: 1.0979623794555664\n",
      "Epoch 1002/3000, Train Loss: 0.8210, Time: 1.1008141040802002\n",
      "Epoch 1003/3000, Train Loss: 0.8629, Time: 1.1032252311706543\n",
      "Epoch 1004/3000, Train Loss: 0.8302, Time: 1.1077091693878174\n",
      "Epoch 1005/3000, Train Loss: 0.8035, Time: 1.1132206916809082\n",
      "Epoch 1006/3000, Train Loss: 0.8155, Time: 1.130976676940918\n",
      "Epoch 1007/3000, Train Loss: 0.7937, Time: 1.1217737197875977\n",
      "Epoch 1008/3000, Train Loss: 0.8071, Time: 1.107987403869629\n",
      "Epoch 1009/3000, Train Loss: 0.8295, Time: 1.1117870807647705\n",
      "Epoch 1010/3000, Train Loss: 0.9811, Time: 1.1130683422088623\n",
      "Epoch 1011/3000, Train Loss: 0.8386, Time: 1.1468636989593506\n",
      "Epoch 1012/3000, Train Loss: 0.8166, Time: 1.260988712310791\n",
      "Epoch 1013/3000, Train Loss: 0.9502, Time: 1.2384517192840576\n",
      "Epoch 1014/3000, Train Loss: 0.7920, Time: 1.1243953704833984\n",
      "Epoch 1015/3000, Train Loss: 0.7836, Time: 1.1250176429748535\n",
      "Epoch 1016/3000, Train Loss: 0.8287, Time: 1.1053650379180908\n",
      "Epoch 1017/3000, Train Loss: 0.8540, Time: 1.1090044975280762\n",
      "Epoch 1018/3000, Train Loss: 0.8004, Time: 1.1073894500732422\n",
      "Epoch 1019/3000, Train Loss: 0.8499, Time: 1.1239981651306152\n",
      "Epoch 1020/3000, Train Loss: 0.8343, Time: 1.1723740100860596\n",
      "Epoch 1021/3000, Train Loss: 0.7920, Time: 1.1252515316009521\n",
      "Epoch 1022/3000, Train Loss: 0.8123, Time: 1.1103301048278809\n",
      "Epoch 1023/3000, Train Loss: 0.8303, Time: 1.11407470703125\n",
      "Epoch 1024/3000, Train Loss: 0.7973, Time: 1.1274712085723877\n",
      "Epoch 1025/3000, Train Loss: 0.8269, Time: 1.1333000659942627\n",
      "Epoch 1026/3000, Train Loss: 0.8012, Time: 1.1298549175262451\n",
      "Epoch 1027/3000, Train Loss: 0.8150, Time: 1.1364450454711914\n",
      "Epoch 1028/3000, Train Loss: 0.8727, Time: 1.1325979232788086\n",
      "Epoch 1029/3000, Train Loss: 0.8294, Time: 1.143115758895874\n",
      "Epoch 1030/3000, Train Loss: 0.8082, Time: 1.1272094249725342\n",
      "Epoch 1031/3000, Train Loss: 0.8376, Time: 1.1271023750305176\n",
      "Epoch 1032/3000, Train Loss: 0.7910, Time: 1.10616135597229\n",
      "Epoch 1033/3000, Train Loss: 0.8128, Time: 1.1983306407928467\n",
      "Epoch 1034/3000, Train Loss: 0.8191, Time: 1.1538138389587402\n",
      "Epoch 1035/3000, Train Loss: 0.8938, Time: 1.131363868713379\n",
      "Epoch 1036/3000, Train Loss: 0.8457, Time: 1.1335201263427734\n",
      "Epoch 1037/3000, Train Loss: 0.9445, Time: 1.1066241264343262\n",
      "Epoch 1038/3000, Train Loss: 0.8293, Time: 1.1342968940734863\n",
      "Epoch 1039/3000, Train Loss: 0.8041, Time: 1.1050102710723877\n",
      "Epoch 1040/3000, Train Loss: 0.7998, Time: 1.109837293624878\n",
      "Epoch 1041/3000, Train Loss: 0.8329, Time: 1.124629020690918\n",
      "Epoch 1042/3000, Train Loss: 0.7994, Time: 1.1341135501861572\n",
      "Epoch 1043/3000, Train Loss: 0.7813, Time: 1.1355676651000977\n",
      "Epoch 1044/3000, Train Loss: 0.7992, Time: 1.1095731258392334\n",
      "Epoch 1045/3000, Train Loss: 0.8244, Time: 1.108154296875\n",
      "Epoch 1046/3000, Train Loss: 0.8370, Time: 1.1069526672363281\n",
      "Epoch 1047/3000, Train Loss: 0.8100, Time: 1.1139156818389893\n",
      "Epoch 1048/3000, Train Loss: 0.8090, Time: 1.1070635318756104\n",
      "Epoch 1049/3000, Train Loss: 0.8059, Time: 1.1511030197143555\n",
      "Epoch 1050/3000, Train Loss: 0.8102, Time: 1.157996654510498\n",
      "Epoch 1051/3000, Train Loss: 0.7798, Time: 1.1209924221038818\n",
      "Epoch 1052/3000, Train Loss: 0.7795, Time: 1.1180322170257568\n",
      "Epoch 1053/3000, Train Loss: 0.8101, Time: 1.106910228729248\n",
      "Epoch 1054/3000, Train Loss: 0.8413, Time: 1.1612825393676758\n",
      "Epoch 1055/3000, Train Loss: 0.7994, Time: 1.1634371280670166\n",
      "Epoch 1056/3000, Train Loss: 0.7802, Time: 1.140958547592163\n",
      "Epoch 1057/3000, Train Loss: 0.8875, Time: 1.1402029991149902\n",
      "Epoch 1058/3000, Train Loss: 0.8263, Time: 1.1461670398712158\n",
      "Epoch 1059/3000, Train Loss: 0.7701, Time: 1.1139745712280273\n",
      "Epoch 1060/3000, Train Loss: 0.7967, Time: 1.1294615268707275\n",
      "Epoch 1061/3000, Train Loss: 0.8261, Time: 1.1108207702636719\n",
      "Epoch 1062/3000, Train Loss: 0.8059, Time: 1.1272718906402588\n",
      "Epoch 1063/3000, Train Loss: 0.8333, Time: 1.1066744327545166\n",
      "Epoch 1064/3000, Train Loss: 0.8357, Time: 1.150749683380127\n",
      "Epoch 1065/3000, Train Loss: 0.8457, Time: 1.0979807376861572\n",
      "Epoch 1066/3000, Train Loss: 0.7805, Time: 1.0771541595458984\n",
      "Epoch 1067/3000, Train Loss: 0.8135, Time: 1.1433050632476807\n",
      "Epoch 1068/3000, Train Loss: 0.8153, Time: 1.1438610553741455\n",
      "Epoch 1069/3000, Train Loss: 0.8106, Time: 1.1224596500396729\n",
      "Epoch 1070/3000, Train Loss: 0.8522, Time: 1.1151044368743896\n",
      "Epoch 1071/3000, Train Loss: 0.8089, Time: 1.107419729232788\n",
      "Epoch 1072/3000, Train Loss: 0.7914, Time: 1.1070151329040527\n",
      "Epoch 1073/3000, Train Loss: 0.8309, Time: 1.104555606842041\n",
      "Epoch 1074/3000, Train Loss: 0.9160, Time: 1.1482937335968018\n",
      "Epoch 1075/3000, Train Loss: 0.8754, Time: 1.1324684619903564\n",
      "Epoch 1076/3000, Train Loss: 0.7656, Time: 1.126905918121338\n",
      "Epoch 1077/3000, Train Loss: 0.7947, Time: 1.105600118637085\n",
      "Epoch 1078/3000, Train Loss: 0.7906, Time: 1.1068165302276611\n",
      "Epoch 1079/3000, Train Loss: 0.8097, Time: 1.1107046604156494\n",
      "Epoch 1080/3000, Train Loss: 0.8001, Time: 1.106274127960205\n",
      "Epoch 1081/3000, Train Loss: 0.8161, Time: 1.1231298446655273\n",
      "Epoch 1082/3000, Train Loss: 0.7874, Time: 1.1589481830596924\n",
      "Epoch 1083/3000, Train Loss: 0.8112, Time: 1.13181471824646\n",
      "Epoch 1084/3000, Train Loss: 0.8920, Time: 1.1030101776123047\n",
      "Epoch 1085/3000, Train Loss: 0.8083, Time: 1.1025795936584473\n",
      "Epoch 1086/3000, Train Loss: 0.8166, Time: 1.0935471057891846\n",
      "Epoch 1087/3000, Train Loss: 0.9399, Time: 1.1023540496826172\n",
      "Epoch 1088/3000, Train Loss: 0.8132, Time: 1.113213062286377\n",
      "Epoch 1089/3000, Train Loss: 0.7879, Time: 1.135307788848877\n",
      "Epoch 1090/3000, Train Loss: 0.7936, Time: 1.1092150211334229\n",
      "Epoch 1091/3000, Train Loss: 0.8267, Time: 1.1256661415100098\n",
      "Epoch 1092/3000, Train Loss: 0.9230, Time: 1.137075424194336\n",
      "Epoch 1093/3000, Train Loss: 0.7897, Time: 1.110717535018921\n",
      "Epoch 1094/3000, Train Loss: 0.8529, Time: 1.113224983215332\n",
      "Epoch 1095/3000, Train Loss: 0.9730, Time: 1.1443846225738525\n",
      "Epoch 1096/3000, Train Loss: 0.8048, Time: 1.1765615940093994\n",
      "Epoch 1097/3000, Train Loss: 0.8359, Time: 1.164236068725586\n",
      "Epoch 1098/3000, Train Loss: 0.7871, Time: 1.1175997257232666\n",
      "Epoch 1099/3000, Train Loss: 0.8201, Time: 1.1245384216308594\n",
      "Epoch 1100/3000, Train Loss: 0.8133, Time: 1.1072821617126465\n",
      "Epoch 1101/3000, Train Loss: 0.9350, Time: 1.104191780090332\n",
      "Epoch 1102/3000, Train Loss: 0.8097, Time: 1.1217007637023926\n",
      "Epoch 1103/3000, Train Loss: 0.8427, Time: 1.1413629055023193\n",
      "Epoch 1104/3000, Train Loss: 0.7781, Time: 1.1124329566955566\n",
      "Epoch 1105/3000, Train Loss: 0.9319, Time: 1.1492595672607422\n",
      "Epoch 1106/3000, Train Loss: 0.7693, Time: 1.1140165328979492\n",
      "Epoch 1107/3000, Train Loss: 0.8059, Time: 1.1090989112854004\n",
      "Epoch 1108/3000, Train Loss: 0.9878, Time: 1.1224424839019775\n",
      "Epoch 1109/3000, Train Loss: 0.7838, Time: 1.1799015998840332\n",
      "Epoch 1110/3000, Train Loss: 0.7971, Time: 1.1506445407867432\n",
      "Epoch 1111/3000, Train Loss: 0.8132, Time: 1.1263427734375\n",
      "Epoch 1112/3000, Train Loss: 0.8653, Time: 1.1118860244750977\n",
      "Epoch 1113/3000, Train Loss: 0.8309, Time: 1.1153984069824219\n",
      "Epoch 1114/3000, Train Loss: 0.7975, Time: 1.1207575798034668\n",
      "Epoch 1115/3000, Train Loss: 0.7699, Time: 1.1216673851013184\n",
      "Epoch 1116/3000, Train Loss: 0.8388, Time: 1.1449284553527832\n",
      "Epoch 1117/3000, Train Loss: 0.8405, Time: 1.1361374855041504\n",
      "Epoch 1118/3000, Train Loss: 0.7958, Time: 1.1920781135559082\n",
      "Epoch 1119/3000, Train Loss: 0.8318, Time: 1.1907169818878174\n",
      "Epoch 1120/3000, Train Loss: 0.7987, Time: 1.108957052230835\n",
      "Epoch 1121/3000, Train Loss: 0.8392, Time: 1.1142563819885254\n",
      "Epoch 1122/3000, Train Loss: 0.8108, Time: 1.1420044898986816\n",
      "Epoch 1123/3000, Train Loss: 0.8109, Time: 1.1610970497131348\n",
      "Epoch 1124/3000, Train Loss: 0.8244, Time: 1.133260726928711\n",
      "Epoch 1125/3000, Train Loss: 0.7906, Time: 1.1093833446502686\n",
      "Epoch 1126/3000, Train Loss: 0.7891, Time: 1.1059379577636719\n",
      "Epoch 1127/3000, Train Loss: 0.8882, Time: 1.126344919204712\n",
      "Epoch 1128/3000, Train Loss: 0.8351, Time: 1.143850326538086\n",
      "Epoch 1129/3000, Train Loss: 0.8134, Time: 1.1213562488555908\n",
      "Epoch 1130/3000, Train Loss: 0.8353, Time: 1.1323304176330566\n",
      "Epoch 1131/3000, Train Loss: 0.8902, Time: 1.099487543106079\n",
      "Epoch 1132/3000, Train Loss: 0.7951, Time: 1.1423869132995605\n",
      "Epoch 1133/3000, Train Loss: 0.7743, Time: 1.138664960861206\n",
      "Epoch 1134/3000, Train Loss: 0.8134, Time: 1.1791136264801025\n",
      "Epoch 1135/3000, Train Loss: 0.8162, Time: 1.125633716583252\n",
      "Epoch 1136/3000, Train Loss: 0.8328, Time: 1.1154210567474365\n",
      "Epoch 1137/3000, Train Loss: 0.8202, Time: 1.142726182937622\n",
      "Epoch 1138/3000, Train Loss: 0.7901, Time: 1.1049268245697021\n",
      "Epoch 1139/3000, Train Loss: 0.7933, Time: 1.1253416538238525\n",
      "Epoch 1140/3000, Train Loss: 0.8954, Time: 1.1689858436584473\n",
      "Epoch 1141/3000, Train Loss: 0.8863, Time: 1.1476023197174072\n",
      "Epoch 1142/3000, Train Loss: 0.7725, Time: 1.1134166717529297\n",
      "Epoch 1143/3000, Train Loss: 0.8270, Time: 1.1115918159484863\n",
      "Epoch 1144/3000, Train Loss: 0.8202, Time: 1.1117901802062988\n",
      "Epoch 1145/3000, Train Loss: 0.8074, Time: 1.1164782047271729\n",
      "Epoch 1146/3000, Train Loss: 0.8099, Time: 1.1191363334655762\n",
      "Epoch 1147/3000, Train Loss: 0.7845, Time: 1.1386573314666748\n",
      "Epoch 1148/3000, Train Loss: 0.9091, Time: 1.1450111865997314\n",
      "Epoch 1149/3000, Train Loss: 0.8627, Time: 1.1629528999328613\n",
      "Epoch 1150/3000, Train Loss: 0.7891, Time: 1.1099488735198975\n",
      "Epoch 1151/3000, Train Loss: 0.7653, Time: 1.1560440063476562\n",
      "Epoch 1152/3000, Train Loss: 0.7995, Time: 1.1097500324249268\n",
      "Epoch 1153/3000, Train Loss: 0.9247, Time: 1.113520860671997\n",
      "Epoch 1154/3000, Train Loss: 0.8905, Time: 1.1323108673095703\n",
      "Epoch 1155/3000, Train Loss: 0.8150, Time: 1.1452088356018066\n",
      "Epoch 1156/3000, Train Loss: 0.8265, Time: 1.162811517715454\n",
      "Epoch 1157/3000, Train Loss: 0.8358, Time: 1.1330153942108154\n",
      "Epoch 1158/3000, Train Loss: 0.8054, Time: 1.1160681247711182\n",
      "Epoch 1159/3000, Train Loss: 0.8714, Time: 1.0615184307098389\n",
      "Epoch 1160/3000, Train Loss: 0.8395, Time: 1.0178704261779785\n",
      "Epoch 1161/3000, Train Loss: 0.8523, Time: 1.0373115539550781\n",
      "Epoch 1162/3000, Train Loss: 0.8236, Time: 1.1376898288726807\n",
      "Epoch 1163/3000, Train Loss: 0.7890, Time: 1.1419274806976318\n",
      "Epoch 1164/3000, Train Loss: 0.7997, Time: 1.1304399967193604\n",
      "Epoch 1165/3000, Train Loss: 0.8382, Time: 1.1144194602966309\n",
      "Epoch 1166/3000, Train Loss: 0.7745, Time: 1.1170294284820557\n",
      "Epoch 1167/3000, Train Loss: 0.9093, Time: 1.1234095096588135\n",
      "Epoch 1168/3000, Train Loss: 0.8096, Time: 1.136000394821167\n",
      "Epoch 1169/3000, Train Loss: 0.7807, Time: 1.1378440856933594\n",
      "Epoch 1170/3000, Train Loss: 0.8387, Time: 1.108274221420288\n",
      "Epoch 1171/3000, Train Loss: 0.8927, Time: 1.2370455265045166\n",
      "Epoch 1172/3000, Train Loss: 0.8895, Time: 1.2024555206298828\n",
      "Epoch 1173/3000, Train Loss: 0.8814, Time: 1.1413285732269287\n",
      "Epoch 1174/3000, Train Loss: 0.7829, Time: 1.1346585750579834\n",
      "Epoch 1175/3000, Train Loss: 0.7852, Time: 1.1283974647521973\n",
      "Epoch 1176/3000, Train Loss: 0.7621, Time: 1.1315174102783203\n",
      "Epoch 1177/3000, Train Loss: 0.8365, Time: 1.1183528900146484\n",
      "Epoch 1178/3000, Train Loss: 0.7899, Time: 1.1501705646514893\n",
      "Epoch 1179/3000, Train Loss: 0.8077, Time: 1.1295602321624756\n",
      "Epoch 1180/3000, Train Loss: 0.7996, Time: 1.1313624382019043\n",
      "Epoch 1181/3000, Train Loss: 0.9261, Time: 1.098095178604126\n",
      "Epoch 1182/3000, Train Loss: 0.7760, Time: 1.1012465953826904\n",
      "Epoch 1183/3000, Train Loss: 0.8117, Time: 1.1187684535980225\n",
      "Epoch 1184/3000, Train Loss: 0.8045, Time: 1.1329214572906494\n",
      "Epoch 1185/3000, Train Loss: 0.7784, Time: 1.1328699588775635\n",
      "Epoch 1186/3000, Train Loss: 0.8876, Time: 1.1150760650634766\n",
      "Epoch 1187/3000, Train Loss: 0.8168, Time: 1.0993688106536865\n",
      "Epoch 1188/3000, Train Loss: 0.7733, Time: 1.1048109531402588\n",
      "Epoch 1189/3000, Train Loss: 0.7959, Time: 1.1049423217773438\n",
      "Epoch 1190/3000, Train Loss: 0.8083, Time: 1.1058640480041504\n",
      "Epoch 1191/3000, Train Loss: 0.7646, Time: 1.1299011707305908\n",
      "Epoch 1192/3000, Train Loss: 0.7697, Time: 1.1193289756774902\n",
      "Epoch 1193/3000, Train Loss: 0.7779, Time: 1.1150002479553223\n",
      "Epoch 1194/3000, Train Loss: 0.8043, Time: 1.0970942974090576\n",
      "Epoch 1195/3000, Train Loss: 0.8705, Time: 1.1031579971313477\n",
      "Epoch 1196/3000, Train Loss: 0.7748, Time: 1.121352195739746\n",
      "Epoch 1197/3000, Train Loss: 0.8155, Time: 1.140716552734375\n",
      "Epoch 1198/3000, Train Loss: 0.7909, Time: 1.1173715591430664\n",
      "Epoch 1199/3000, Train Loss: 0.7897, Time: 1.1255409717559814\n",
      "Epoch 1200/3000, Train Loss: 0.7897, Time: 1.133439302444458\n",
      "Epoch 1201/3000, Train Loss: 0.7835, Time: 1.1182162761688232\n",
      "Epoch 1202/3000, Train Loss: 0.7833, Time: 1.1047172546386719\n",
      "Epoch 1203/3000, Train Loss: 0.7830, Time: 1.105285882949829\n",
      "Epoch 1204/3000, Train Loss: 0.8480, Time: 1.106078863143921\n",
      "Epoch 1205/3000, Train Loss: 0.9223, Time: 1.129091739654541\n",
      "Epoch 1206/3000, Train Loss: 0.8153, Time: 1.150655746459961\n",
      "Epoch 1207/3000, Train Loss: 0.8286, Time: 1.1388204097747803\n",
      "Epoch 1208/3000, Train Loss: 0.8321, Time: 1.1642804145812988\n",
      "Epoch 1209/3000, Train Loss: 0.8691, Time: 1.1243407726287842\n",
      "Epoch 1210/3000, Train Loss: 0.8339, Time: 1.111598253250122\n",
      "Epoch 1211/3000, Train Loss: 0.7986, Time: 1.1090452671051025\n",
      "Epoch 1212/3000, Train Loss: 0.7859, Time: 1.1130998134613037\n",
      "Epoch 1213/3000, Train Loss: 0.7871, Time: 1.1073596477508545\n",
      "Epoch 1214/3000, Train Loss: 0.8141, Time: 1.1137425899505615\n",
      "Epoch 1215/3000, Train Loss: 0.7899, Time: 1.150810956954956\n",
      "Epoch 1216/3000, Train Loss: 0.8668, Time: 1.0986475944519043\n",
      "Epoch 1217/3000, Train Loss: 0.8324, Time: 1.0976717472076416\n",
      "Epoch 1218/3000, Train Loss: 0.8568, Time: 1.1270534992218018\n",
      "Epoch 1219/3000, Train Loss: 0.8069, Time: 1.0956447124481201\n",
      "Epoch 1220/3000, Train Loss: 0.7803, Time: 1.092482089996338\n",
      "Epoch 1221/3000, Train Loss: 0.7880, Time: 1.1164836883544922\n",
      "Epoch 1222/3000, Train Loss: 0.7780, Time: 1.1212236881256104\n",
      "Epoch 1223/3000, Train Loss: 0.8859, Time: 1.1168456077575684\n",
      "Epoch 1224/3000, Train Loss: 0.8308, Time: 1.174551248550415\n",
      "Epoch 1225/3000, Train Loss: 0.8104, Time: 1.167057991027832\n",
      "Epoch 1226/3000, Train Loss: 0.7849, Time: 1.1011435985565186\n",
      "Epoch 1227/3000, Train Loss: 0.7890, Time: 1.104433536529541\n",
      "Epoch 1228/3000, Train Loss: 0.7718, Time: 1.1145050525665283\n",
      "Epoch 1229/3000, Train Loss: 0.8829, Time: 1.1290240287780762\n",
      "Epoch 1230/3000, Train Loss: 0.8496, Time: 1.1144845485687256\n",
      "Epoch 1231/3000, Train Loss: 0.8743, Time: 1.1026957035064697\n",
      "Epoch 1232/3000, Train Loss: 0.8070, Time: 1.110041618347168\n",
      "Epoch 1233/3000, Train Loss: 0.8485, Time: 1.0980565547943115\n",
      "Epoch 1234/3000, Train Loss: 0.8477, Time: 1.10347318649292\n",
      "Epoch 1235/3000, Train Loss: 0.8054, Time: 1.1309919357299805\n",
      "Epoch 1236/3000, Train Loss: 0.8018, Time: 1.120408296585083\n",
      "Epoch 1237/3000, Train Loss: 0.7836, Time: 1.1023430824279785\n",
      "Epoch 1238/3000, Train Loss: 0.8912, Time: 1.09922456741333\n",
      "Epoch 1239/3000, Train Loss: 0.7901, Time: 1.108196496963501\n",
      "Epoch 1240/3000, Train Loss: 0.8028, Time: 1.1053483486175537\n",
      "Epoch 1241/3000, Train Loss: 0.8123, Time: 1.1057910919189453\n",
      "Epoch 1242/3000, Train Loss: 0.8957, Time: 1.1802434921264648\n",
      "Epoch 1243/3000, Train Loss: 0.8737, Time: 1.1592001914978027\n",
      "Epoch 1244/3000, Train Loss: 0.8350, Time: 1.1118292808532715\n",
      "Epoch 1245/3000, Train Loss: 0.8330, Time: 1.1163363456726074\n",
      "Epoch 1246/3000, Train Loss: 0.8076, Time: 1.1114990711212158\n",
      "Epoch 1247/3000, Train Loss: 0.9142, Time: 1.132866382598877\n",
      "Epoch 1248/3000, Train Loss: 0.7812, Time: 1.1125514507293701\n",
      "Epoch 1249/3000, Train Loss: 0.7662, Time: 1.129882574081421\n",
      "Epoch 1250/3000, Train Loss: 0.8034, Time: 1.1384642124176025\n",
      "Epoch 1251/3000, Train Loss: 0.8088, Time: 1.1360948085784912\n",
      "Epoch 1252/3000, Train Loss: 0.7982, Time: 1.1429617404937744\n",
      "Epoch 1253/3000, Train Loss: 0.8265, Time: 1.1117706298828125\n",
      "Epoch 1254/3000, Train Loss: 0.8496, Time: 1.1011180877685547\n",
      "Epoch 1255/3000, Train Loss: 0.8203, Time: 1.0124874114990234\n",
      "Epoch 1256/3000, Train Loss: 0.7958, Time: 1.0057835578918457\n",
      "Epoch 1257/3000, Train Loss: 0.9083, Time: 1.0122900009155273\n",
      "Epoch 1258/3000, Train Loss: 0.7866, Time: 1.0104591846466064\n",
      "Epoch 1259/3000, Train Loss: 0.8258, Time: 0.9910175800323486\n",
      "Epoch 1260/3000, Train Loss: 0.7981, Time: 0.9869413375854492\n",
      "Epoch 1261/3000, Train Loss: 0.8287, Time: 0.9913883209228516\n",
      "Epoch 1262/3000, Train Loss: 0.8594, Time: 0.9892759323120117\n",
      "Epoch 1263/3000, Train Loss: 0.7949, Time: 0.9881365299224854\n",
      "Epoch 1264/3000, Train Loss: 0.8878, Time: 0.9993958473205566\n",
      "Epoch 1265/3000, Train Loss: 0.7538, Time: 1.026660442352295\n",
      "Epoch 1266/3000, Train Loss: 0.8239, Time: 1.0320284366607666\n",
      "Epoch 1267/3000, Train Loss: 0.7979, Time: 1.0011510848999023\n",
      "Epoch 1268/3000, Train Loss: 0.7743, Time: 0.9912440776824951\n",
      "Epoch 1269/3000, Train Loss: 0.8703, Time: 1.0147480964660645\n",
      "Epoch 1270/3000, Train Loss: 0.7905, Time: 1.0085091590881348\n",
      "Epoch 1271/3000, Train Loss: 0.7854, Time: 0.9950971603393555\n",
      "Epoch 1272/3000, Train Loss: 0.8028, Time: 1.0123145580291748\n",
      "Epoch 1273/3000, Train Loss: 0.8994, Time: 1.000244140625\n",
      "Epoch 1274/3000, Train Loss: 0.7819, Time: 0.9908773899078369\n",
      "Epoch 1275/3000, Train Loss: 0.9302, Time: 0.9965610504150391\n",
      "Epoch 1276/3000, Train Loss: 0.9207, Time: 1.006134033203125\n",
      "Epoch 1277/3000, Train Loss: 0.7954, Time: 1.0023748874664307\n",
      "Epoch 1278/3000, Train Loss: 0.7920, Time: 0.995978593826294\n",
      "Epoch 1279/3000, Train Loss: 0.7691, Time: 0.9831957817077637\n",
      "Epoch 1280/3000, Train Loss: 0.7844, Time: 1.0072014331817627\n",
      "Epoch 1281/3000, Train Loss: 0.7718, Time: 1.1988437175750732\n",
      "Epoch 1282/3000, Train Loss: 0.7657, Time: 1.0685789585113525\n",
      "Epoch 1283/3000, Train Loss: 0.7923, Time: 1.0063974857330322\n",
      "Epoch 1284/3000, Train Loss: 0.8092, Time: 0.9916784763336182\n",
      "Epoch 1285/3000, Train Loss: 0.7727, Time: 0.9886288642883301\n",
      "Epoch 1286/3000, Train Loss: 0.7514, Time: 0.9955587387084961\n",
      "Epoch 1287/3000, Train Loss: 0.7858, Time: 1.0062987804412842\n",
      "Epoch 1288/3000, Train Loss: 0.7980, Time: 1.009990930557251\n",
      "Epoch 1289/3000, Train Loss: 0.8312, Time: 1.0284862518310547\n",
      "Epoch 1290/3000, Train Loss: 0.8989, Time: 0.9983477592468262\n",
      "Epoch 1291/3000, Train Loss: 0.8037, Time: 0.9879353046417236\n",
      "Epoch 1292/3000, Train Loss: 0.7964, Time: 0.9857265949249268\n",
      "Epoch 1293/3000, Train Loss: 0.8349, Time: 0.9943430423736572\n",
      "Epoch 1294/3000, Train Loss: 0.8316, Time: 1.0174767971038818\n",
      "Epoch 1295/3000, Train Loss: 0.8000, Time: 1.0525946617126465\n",
      "Epoch 1296/3000, Train Loss: 0.7961, Time: 1.014798641204834\n",
      "Epoch 1297/3000, Train Loss: 0.8063, Time: 0.993025541305542\n",
      "Epoch 1298/3000, Train Loss: 0.7745, Time: 0.9896419048309326\n",
      "Epoch 1299/3000, Train Loss: 0.8138, Time: 0.9910848140716553\n",
      "Epoch 1300/3000, Train Loss: 0.7811, Time: 1.0223493576049805\n",
      "Epoch 1301/3000, Train Loss: 0.7959, Time: 1.015366554260254\n",
      "Epoch 1302/3000, Train Loss: 0.7753, Time: 0.9974501132965088\n",
      "Epoch 1303/3000, Train Loss: 0.8011, Time: 0.994574785232544\n",
      "Epoch 1304/3000, Train Loss: 0.8189, Time: 0.9859931468963623\n",
      "Epoch 1305/3000, Train Loss: 0.8313, Time: 1.006439447402954\n",
      "Epoch 1306/3000, Train Loss: 0.8174, Time: 1.0104222297668457\n",
      "Epoch 1307/3000, Train Loss: 0.7750, Time: 0.9895162582397461\n",
      "Epoch 1308/3000, Train Loss: 0.7823, Time: 0.9848508834838867\n",
      "Epoch 1309/3000, Train Loss: 0.7850, Time: 0.9876914024353027\n",
      "Epoch 1310/3000, Train Loss: 0.8223, Time: 1.0079338550567627\n",
      "Epoch 1311/3000, Train Loss: 0.7755, Time: 1.1046137809753418\n",
      "Epoch 1312/3000, Train Loss: 0.8280, Time: 1.1420888900756836\n",
      "Epoch 1313/3000, Train Loss: 0.7897, Time: 1.1007766723632812\n",
      "Epoch 1314/3000, Train Loss: 0.7587, Time: 1.0212583541870117\n",
      "Epoch 1315/3000, Train Loss: 0.8187, Time: 0.9827711582183838\n",
      "Epoch 1316/3000, Train Loss: 0.7870, Time: 0.9841940402984619\n",
      "Epoch 1317/3000, Train Loss: 0.7905, Time: 0.9855399131774902\n",
      "Epoch 1318/3000, Train Loss: 0.8200, Time: 0.9847395420074463\n",
      "Epoch 1319/3000, Train Loss: 0.7879, Time: 0.9881761074066162\n",
      "Epoch 1320/3000, Train Loss: 0.8130, Time: 1.00150465965271\n",
      "Epoch 1321/3000, Train Loss: 0.8074, Time: 1.0154390335083008\n",
      "Epoch 1322/3000, Train Loss: 0.8041, Time: 1.007378339767456\n",
      "Epoch 1323/3000, Train Loss: 0.7806, Time: 0.990532398223877\n",
      "Epoch 1324/3000, Train Loss: 0.7783, Time: 0.9848473072052002\n",
      "Epoch 1325/3000, Train Loss: 0.7648, Time: 0.986353874206543\n",
      "Epoch 1326/3000, Train Loss: 0.7649, Time: 0.9758632183074951\n",
      "Epoch 1327/3000, Train Loss: 0.7681, Time: 1.000401496887207\n",
      "Epoch 1328/3000, Train Loss: 0.8197, Time: 1.0058567523956299\n",
      "Epoch 1329/3000, Train Loss: 0.7836, Time: 1.012345552444458\n",
      "Epoch 1330/3000, Train Loss: 0.8431, Time: 0.9962868690490723\n",
      "Epoch 1331/3000, Train Loss: 0.8026, Time: 1.0155346393585205\n",
      "Epoch 1332/3000, Train Loss: 0.7936, Time: 0.9814116954803467\n",
      "Epoch 1333/3000, Train Loss: 0.8005, Time: 0.981015682220459\n",
      "Epoch 1334/3000, Train Loss: 0.7661, Time: 0.9810714721679688\n",
      "Epoch 1335/3000, Train Loss: 0.7488, Time: 0.9937446117401123\n",
      "Epoch 1336/3000, Train Loss: 0.7741, Time: 1.008528470993042\n",
      "Epoch 1337/3000, Train Loss: 0.7845, Time: 1.0172715187072754\n",
      "Epoch 1338/3000, Train Loss: 0.8608, Time: 1.0182304382324219\n",
      "Epoch 1339/3000, Train Loss: 0.7698, Time: 0.9876830577850342\n",
      "Epoch 1340/3000, Train Loss: 0.7945, Time: 1.084601879119873\n",
      "Epoch 1341/3000, Train Loss: 0.8777, Time: 0.9830267429351807\n",
      "Epoch 1342/3000, Train Loss: 0.7694, Time: 1.0616044998168945\n",
      "Epoch 1343/3000, Train Loss: 0.8026, Time: 0.9856703281402588\n",
      "Epoch 1344/3000, Train Loss: 0.7749, Time: 0.9893383979797363\n",
      "Epoch 1345/3000, Train Loss: 0.8066, Time: 1.0153272151947021\n",
      "Epoch 1346/3000, Train Loss: 0.8271, Time: 1.0107026100158691\n",
      "Epoch 1347/3000, Train Loss: 0.8697, Time: 0.9901602268218994\n",
      "Epoch 1348/3000, Train Loss: 0.7743, Time: 0.9832279682159424\n",
      "Epoch 1349/3000, Train Loss: 0.8007, Time: 1.1046068668365479\n",
      "Epoch 1350/3000, Train Loss: 0.7966, Time: 1.1133091449737549\n",
      "Epoch 1351/3000, Train Loss: 0.8021, Time: 1.1266310214996338\n",
      "Epoch 1352/3000, Train Loss: 0.8169, Time: 1.1406376361846924\n",
      "Epoch 1353/3000, Train Loss: 0.8471, Time: 1.1488018035888672\n",
      "Epoch 1354/3000, Train Loss: 0.8180, Time: 1.1448922157287598\n",
      "Epoch 1355/3000, Train Loss: 0.7843, Time: 1.1112146377563477\n",
      "Epoch 1356/3000, Train Loss: 0.7773, Time: 1.1174776554107666\n",
      "Epoch 1357/3000, Train Loss: 0.8194, Time: 1.1077301502227783\n",
      "Epoch 1358/3000, Train Loss: 0.7773, Time: 1.1069755554199219\n",
      "Epoch 1359/3000, Train Loss: 0.7763, Time: 1.1110506057739258\n",
      "Epoch 1360/3000, Train Loss: 0.7715, Time: 1.1412482261657715\n",
      "Epoch 1361/3000, Train Loss: 0.7865, Time: 1.1725311279296875\n",
      "Epoch 1362/3000, Train Loss: 0.8441, Time: 1.1349730491638184\n",
      "Epoch 1363/3000, Train Loss: 0.7846, Time: 1.1096138954162598\n",
      "Epoch 1364/3000, Train Loss: 0.7791, Time: 1.1429932117462158\n",
      "Epoch 1365/3000, Train Loss: 0.8021, Time: 1.1304991245269775\n",
      "Epoch 1366/3000, Train Loss: 0.8301, Time: 1.1275715827941895\n",
      "Epoch 1367/3000, Train Loss: 0.7914, Time: 1.1206409931182861\n",
      "Epoch 1368/3000, Train Loss: 0.7989, Time: 1.141937017440796\n",
      "Epoch 1369/3000, Train Loss: 0.7737, Time: 1.12955641746521\n",
      "Epoch 1370/3000, Train Loss: 0.8112, Time: 1.133610725402832\n",
      "Epoch 1371/3000, Train Loss: 0.8004, Time: 1.105794906616211\n",
      "Epoch 1372/3000, Train Loss: 0.8393, Time: 1.109985589981079\n",
      "Epoch 1373/3000, Train Loss: 0.8017, Time: 1.09222412109375\n",
      "Epoch 1374/3000, Train Loss: 0.8654, Time: 1.1243627071380615\n",
      "Epoch 1375/3000, Train Loss: 0.7889, Time: 1.165748119354248\n",
      "Epoch 1376/3000, Train Loss: 0.8029, Time: 1.1170735359191895\n",
      "Epoch 1377/3000, Train Loss: 0.8169, Time: 1.1115002632141113\n",
      "Epoch 1378/3000, Train Loss: 0.7574, Time: 1.128647804260254\n",
      "Epoch 1379/3000, Train Loss: 0.7607, Time: 1.0339288711547852\n",
      "Epoch 1380/3000, Train Loss: 0.8134, Time: 1.0384325981140137\n",
      "Epoch 1381/3000, Train Loss: 0.7873, Time: 1.09140944480896\n",
      "Epoch 1382/3000, Train Loss: 0.7705, Time: 1.13716459274292\n",
      "Epoch 1383/3000, Train Loss: 0.7615, Time: 1.1438977718353271\n",
      "Epoch 1384/3000, Train Loss: 0.7726, Time: 1.1128416061401367\n",
      "Epoch 1385/3000, Train Loss: 0.8325, Time: 1.111341953277588\n",
      "Epoch 1386/3000, Train Loss: 0.7823, Time: 1.1104638576507568\n",
      "Epoch 1387/3000, Train Loss: 0.8652, Time: 1.1185755729675293\n",
      "Epoch 1388/3000, Train Loss: 0.8125, Time: 1.1132357120513916\n",
      "Epoch 1389/3000, Train Loss: 0.8021, Time: 1.145317554473877\n",
      "Epoch 1390/3000, Train Loss: 0.7524, Time: 1.1457674503326416\n",
      "Epoch 1391/3000, Train Loss: 0.7682, Time: 1.1449408531188965\n",
      "Epoch 1392/3000, Train Loss: 0.7784, Time: 1.1284070014953613\n",
      "Epoch 1393/3000, Train Loss: 0.7948, Time: 1.112093210220337\n",
      "Epoch 1394/3000, Train Loss: 0.8432, Time: 1.114884376525879\n",
      "Epoch 1395/3000, Train Loss: 0.7972, Time: 1.1908435821533203\n",
      "Epoch 1396/3000, Train Loss: 0.7940, Time: 1.1129226684570312\n",
      "Epoch 1397/3000, Train Loss: 0.7657, Time: 1.2076866626739502\n",
      "Epoch 1398/3000, Train Loss: 0.7702, Time: 1.1437509059906006\n",
      "Epoch 1399/3000, Train Loss: 0.7912, Time: 1.169074535369873\n",
      "Epoch 1400/3000, Train Loss: 0.7650, Time: 1.1164135932922363\n",
      "Epoch 1401/3000, Train Loss: 0.7661, Time: 1.1082684993743896\n",
      "Epoch 1402/3000, Train Loss: 0.7953, Time: 1.1140174865722656\n",
      "Epoch 1403/3000, Train Loss: 0.8215, Time: 1.1062660217285156\n",
      "Epoch 1404/3000, Train Loss: 0.7766, Time: 1.1327471733093262\n",
      "Epoch 1405/3000, Train Loss: 0.7672, Time: 1.1382462978363037\n",
      "Epoch 1406/3000, Train Loss: 0.7541, Time: 1.1219878196716309\n",
      "Epoch 1407/3000, Train Loss: 0.8023, Time: 1.1043291091918945\n",
      "Epoch 1408/3000, Train Loss: 0.7883, Time: 1.1276733875274658\n",
      "Epoch 1409/3000, Train Loss: 0.7848, Time: 1.1118817329406738\n",
      "Epoch 1410/3000, Train Loss: 0.7732, Time: 1.1398310661315918\n",
      "Epoch 1411/3000, Train Loss: 0.8311, Time: 1.1332404613494873\n",
      "Epoch 1412/3000, Train Loss: 0.7861, Time: 1.127385139465332\n",
      "Epoch 1413/3000, Train Loss: 0.7697, Time: 1.10518479347229\n",
      "Epoch 1414/3000, Train Loss: 0.7767, Time: 1.1274194717407227\n",
      "Epoch 1415/3000, Train Loss: 0.8056, Time: 1.1514496803283691\n",
      "Epoch 1416/3000, Train Loss: 0.7687, Time: 1.1262767314910889\n",
      "Epoch 1417/3000, Train Loss: 0.7604, Time: 1.1241958141326904\n",
      "Epoch 1418/3000, Train Loss: 0.8705, Time: 1.1183357238769531\n",
      "Epoch 1419/3000, Train Loss: 0.7689, Time: 1.106335163116455\n",
      "Epoch 1420/3000, Train Loss: 0.7571, Time: 1.0977158546447754\n",
      "Epoch 1421/3000, Train Loss: 0.8688, Time: 1.096508502960205\n",
      "Epoch 1422/3000, Train Loss: 0.8012, Time: 1.0979588031768799\n",
      "Epoch 1423/3000, Train Loss: 0.8024, Time: 1.1172404289245605\n",
      "Epoch 1424/3000, Train Loss: 0.8990, Time: 1.1368718147277832\n",
      "Epoch 1425/3000, Train Loss: 0.8103, Time: 1.1377394199371338\n",
      "Epoch 1426/3000, Train Loss: 0.8315, Time: 1.1028554439544678\n",
      "Epoch 1427/3000, Train Loss: 0.7593, Time: 1.1030216217041016\n",
      "Epoch 1428/3000, Train Loss: 0.7562, Time: 1.1037917137145996\n",
      "Epoch 1429/3000, Train Loss: 0.7798, Time: 1.1291248798370361\n",
      "Epoch 1430/3000, Train Loss: 0.7988, Time: 1.1287281513214111\n",
      "Epoch 1431/3000, Train Loss: 0.7859, Time: 1.1341090202331543\n",
      "Epoch 1432/3000, Train Loss: 0.7774, Time: 1.1649587154388428\n",
      "Epoch 1433/3000, Train Loss: 0.7655, Time: 1.104163408279419\n",
      "Epoch 1434/3000, Train Loss: 0.8020, Time: 1.1087276935577393\n",
      "Epoch 1435/3000, Train Loss: 0.7999, Time: 1.1037297248840332\n",
      "Epoch 1436/3000, Train Loss: 0.7971, Time: 1.1449882984161377\n",
      "Epoch 1437/3000, Train Loss: 0.7418, Time: 1.1322834491729736\n",
      "Epoch 1438/3000, Train Loss: 0.7436, Time: 1.1343185901641846\n",
      "Epoch 1439/3000, Train Loss: 0.7900, Time: 1.1065762042999268\n",
      "Epoch 1440/3000, Train Loss: 0.7469, Time: 1.108341932296753\n",
      "Epoch 1441/3000, Train Loss: 0.7686, Time: 1.1106550693511963\n",
      "Epoch 1442/3000, Train Loss: 0.7697, Time: 1.1105599403381348\n",
      "Epoch 1443/3000, Train Loss: 0.8153, Time: 1.1521944999694824\n",
      "Epoch 1444/3000, Train Loss: 0.7888, Time: 1.1380724906921387\n",
      "Epoch 1445/3000, Train Loss: 0.7688, Time: 1.11027193069458\n",
      "Epoch 1446/3000, Train Loss: 0.7932, Time: 1.1266794204711914\n",
      "Epoch 1447/3000, Train Loss: 0.7682, Time: 1.0942120552062988\n",
      "Epoch 1448/3000, Train Loss: 0.7635, Time: 1.3358075618743896\n",
      "Epoch 1449/3000, Train Loss: 0.7649, Time: 1.1497769355773926\n",
      "Epoch 1450/3000, Train Loss: 0.8541, Time: 1.1129918098449707\n",
      "Epoch 1451/3000, Train Loss: 0.7941, Time: 1.1041285991668701\n",
      "Epoch 1452/3000, Train Loss: 0.7564, Time: 1.1050760746002197\n",
      "Epoch 1453/3000, Train Loss: 0.7546, Time: 1.1091077327728271\n",
      "Epoch 1454/3000, Train Loss: 0.7645, Time: 1.1364104747772217\n",
      "Epoch 1455/3000, Train Loss: 0.7600, Time: 1.1945552825927734\n",
      "Epoch 1456/3000, Train Loss: 0.7723, Time: 1.1096272468566895\n",
      "Epoch 1457/3000, Train Loss: 0.8785, Time: 1.145556926727295\n",
      "Epoch 1458/3000, Train Loss: 0.8047, Time: 1.1364641189575195\n",
      "Epoch 1459/3000, Train Loss: 0.7837, Time: 1.1369469165802002\n",
      "Epoch 1460/3000, Train Loss: 0.7778, Time: 1.1709856986999512\n",
      "Epoch 1461/3000, Train Loss: 0.7642, Time: 1.1570475101470947\n",
      "Epoch 1462/3000, Train Loss: 0.7613, Time: 1.1511008739471436\n",
      "Epoch 1463/3000, Train Loss: 0.8021, Time: 1.1436257362365723\n",
      "Epoch 1464/3000, Train Loss: 0.8924, Time: 1.135303020477295\n",
      "Epoch 1465/3000, Train Loss: 0.8097, Time: 1.1468086242675781\n",
      "Epoch 1466/3000, Train Loss: 0.7722, Time: 1.1517157554626465\n",
      "Epoch 1467/3000, Train Loss: 0.7782, Time: 1.1779148578643799\n",
      "Epoch 1468/3000, Train Loss: 0.8067, Time: 1.1880533695220947\n",
      "Epoch 1469/3000, Train Loss: 0.7864, Time: 1.1455779075622559\n",
      "Epoch 1470/3000, Train Loss: 0.7700, Time: 1.1435906887054443\n",
      "Epoch 1471/3000, Train Loss: 0.7669, Time: 1.1480882167816162\n",
      "Epoch 1472/3000, Train Loss: 0.7617, Time: 1.1603443622589111\n",
      "Epoch 1473/3000, Train Loss: 0.7499, Time: 1.201371431350708\n",
      "Epoch 1474/3000, Train Loss: 0.7638, Time: 1.0657007694244385\n",
      "Epoch 1475/3000, Train Loss: 0.7735, Time: 1.0765953063964844\n",
      "Epoch 1476/3000, Train Loss: 0.9659, Time: 1.08931565284729\n",
      "Epoch 1477/3000, Train Loss: 0.8929, Time: 1.1368601322174072\n",
      "Epoch 1478/3000, Train Loss: 0.9003, Time: 1.107590675354004\n",
      "Epoch 1479/3000, Train Loss: 0.7730, Time: 1.1349411010742188\n",
      "Epoch 1480/3000, Train Loss: 0.7931, Time: 1.12601637840271\n",
      "Epoch 1481/3000, Train Loss: 0.7938, Time: 1.1165030002593994\n",
      "Epoch 1482/3000, Train Loss: 0.7559, Time: 1.1223833560943604\n",
      "Epoch 1483/3000, Train Loss: 0.7898, Time: 1.1391634941101074\n",
      "Epoch 1484/3000, Train Loss: 0.8571, Time: 1.1149704456329346\n",
      "Epoch 1485/3000, Train Loss: 0.7844, Time: 1.1081864833831787\n",
      "Epoch 1486/3000, Train Loss: 0.7879, Time: 1.1061437129974365\n",
      "Epoch 1487/3000, Train Loss: 0.7763, Time: 1.1283669471740723\n",
      "Epoch 1488/3000, Train Loss: 0.7587, Time: 1.1357660293579102\n",
      "Epoch 1489/3000, Train Loss: 0.7709, Time: 1.1296417713165283\n",
      "Epoch 1490/3000, Train Loss: 0.7787, Time: 1.1068217754364014\n",
      "Epoch 1491/3000, Train Loss: 0.7931, Time: 1.1046993732452393\n",
      "Epoch 1492/3000, Train Loss: 0.8256, Time: 1.1079680919647217\n",
      "Epoch 1493/3000, Train Loss: 0.8289, Time: 1.121957540512085\n",
      "Epoch 1494/3000, Train Loss: 0.7598, Time: 1.136092185974121\n",
      "Epoch 1495/3000, Train Loss: 0.7757, Time: 1.1188888549804688\n",
      "Epoch 1496/3000, Train Loss: 0.7349, Time: 1.1145331859588623\n",
      "Epoch 1497/3000, Train Loss: 0.7664, Time: 1.1064856052398682\n",
      "Epoch 1498/3000, Train Loss: 0.7992, Time: 1.107691764831543\n",
      "Epoch 1499/3000, Train Loss: 0.7758, Time: 1.1060876846313477\n",
      "Epoch 1500/3000, Train Loss: 0.7810, Time: 1.1778466701507568\n",
      "Epoch 1501/3000, Train Loss: 0.7947, Time: 1.1603329181671143\n",
      "Epoch 1502/3000, Train Loss: 0.7629, Time: 1.1950008869171143\n",
      "Epoch 1503/3000, Train Loss: 0.7515, Time: 1.1079585552215576\n",
      "Epoch 1504/3000, Train Loss: 0.7610, Time: 1.1093339920043945\n",
      "Epoch 1505/3000, Train Loss: 0.7672, Time: 1.1128230094909668\n",
      "Epoch 1506/3000, Train Loss: 0.7934, Time: 1.1461210250854492\n",
      "Epoch 1507/3000, Train Loss: 0.7705, Time: 1.13718843460083\n",
      "Epoch 1508/3000, Train Loss: 0.7750, Time: 1.148848295211792\n",
      "Epoch 1509/3000, Train Loss: 0.7489, Time: 1.103015661239624\n",
      "Epoch 1510/3000, Train Loss: 0.8052, Time: 1.110623836517334\n",
      "Epoch 1511/3000, Train Loss: 0.7647, Time: 1.1084575653076172\n",
      "Epoch 1512/3000, Train Loss: 0.7705, Time: 1.1036550998687744\n",
      "Epoch 1513/3000, Train Loss: 0.7615, Time: 1.099776029586792\n",
      "Epoch 1514/3000, Train Loss: 0.7966, Time: 1.1202917098999023\n",
      "Epoch 1515/3000, Train Loss: 0.7618, Time: 1.1108338832855225\n",
      "Epoch 1516/3000, Train Loss: 0.7496, Time: 1.101600170135498\n",
      "Epoch 1517/3000, Train Loss: 0.7799, Time: 1.1079936027526855\n",
      "Epoch 1518/3000, Train Loss: 0.7587, Time: 1.1165497303009033\n",
      "Epoch 1519/3000, Train Loss: 0.7734, Time: 1.1001050472259521\n",
      "Epoch 1520/3000, Train Loss: 0.8577, Time: 1.1101202964782715\n",
      "Epoch 1521/3000, Train Loss: 0.7641, Time: 1.1288833618164062\n",
      "Epoch 1522/3000, Train Loss: 0.8121, Time: 1.115488052368164\n",
      "Epoch 1523/3000, Train Loss: 0.7868, Time: 1.1021523475646973\n",
      "Epoch 1524/3000, Train Loss: 0.7718, Time: 1.1070384979248047\n",
      "Epoch 1525/3000, Train Loss: 0.7642, Time: 1.149808645248413\n",
      "Epoch 1526/3000, Train Loss: 0.7720, Time: 1.1622371673583984\n",
      "Epoch 1527/3000, Train Loss: 0.7697, Time: 1.1222035884857178\n",
      "Epoch 1528/3000, Train Loss: 0.7725, Time: 1.1340768337249756\n",
      "Epoch 1529/3000, Train Loss: 0.9080, Time: 1.1075680255889893\n",
      "Epoch 1530/3000, Train Loss: 0.7918, Time: 1.1621921062469482\n",
      "Epoch 1531/3000, Train Loss: 0.9058, Time: 1.1183075904846191\n",
      "Epoch 1532/3000, Train Loss: 0.9021, Time: 1.1047461032867432\n",
      "Epoch 1533/3000, Train Loss: 0.7751, Time: 1.1130151748657227\n",
      "Epoch 1534/3000, Train Loss: 0.7701, Time: 1.1317634582519531\n",
      "Epoch 1535/3000, Train Loss: 0.8877, Time: 1.1305718421936035\n",
      "Epoch 1536/3000, Train Loss: 0.7907, Time: 1.1151669025421143\n",
      "Epoch 1537/3000, Train Loss: 0.8136, Time: 1.111495018005371\n",
      "Epoch 1538/3000, Train Loss: 0.8182, Time: 1.1501617431640625\n",
      "Epoch 1539/3000, Train Loss: 0.8304, Time: 1.1052308082580566\n",
      "Epoch 1540/3000, Train Loss: 0.7577, Time: 1.1597890853881836\n",
      "Epoch 1541/3000, Train Loss: 0.7470, Time: 1.1302015781402588\n",
      "Epoch 1542/3000, Train Loss: 0.7966, Time: 1.1161680221557617\n",
      "Epoch 1543/3000, Train Loss: 0.7656, Time: 1.1115946769714355\n",
      "Epoch 1544/3000, Train Loss: 0.7573, Time: 1.1036434173583984\n",
      "Epoch 1545/3000, Train Loss: 0.7563, Time: 1.1067767143249512\n",
      "Epoch 1546/3000, Train Loss: 0.7866, Time: 1.0968821048736572\n",
      "Epoch 1547/3000, Train Loss: 0.7797, Time: 1.1335644721984863\n",
      "Epoch 1548/3000, Train Loss: 0.7678, Time: 1.1494495868682861\n",
      "Epoch 1549/3000, Train Loss: 0.7789, Time: 1.144960880279541\n",
      "Epoch 1550/3000, Train Loss: 0.7392, Time: 1.124607801437378\n",
      "Epoch 1551/3000, Train Loss: 0.7682, Time: 1.1151182651519775\n",
      "Epoch 1552/3000, Train Loss: 0.7625, Time: 1.1123113632202148\n",
      "Epoch 1553/3000, Train Loss: 0.7734, Time: 1.1129045486450195\n",
      "Epoch 1554/3000, Train Loss: 0.7755, Time: 1.2822022438049316\n",
      "Epoch 1555/3000, Train Loss: 0.7534, Time: 1.1093344688415527\n",
      "Epoch 1556/3000, Train Loss: 0.7536, Time: 1.125138282775879\n",
      "Epoch 1557/3000, Train Loss: 0.7746, Time: 1.1659698486328125\n",
      "Epoch 1558/3000, Train Loss: 0.8012, Time: 1.1015911102294922\n",
      "Epoch 1559/3000, Train Loss: 0.7912, Time: 1.0982983112335205\n",
      "Epoch 1560/3000, Train Loss: 0.7604, Time: 1.1050443649291992\n",
      "Epoch 1561/3000, Train Loss: 0.7788, Time: 1.115053653717041\n",
      "Epoch 1562/3000, Train Loss: 0.7739, Time: 1.1092872619628906\n",
      "Epoch 1563/3000, Train Loss: 0.7679, Time: 1.1404829025268555\n",
      "Epoch 1564/3000, Train Loss: 0.7415, Time: 1.1618156433105469\n",
      "Epoch 1565/3000, Train Loss: 0.7734, Time: 1.1279680728912354\n",
      "Epoch 1566/3000, Train Loss: 0.7750, Time: 1.1183326244354248\n",
      "Epoch 1567/3000, Train Loss: 0.7756, Time: 1.115065574645996\n",
      "Epoch 1568/3000, Train Loss: 0.7543, Time: 1.1096386909484863\n",
      "Epoch 1569/3000, Train Loss: 0.7648, Time: 1.1015684604644775\n",
      "Epoch 1570/3000, Train Loss: 0.7625, Time: 1.021988868713379\n",
      "Epoch 1571/3000, Train Loss: 0.7761, Time: 1.0624849796295166\n",
      "Epoch 1572/3000, Train Loss: 0.7638, Time: 1.1469967365264893\n",
      "Epoch 1573/3000, Train Loss: 0.7737, Time: 1.1600570678710938\n",
      "Epoch 1574/3000, Train Loss: 0.8010, Time: 1.1762516498565674\n",
      "Epoch 1575/3000, Train Loss: 0.8368, Time: 1.1144754886627197\n",
      "Epoch 1576/3000, Train Loss: 0.8137, Time: 1.1121904850006104\n",
      "Epoch 1577/3000, Train Loss: 0.7967, Time: 1.107734203338623\n",
      "Epoch 1578/3000, Train Loss: 0.7639, Time: 1.1331543922424316\n",
      "Epoch 1579/3000, Train Loss: 0.8026, Time: 1.1381587982177734\n",
      "Epoch 1580/3000, Train Loss: 0.7580, Time: 1.1219565868377686\n",
      "Epoch 1581/3000, Train Loss: 0.7681, Time: 1.1116223335266113\n",
      "Epoch 1582/3000, Train Loss: 0.8075, Time: 1.1161315441131592\n",
      "Epoch 1583/3000, Train Loss: 0.7484, Time: 1.1437020301818848\n",
      "Epoch 1584/3000, Train Loss: 0.7625, Time: 1.0963354110717773\n",
      "Epoch 1585/3000, Train Loss: 0.7719, Time: 1.1193323135375977\n",
      "Epoch 1586/3000, Train Loss: 0.7761, Time: 1.1338109970092773\n",
      "Epoch 1587/3000, Train Loss: 0.7478, Time: 1.1341438293457031\n",
      "Epoch 1588/3000, Train Loss: 0.7666, Time: 1.1001715660095215\n",
      "Epoch 1589/3000, Train Loss: 0.7640, Time: 1.0997321605682373\n",
      "Epoch 1590/3000, Train Loss: 0.7898, Time: 1.102625846862793\n",
      "Epoch 1591/3000, Train Loss: 0.7542, Time: 1.1014232635498047\n",
      "Epoch 1592/3000, Train Loss: 0.7694, Time: 1.1074607372283936\n",
      "Epoch 1593/3000, Train Loss: 0.7805, Time: 1.171691656112671\n",
      "Epoch 1594/3000, Train Loss: 0.7408, Time: 1.131695032119751\n",
      "Epoch 1595/3000, Train Loss: 0.7571, Time: 1.187819004058838\n",
      "Epoch 1596/3000, Train Loss: 0.7579, Time: 1.1423087120056152\n",
      "Epoch 1597/3000, Train Loss: 0.7859, Time: 1.1431398391723633\n",
      "Epoch 1598/3000, Train Loss: 0.7749, Time: 1.1394851207733154\n",
      "Epoch 1599/3000, Train Loss: 0.7602, Time: 1.1428143978118896\n",
      "Epoch 1600/3000, Train Loss: 0.8088, Time: 1.168412685394287\n",
      "Epoch 1601/3000, Train Loss: 0.7456, Time: 1.1770131587982178\n",
      "Epoch 1602/3000, Train Loss: 0.7775, Time: 1.1807851791381836\n",
      "Epoch 1603/3000, Train Loss: 0.7466, Time: 1.1568691730499268\n",
      "Epoch 1604/3000, Train Loss: 0.7737, Time: 1.1307907104492188\n",
      "Epoch 1605/3000, Train Loss: 0.7540, Time: 1.1328966617584229\n",
      "Epoch 1606/3000, Train Loss: 0.7590, Time: 1.136265754699707\n",
      "Epoch 1607/3000, Train Loss: 0.7513, Time: 1.213585615158081\n",
      "Epoch 1608/3000, Train Loss: 0.7427, Time: 1.2864727973937988\n",
      "Epoch 1609/3000, Train Loss: 0.7520, Time: 1.1693177223205566\n",
      "Epoch 1610/3000, Train Loss: 0.7419, Time: 1.1433031558990479\n",
      "Epoch 1611/3000, Train Loss: 0.7698, Time: 1.1282360553741455\n",
      "Epoch 1612/3000, Train Loss: 0.7554, Time: 1.1196880340576172\n",
      "Epoch 1613/3000, Train Loss: 0.7630, Time: 1.1494731903076172\n",
      "Epoch 1614/3000, Train Loss: 0.7552, Time: 1.1529619693756104\n",
      "Epoch 1615/3000, Train Loss: 0.7662, Time: 1.1334660053253174\n",
      "Epoch 1616/3000, Train Loss: 0.7491, Time: 1.106287956237793\n",
      "Epoch 1617/3000, Train Loss: 0.7466, Time: 1.1109378337860107\n",
      "Epoch 1618/3000, Train Loss: 0.7847, Time: 1.1264150142669678\n",
      "Epoch 1619/3000, Train Loss: 0.7888, Time: 1.094658374786377\n",
      "Epoch 1620/3000, Train Loss: 0.7939, Time: 1.0942981243133545\n",
      "Epoch 1621/3000, Train Loss: 0.7571, Time: 1.1116704940795898\n",
      "Epoch 1622/3000, Train Loss: 0.8046, Time: 1.0961313247680664\n",
      "Epoch 1623/3000, Train Loss: 0.7625, Time: 1.0974557399749756\n",
      "Epoch 1624/3000, Train Loss: 0.7766, Time: 1.1301493644714355\n",
      "Epoch 1625/3000, Train Loss: 0.7404, Time: 1.1128556728363037\n",
      "Epoch 1626/3000, Train Loss: 0.7842, Time: 1.1672844886779785\n",
      "Epoch 1627/3000, Train Loss: 0.8070, Time: 1.1117477416992188\n",
      "Epoch 1628/3000, Train Loss: 0.7576, Time: 1.1090705394744873\n",
      "Epoch 1629/3000, Train Loss: 0.7739, Time: 1.1121594905853271\n",
      "Epoch 1630/3000, Train Loss: 0.7640, Time: 1.1342308521270752\n",
      "Epoch 1631/3000, Train Loss: 0.8039, Time: 1.1097254753112793\n",
      "Epoch 1632/3000, Train Loss: 0.7562, Time: 1.1080832481384277\n",
      "Epoch 1633/3000, Train Loss: 0.7426, Time: 1.10701322555542\n",
      "Epoch 1634/3000, Train Loss: 0.7544, Time: 1.112302303314209\n",
      "Epoch 1635/3000, Train Loss: 0.8097, Time: 1.1193773746490479\n",
      "Epoch 1636/3000, Train Loss: 0.7774, Time: 1.1386687755584717\n",
      "Epoch 1637/3000, Train Loss: 0.7682, Time: 1.1247615814208984\n",
      "Epoch 1638/3000, Train Loss: 0.7409, Time: 1.1067578792572021\n",
      "Epoch 1639/3000, Train Loss: 0.7666, Time: 1.1069204807281494\n",
      "Epoch 1640/3000, Train Loss: 0.8865, Time: 1.1088323593139648\n",
      "Epoch 1641/3000, Train Loss: 0.7895, Time: 1.1372747421264648\n",
      "Epoch 1642/3000, Train Loss: 0.7578, Time: 1.115729570388794\n",
      "Epoch 1643/3000, Train Loss: 0.7760, Time: 1.135538101196289\n",
      "Epoch 1644/3000, Train Loss: 0.7643, Time: 1.1542751789093018\n",
      "Epoch 1645/3000, Train Loss: 0.7668, Time: 1.124631643295288\n",
      "Epoch 1646/3000, Train Loss: 0.7492, Time: 1.1077871322631836\n",
      "Epoch 1647/3000, Train Loss: 0.7648, Time: 1.112919569015503\n",
      "Epoch 1648/3000, Train Loss: 0.7737, Time: 1.1177115440368652\n",
      "Epoch 1649/3000, Train Loss: 0.7538, Time: 1.133335828781128\n",
      "Epoch 1650/3000, Train Loss: 0.7543, Time: 1.1659631729125977\n",
      "Epoch 1651/3000, Train Loss: 0.7409, Time: 1.1171619892120361\n",
      "Epoch 1652/3000, Train Loss: 0.7614, Time: 1.1426782608032227\n",
      "Epoch 1653/3000, Train Loss: 0.7883, Time: 1.1163556575775146\n",
      "Epoch 1654/3000, Train Loss: 0.7394, Time: 1.1273531913757324\n",
      "Epoch 1655/3000, Train Loss: 0.7815, Time: 1.138413429260254\n",
      "Epoch 1656/3000, Train Loss: 0.7700, Time: 1.1431634426116943\n",
      "Epoch 1657/3000, Train Loss: 0.7579, Time: 1.1375837326049805\n",
      "Epoch 1658/3000, Train Loss: 0.7606, Time: 1.1191847324371338\n",
      "Epoch 1659/3000, Train Loss: 0.7848, Time: 1.114664077758789\n",
      "Epoch 1660/3000, Train Loss: 0.7524, Time: 1.2755284309387207\n",
      "Epoch 1661/3000, Train Loss: 0.7591, Time: 1.1466665267944336\n",
      "Epoch 1662/3000, Train Loss: 0.7503, Time: 1.1294846534729004\n",
      "Epoch 1663/3000, Train Loss: 0.7846, Time: 1.1027872562408447\n",
      "Epoch 1664/3000, Train Loss: 0.7460, Time: 1.0309059619903564\n",
      "Epoch 1665/3000, Train Loss: 0.7783, Time: 0.998046875\n",
      "Epoch 1666/3000, Train Loss: 0.7643, Time: 1.0364582538604736\n",
      "Epoch 1667/3000, Train Loss: 0.7940, Time: 1.0985701084136963\n",
      "Epoch 1668/3000, Train Loss: 0.7629, Time: 1.1368274688720703\n",
      "Epoch 1669/3000, Train Loss: 0.8376, Time: 1.1300442218780518\n",
      "Epoch 1670/3000, Train Loss: 0.7316, Time: 1.1333296298980713\n",
      "Epoch 1671/3000, Train Loss: 0.7431, Time: 1.1036124229431152\n",
      "Epoch 1672/3000, Train Loss: 0.7514, Time: 1.1037406921386719\n",
      "Epoch 1673/3000, Train Loss: 0.7503, Time: 1.1104533672332764\n",
      "Epoch 1674/3000, Train Loss: 0.8166, Time: 1.1049058437347412\n",
      "Epoch 1675/3000, Train Loss: 0.7980, Time: 1.1037640571594238\n",
      "Epoch 1676/3000, Train Loss: 0.7891, Time: 1.1318020820617676\n",
      "Epoch 1677/3000, Train Loss: 0.7646, Time: 1.1332266330718994\n",
      "Epoch 1678/3000, Train Loss: 0.7675, Time: 1.1208441257476807\n",
      "Epoch 1679/3000, Train Loss: 0.7452, Time: 1.105839729309082\n",
      "Epoch 1680/3000, Train Loss: 0.7879, Time: 1.1037747859954834\n",
      "Epoch 1681/3000, Train Loss: 0.7995, Time: 1.1085426807403564\n",
      "Epoch 1682/3000, Train Loss: 0.7499, Time: 1.1011326313018799\n",
      "Epoch 1683/3000, Train Loss: 0.7494, Time: 1.1088213920593262\n",
      "Epoch 1684/3000, Train Loss: 0.7543, Time: 1.1308960914611816\n",
      "Epoch 1685/3000, Train Loss: 0.7596, Time: 1.1291091442108154\n",
      "Epoch 1686/3000, Train Loss: 0.7448, Time: 1.1502127647399902\n",
      "Epoch 1687/3000, Train Loss: 0.7824, Time: 1.129117488861084\n",
      "Epoch 1688/3000, Train Loss: 0.7466, Time: 1.0946121215820312\n",
      "Epoch 1689/3000, Train Loss: 0.7633, Time: 1.0919969081878662\n",
      "Epoch 1690/3000, Train Loss: 0.7525, Time: 1.1005423069000244\n",
      "Epoch 1691/3000, Train Loss: 0.7415, Time: 1.114168405532837\n",
      "Epoch 1692/3000, Train Loss: 0.7763, Time: 1.1207695007324219\n",
      "Epoch 1693/3000, Train Loss: 0.7704, Time: 1.1271741390228271\n",
      "Epoch 1694/3000, Train Loss: 0.7691, Time: 1.0951745510101318\n",
      "Epoch 1695/3000, Train Loss: 0.7518, Time: 1.0930943489074707\n",
      "Epoch 1696/3000, Train Loss: 0.7608, Time: 1.093984842300415\n",
      "Epoch 1697/3000, Train Loss: 0.7750, Time: 1.1205346584320068\n",
      "Epoch 1698/3000, Train Loss: 0.7735, Time: 1.1232237815856934\n",
      "Epoch 1699/3000, Train Loss: 0.8128, Time: 1.1019396781921387\n",
      "Epoch 1700/3000, Train Loss: 0.7731, Time: 1.1367146968841553\n",
      "Epoch 1701/3000, Train Loss: 0.7526, Time: 1.110248327255249\n",
      "Epoch 1702/3000, Train Loss: 0.7612, Time: 1.109506368637085\n",
      "Epoch 1703/3000, Train Loss: 0.7543, Time: 1.1105375289916992\n",
      "Epoch 1704/3000, Train Loss: 0.7485, Time: 1.161562204360962\n",
      "Epoch 1705/3000, Train Loss: 0.7888, Time: 1.1647758483886719\n",
      "Epoch 1706/3000, Train Loss: 0.7734, Time: 1.1317713260650635\n",
      "Epoch 1707/3000, Train Loss: 0.7599, Time: 1.1048603057861328\n",
      "Epoch 1708/3000, Train Loss: 0.7518, Time: 1.1112124919891357\n",
      "Epoch 1709/3000, Train Loss: 0.7544, Time: 1.1038329601287842\n",
      "Epoch 1710/3000, Train Loss: 0.7577, Time: 1.1037049293518066\n",
      "Epoch 1711/3000, Train Loss: 0.7378, Time: 1.1262714862823486\n",
      "Epoch 1712/3000, Train Loss: 0.8076, Time: 1.134268045425415\n",
      "Epoch 1713/3000, Train Loss: 0.7663, Time: 1.1431970596313477\n",
      "Epoch 1714/3000, Train Loss: 0.7506, Time: 1.293144702911377\n",
      "Epoch 1715/3000, Train Loss: 0.7588, Time: 1.0979113578796387\n",
      "Epoch 1716/3000, Train Loss: 0.7330, Time: 1.0969412326812744\n",
      "Epoch 1717/3000, Train Loss: 0.7526, Time: 1.0999631881713867\n",
      "Epoch 1718/3000, Train Loss: 0.7578, Time: 1.136780023574829\n",
      "Epoch 1719/3000, Train Loss: 0.7494, Time: 1.133129596710205\n",
      "Epoch 1720/3000, Train Loss: 0.7476, Time: 1.1169419288635254\n",
      "Epoch 1721/3000, Train Loss: 0.7634, Time: 1.1070618629455566\n",
      "Epoch 1722/3000, Train Loss: 0.7426, Time: 1.1061928272247314\n",
      "Epoch 1723/3000, Train Loss: 0.7719, Time: 1.1111583709716797\n",
      "Epoch 1724/3000, Train Loss: 0.7467, Time: 1.1126940250396729\n",
      "Epoch 1725/3000, Train Loss: 0.7478, Time: 1.1264688968658447\n",
      "Epoch 1726/3000, Train Loss: 0.7438, Time: 1.1370785236358643\n",
      "Epoch 1727/3000, Train Loss: 0.7600, Time: 1.1210963726043701\n",
      "Epoch 1728/3000, Train Loss: 0.7410, Time: 1.1087570190429688\n",
      "Epoch 1729/3000, Train Loss: 0.7679, Time: 1.1070597171783447\n",
      "Epoch 1730/3000, Train Loss: 0.7370, Time: 1.1120860576629639\n",
      "Epoch 1731/3000, Train Loss: 0.7553, Time: 1.1098251342773438\n",
      "Epoch 1732/3000, Train Loss: 0.7432, Time: 1.129490613937378\n",
      "Epoch 1733/3000, Train Loss: 0.7443, Time: 1.14200758934021\n",
      "Epoch 1734/3000, Train Loss: 0.7802, Time: 1.116962194442749\n",
      "Epoch 1735/3000, Train Loss: 0.7450, Time: 1.106766700744629\n",
      "Epoch 1736/3000, Train Loss: 0.7876, Time: 1.1054542064666748\n",
      "Epoch 1737/3000, Train Loss: 0.7501, Time: 1.1050281524658203\n",
      "Epoch 1738/3000, Train Loss: 0.7648, Time: 1.105400800704956\n",
      "Epoch 1739/3000, Train Loss: 0.7845, Time: 1.1205110549926758\n",
      "Epoch 1740/3000, Train Loss: 0.7509, Time: 1.131664514541626\n",
      "Epoch 1741/3000, Train Loss: 0.7752, Time: 1.1352434158325195\n",
      "Epoch 1742/3000, Train Loss: 0.7415, Time: 1.1190567016601562\n",
      "Epoch 1743/3000, Train Loss: 0.7225, Time: 1.1138582229614258\n",
      "Epoch 1744/3000, Train Loss: 0.7549, Time: 1.107537031173706\n",
      "Epoch 1745/3000, Train Loss: 0.7397, Time: 1.0998075008392334\n",
      "Epoch 1746/3000, Train Loss: 0.7416, Time: 1.0982632637023926\n",
      "Epoch 1747/3000, Train Loss: 0.7407, Time: 1.1093714237213135\n",
      "Epoch 1748/3000, Train Loss: 0.7468, Time: 1.1326451301574707\n",
      "Epoch 1749/3000, Train Loss: 0.7477, Time: 1.135615348815918\n",
      "Epoch 1750/3000, Train Loss: 0.7349, Time: 1.124774694442749\n",
      "Epoch 1751/3000, Train Loss: 0.7667, Time: 1.1047639846801758\n",
      "Epoch 1752/3000, Train Loss: 0.7537, Time: 1.1060514450073242\n",
      "Epoch 1753/3000, Train Loss: 0.7354, Time: 1.1057841777801514\n",
      "Epoch 1754/3000, Train Loss: 0.7492, Time: 1.111609697341919\n",
      "Epoch 1755/3000, Train Loss: 0.7374, Time: 1.1168406009674072\n",
      "Epoch 1756/3000, Train Loss: 0.7861, Time: 1.1437549591064453\n",
      "Epoch 1757/3000, Train Loss: 0.7480, Time: 1.1672494411468506\n",
      "Epoch 1758/3000, Train Loss: 0.7340, Time: 1.1253061294555664\n",
      "Epoch 1759/3000, Train Loss: 0.8403, Time: 1.101200819015503\n",
      "Epoch 1760/3000, Train Loss: 0.7854, Time: 1.0998499393463135\n",
      "Epoch 1761/3000, Train Loss: 0.7634, Time: 0.9996991157531738\n",
      "Epoch 1762/3000, Train Loss: 0.7536, Time: 1.0014278888702393\n",
      "Epoch 1763/3000, Train Loss: 0.7653, Time: 1.077540636062622\n",
      "Epoch 1764/3000, Train Loss: 0.7689, Time: 1.1671795845031738\n",
      "Epoch 1765/3000, Train Loss: 0.8012, Time: 1.1318426132202148\n",
      "Epoch 1766/3000, Train Loss: 0.7736, Time: 1.133514642715454\n",
      "Epoch 1767/3000, Train Loss: 0.8361, Time: 1.1133019924163818\n",
      "Epoch 1768/3000, Train Loss: 0.7535, Time: 1.1960217952728271\n",
      "Epoch 1769/3000, Train Loss: 0.7710, Time: 1.1863300800323486\n",
      "Epoch 1770/3000, Train Loss: 0.7485, Time: 1.1196141242980957\n",
      "Epoch 1771/3000, Train Loss: 0.7428, Time: 1.1179323196411133\n",
      "Epoch 1772/3000, Train Loss: 0.7817, Time: 1.1284968852996826\n",
      "Epoch 1773/3000, Train Loss: 0.7494, Time: 1.1263880729675293\n",
      "Epoch 1774/3000, Train Loss: 0.7319, Time: 1.1006581783294678\n",
      "Epoch 1775/3000, Train Loss: 0.7505, Time: 1.1085450649261475\n",
      "Epoch 1776/3000, Train Loss: 0.7685, Time: 1.1312026977539062\n",
      "Epoch 1777/3000, Train Loss: 0.7607, Time: 1.1067893505096436\n",
      "Epoch 1778/3000, Train Loss: 0.7515, Time: 1.1224830150604248\n",
      "Epoch 1779/3000, Train Loss: 0.7541, Time: 1.129176378250122\n",
      "Epoch 1780/3000, Train Loss: 0.7799, Time: 1.1052982807159424\n",
      "Epoch 1781/3000, Train Loss: 0.7362, Time: 1.1048285961151123\n",
      "Epoch 1782/3000, Train Loss: 0.7387, Time: 1.1029307842254639\n",
      "Epoch 1783/3000, Train Loss: 0.7821, Time: 1.1032123565673828\n",
      "Epoch 1784/3000, Train Loss: 0.7689, Time: 1.1322648525238037\n",
      "Epoch 1785/3000, Train Loss: 0.7478, Time: 1.1265311241149902\n",
      "Epoch 1786/3000, Train Loss: 0.7422, Time: 1.1060757637023926\n",
      "Epoch 1787/3000, Train Loss: 0.7618, Time: 1.1024558544158936\n",
      "Epoch 1788/3000, Train Loss: 0.7768, Time: 1.0985679626464844\n",
      "Epoch 1789/3000, Train Loss: 0.7661, Time: 1.0971927642822266\n",
      "Epoch 1790/3000, Train Loss: 0.7421, Time: 1.112036943435669\n",
      "Epoch 1791/3000, Train Loss: 0.7665, Time: 1.1303584575653076\n",
      "Epoch 1792/3000, Train Loss: 0.7619, Time: 1.1302101612091064\n",
      "Epoch 1793/3000, Train Loss: 0.7424, Time: 1.099958896636963\n",
      "Epoch 1794/3000, Train Loss: 0.7486, Time: 1.1001553535461426\n",
      "Epoch 1795/3000, Train Loss: 0.7913, Time: 1.1019423007965088\n",
      "Epoch 1796/3000, Train Loss: 0.7485, Time: 1.1045992374420166\n",
      "Epoch 1797/3000, Train Loss: 0.7378, Time: 1.1022090911865234\n",
      "Epoch 1798/3000, Train Loss: 0.7379, Time: 1.1266498565673828\n",
      "Epoch 1799/3000, Train Loss: 0.7762, Time: 1.1334097385406494\n",
      "Epoch 1800/3000, Train Loss: 0.7603, Time: 1.1360013484954834\n",
      "Epoch 1801/3000, Train Loss: 0.7480, Time: 1.120086908340454\n",
      "Epoch 1802/3000, Train Loss: 0.7393, Time: 1.1068637371063232\n",
      "Epoch 1803/3000, Train Loss: 0.7323, Time: 1.102895736694336\n",
      "Epoch 1804/3000, Train Loss: 0.7500, Time: 1.1036248207092285\n",
      "Epoch 1805/3000, Train Loss: 0.7523, Time: 1.1227514743804932\n",
      "Epoch 1806/3000, Train Loss: 0.7473, Time: 1.104865312576294\n",
      "Epoch 1807/3000, Train Loss: 0.7402, Time: 1.1132783889770508\n",
      "Epoch 1808/3000, Train Loss: 0.7457, Time: 1.1089327335357666\n",
      "Epoch 1809/3000, Train Loss: 0.7281, Time: 1.1057045459747314\n",
      "Epoch 1810/3000, Train Loss: 0.7800, Time: 1.1058690547943115\n",
      "Epoch 1811/3000, Train Loss: 0.7864, Time: 1.130476951599121\n",
      "Epoch 1812/3000, Train Loss: 0.7509, Time: 1.1299688816070557\n",
      "Epoch 1813/3000, Train Loss: 0.7431, Time: 1.1070988178253174\n",
      "Epoch 1814/3000, Train Loss: 0.7944, Time: 1.0988266468048096\n",
      "Epoch 1815/3000, Train Loss: 0.8157, Time: 1.1101317405700684\n",
      "Epoch 1816/3000, Train Loss: 0.7406, Time: 1.109205961227417\n",
      "Epoch 1817/3000, Train Loss: 0.7422, Time: 1.1110947132110596\n",
      "Epoch 1818/3000, Train Loss: 0.7322, Time: 1.1212763786315918\n",
      "Epoch 1819/3000, Train Loss: 0.7621, Time: 1.1694469451904297\n",
      "Epoch 1820/3000, Train Loss: 0.7648, Time: 1.115339756011963\n",
      "Epoch 1821/3000, Train Loss: 0.7677, Time: 1.2455251216888428\n",
      "Epoch 1822/3000, Train Loss: 0.7479, Time: 1.114121437072754\n",
      "Epoch 1823/3000, Train Loss: 0.7402, Time: 1.0998458862304688\n",
      "Epoch 1824/3000, Train Loss: 0.7683, Time: 1.1009628772735596\n",
      "Epoch 1825/3000, Train Loss: 0.7465, Time: 1.1219429969787598\n",
      "Epoch 1826/3000, Train Loss: 0.7541, Time: 1.1262624263763428\n",
      "Epoch 1827/3000, Train Loss: 0.7339, Time: 1.1032495498657227\n",
      "Epoch 1828/3000, Train Loss: 0.7527, Time: 1.1060490608215332\n",
      "Epoch 1829/3000, Train Loss: 0.7510, Time: 1.104349136352539\n",
      "Epoch 1830/3000, Train Loss: 0.7410, Time: 1.1004984378814697\n",
      "Epoch 1831/3000, Train Loss: 0.7289, Time: 1.099989891052246\n",
      "Epoch 1832/3000, Train Loss: 0.7309, Time: 1.1183714866638184\n",
      "Epoch 1833/3000, Train Loss: 0.7559, Time: 1.1330633163452148\n",
      "Epoch 1834/3000, Train Loss: 0.7424, Time: 1.1154165267944336\n",
      "Epoch 1835/3000, Train Loss: 0.7360, Time: 1.0994510650634766\n",
      "Epoch 1836/3000, Train Loss: 0.7879, Time: 1.0986247062683105\n",
      "Epoch 1837/3000, Train Loss: 0.7557, Time: 1.1010162830352783\n",
      "Epoch 1838/3000, Train Loss: 0.7853, Time: 1.1239674091339111\n",
      "Epoch 1839/3000, Train Loss: 0.7786, Time: 1.1063811779022217\n",
      "Epoch 1840/3000, Train Loss: 0.7743, Time: 1.129812240600586\n",
      "Epoch 1841/3000, Train Loss: 0.7267, Time: 1.1405272483825684\n",
      "Epoch 1842/3000, Train Loss: 0.7528, Time: 1.1347792148590088\n",
      "Epoch 1843/3000, Train Loss: 0.7324, Time: 1.104738712310791\n",
      "Epoch 1844/3000, Train Loss: 0.7339, Time: 1.1023039817810059\n",
      "Epoch 1845/3000, Train Loss: 0.7255, Time: 1.1041944026947021\n",
      "Epoch 1846/3000, Train Loss: 0.7582, Time: 1.1553006172180176\n",
      "Epoch 1847/3000, Train Loss: 0.7505, Time: 1.152066707611084\n",
      "Epoch 1848/3000, Train Loss: 0.7411, Time: 1.1098248958587646\n",
      "Epoch 1849/3000, Train Loss: 0.7519, Time: 1.0980861186981201\n",
      "Epoch 1850/3000, Train Loss: 0.7457, Time: 1.108137845993042\n",
      "Epoch 1851/3000, Train Loss: 0.7376, Time: 1.0895705223083496\n",
      "Epoch 1852/3000, Train Loss: 0.7393, Time: 1.1520938873291016\n",
      "Epoch 1853/3000, Train Loss: 0.7450, Time: 1.12353515625\n",
      "Epoch 1854/3000, Train Loss: 0.7295, Time: 1.1031644344329834\n",
      "Epoch 1855/3000, Train Loss: 0.7855, Time: 1.1011836528778076\n",
      "Epoch 1856/3000, Train Loss: 0.7404, Time: 1.0918235778808594\n",
      "Epoch 1857/3000, Train Loss: 0.7586, Time: 0.9971790313720703\n",
      "Epoch 1858/3000, Train Loss: 0.7294, Time: 1.0497093200683594\n",
      "Epoch 1859/3000, Train Loss: 0.7385, Time: 1.122629165649414\n",
      "Epoch 1860/3000, Train Loss: 0.7370, Time: 1.1186952590942383\n",
      "Epoch 1861/3000, Train Loss: 0.7330, Time: 1.1047909259796143\n",
      "Epoch 1862/3000, Train Loss: 0.7627, Time: 1.103905439376831\n",
      "Epoch 1863/3000, Train Loss: 0.7818, Time: 1.1327433586120605\n",
      "Epoch 1864/3000, Train Loss: 0.7404, Time: 1.1040806770324707\n",
      "Epoch 1865/3000, Train Loss: 0.7292, Time: 1.1416826248168945\n",
      "Epoch 1866/3000, Train Loss: 0.7347, Time: 1.1320011615753174\n",
      "Epoch 1867/3000, Train Loss: 0.7492, Time: 1.1446607112884521\n",
      "Epoch 1868/3000, Train Loss: 0.7253, Time: 1.104663610458374\n",
      "Epoch 1869/3000, Train Loss: 0.7322, Time: 1.123030185699463\n",
      "Epoch 1870/3000, Train Loss: 0.7392, Time: 1.1259400844573975\n",
      "Epoch 1871/3000, Train Loss: 0.7476, Time: 1.1177606582641602\n",
      "Epoch 1872/3000, Train Loss: 0.7414, Time: 1.135812759399414\n",
      "Epoch 1873/3000, Train Loss: 0.7433, Time: 1.1459362506866455\n",
      "Epoch 1874/3000, Train Loss: 0.7355, Time: 1.141960620880127\n",
      "Epoch 1875/3000, Train Loss: 0.7527, Time: 1.1647944450378418\n",
      "Epoch 1876/3000, Train Loss: 0.7292, Time: 1.1155905723571777\n",
      "Epoch 1877/3000, Train Loss: 0.7308, Time: 1.1807825565338135\n",
      "Epoch 1878/3000, Train Loss: 0.7460, Time: 1.1059849262237549\n",
      "Epoch 1879/3000, Train Loss: 0.7235, Time: 1.1169652938842773\n",
      "Epoch 1880/3000, Train Loss: 0.7328, Time: 1.1364102363586426\n",
      "Epoch 1881/3000, Train Loss: 0.7333, Time: 1.1366214752197266\n",
      "Epoch 1882/3000, Train Loss: 0.7335, Time: 1.1255083084106445\n",
      "Epoch 1883/3000, Train Loss: 0.7654, Time: 1.1352410316467285\n",
      "Epoch 1884/3000, Train Loss: 0.7863, Time: 1.1059529781341553\n",
      "Epoch 1885/3000, Train Loss: 0.7708, Time: 1.1290643215179443\n",
      "Epoch 1886/3000, Train Loss: 0.7306, Time: 1.1247992515563965\n",
      "Epoch 1887/3000, Train Loss: 0.7501, Time: 1.1229140758514404\n",
      "Epoch 1888/3000, Train Loss: 0.7394, Time: 1.1484220027923584\n",
      "Epoch 1889/3000, Train Loss: 0.7297, Time: 1.1140451431274414\n",
      "Epoch 1890/3000, Train Loss: 0.7746, Time: 1.1109402179718018\n",
      "Epoch 1891/3000, Train Loss: 0.7470, Time: 1.110260009765625\n",
      "Epoch 1892/3000, Train Loss: 0.7352, Time: 1.1093380451202393\n",
      "Epoch 1893/3000, Train Loss: 0.7258, Time: 1.1097521781921387\n",
      "Epoch 1894/3000, Train Loss: 0.7181, Time: 1.1473090648651123\n",
      "Epoch 1895/3000, Train Loss: 0.7447, Time: 1.1411643028259277\n",
      "Epoch 1896/3000, Train Loss: 0.8028, Time: 1.1349849700927734\n",
      "Epoch 1897/3000, Train Loss: 0.7865, Time: 1.1043918132781982\n",
      "Epoch 1898/3000, Train Loss: 0.7678, Time: 1.0994417667388916\n",
      "Epoch 1899/3000, Train Loss: 0.7228, Time: 1.1397874355316162\n",
      "Epoch 1900/3000, Train Loss: 0.7327, Time: 1.0981881618499756\n",
      "Epoch 1901/3000, Train Loss: 0.7409, Time: 1.0992445945739746\n",
      "Epoch 1902/3000, Train Loss: 0.7434, Time: 1.1215510368347168\n",
      "Epoch 1903/3000, Train Loss: 0.7473, Time: 1.1343886852264404\n",
      "Epoch 1904/3000, Train Loss: 0.7248, Time: 1.1062171459197998\n",
      "Epoch 1905/3000, Train Loss: 0.7341, Time: 1.0980744361877441\n",
      "Epoch 1906/3000, Train Loss: 0.7864, Time: 1.099308729171753\n",
      "Epoch 1907/3000, Train Loss: 0.7248, Time: 1.1268749237060547\n",
      "Epoch 1908/3000, Train Loss: 0.7296, Time: 1.1013424396514893\n",
      "Epoch 1909/3000, Train Loss: 0.7295, Time: 1.1309020519256592\n",
      "Epoch 1910/3000, Train Loss: 0.7278, Time: 1.108205795288086\n",
      "Epoch 1911/3000, Train Loss: 0.7374, Time: 1.1031732559204102\n",
      "Epoch 1912/3000, Train Loss: 0.7292, Time: 1.104750394821167\n",
      "Epoch 1913/3000, Train Loss: 0.7323, Time: 1.108642816543579\n",
      "Epoch 1914/3000, Train Loss: 0.7310, Time: 1.1141865253448486\n",
      "Epoch 1915/3000, Train Loss: 0.7355, Time: 1.1042563915252686\n",
      "Epoch 1916/3000, Train Loss: 0.7376, Time: 1.101837158203125\n",
      "Epoch 1917/3000, Train Loss: 0.7280, Time: 1.105682611465454\n",
      "Epoch 1918/3000, Train Loss: 0.7313, Time: 1.1362438201904297\n",
      "Epoch 1919/3000, Train Loss: 0.7503, Time: 1.121852159500122\n",
      "Epoch 1920/3000, Train Loss: 0.7280, Time: 1.1050660610198975\n",
      "Epoch 1921/3000, Train Loss: 0.7400, Time: 1.1035454273223877\n",
      "Epoch 1922/3000, Train Loss: 0.7135, Time: 1.1292972564697266\n",
      "Epoch 1923/3000, Train Loss: 0.7377, Time: 1.1242027282714844\n",
      "Epoch 1924/3000, Train Loss: 0.7651, Time: 1.135948657989502\n",
      "Epoch 1925/3000, Train Loss: 0.7893, Time: 1.1040928363800049\n",
      "Epoch 1926/3000, Train Loss: 0.7397, Time: 1.0992929935455322\n",
      "Epoch 1927/3000, Train Loss: 0.7474, Time: 1.1076533794403076\n",
      "Epoch 1928/3000, Train Loss: 0.7254, Time: 1.1320488452911377\n",
      "Epoch 1929/3000, Train Loss: 0.7306, Time: 1.1809890270233154\n",
      "Epoch 1930/3000, Train Loss: 0.7312, Time: 1.2242414951324463\n",
      "Epoch 1931/3000, Train Loss: 0.7273, Time: 1.186004638671875\n",
      "Epoch 1932/3000, Train Loss: 0.7324, Time: 1.1236505508422852\n",
      "Epoch 1933/3000, Train Loss: 0.7357, Time: 1.1021547317504883\n",
      "Epoch 1934/3000, Train Loss: 0.7288, Time: 1.1020126342773438\n",
      "Epoch 1935/3000, Train Loss: 0.7249, Time: 1.1083290576934814\n",
      "Epoch 1936/3000, Train Loss: 0.7213, Time: 1.116844654083252\n",
      "Epoch 1937/3000, Train Loss: 0.7330, Time: 1.1304888725280762\n",
      "Epoch 1938/3000, Train Loss: 0.7374, Time: 1.116379976272583\n",
      "Epoch 1939/3000, Train Loss: 0.7445, Time: 1.1036920547485352\n",
      "Epoch 1940/3000, Train Loss: 0.7480, Time: 1.1086194515228271\n",
      "Epoch 1941/3000, Train Loss: 0.7214, Time: 1.1020398139953613\n",
      "Epoch 1942/3000, Train Loss: 0.7284, Time: 1.1050200462341309\n",
      "Epoch 1943/3000, Train Loss: 0.7179, Time: 1.1171560287475586\n",
      "Epoch 1944/3000, Train Loss: 0.7245, Time: 1.1320984363555908\n",
      "Epoch 1945/3000, Train Loss: 0.7172, Time: 1.127969741821289\n",
      "Epoch 1946/3000, Train Loss: 0.7313, Time: 1.1104824542999268\n",
      "Epoch 1947/3000, Train Loss: 0.7358, Time: 1.1164588928222656\n",
      "Epoch 1948/3000, Train Loss: 0.7271, Time: 1.148723840713501\n",
      "Epoch 1949/3000, Train Loss: 0.7305, Time: 1.1185064315795898\n",
      "Epoch 1950/3000, Train Loss: 0.7265, Time: 1.1389973163604736\n",
      "Epoch 1951/3000, Train Loss: 0.7516, Time: 1.1580359935760498\n",
      "Epoch 1952/3000, Train Loss: 0.7152, Time: 1.1037466526031494\n",
      "Epoch 1953/3000, Train Loss: 0.7297, Time: 1.0114343166351318\n",
      "Epoch 1954/3000, Train Loss: 0.7408, Time: 1.0063323974609375\n",
      "Epoch 1955/3000, Train Loss: 0.7562, Time: 1.0965325832366943\n",
      "Epoch 1956/3000, Train Loss: 0.7290, Time: 1.137702465057373\n",
      "Epoch 1957/3000, Train Loss: 0.7282, Time: 1.1366057395935059\n",
      "Epoch 1958/3000, Train Loss: 0.7577, Time: 1.1262643337249756\n",
      "Epoch 1959/3000, Train Loss: 0.7285, Time: 1.1335928440093994\n",
      "Epoch 1960/3000, Train Loss: 0.7168, Time: 1.1075491905212402\n",
      "Epoch 1961/3000, Train Loss: 0.7250, Time: 1.1249802112579346\n",
      "Epoch 1962/3000, Train Loss: 0.7370, Time: 1.1355700492858887\n",
      "Epoch 1963/3000, Train Loss: 0.7418, Time: 1.1374988555908203\n",
      "Epoch 1964/3000, Train Loss: 0.7166, Time: 1.1291918754577637\n",
      "Epoch 1965/3000, Train Loss: 0.7387, Time: 1.1243937015533447\n",
      "Epoch 1966/3000, Train Loss: 0.7289, Time: 1.104959487915039\n",
      "Epoch 1967/3000, Train Loss: 0.7147, Time: 1.1160311698913574\n",
      "Epoch 1968/3000, Train Loss: 0.7157, Time: 1.1450133323669434\n",
      "Epoch 1969/3000, Train Loss: 0.7233, Time: 1.1349942684173584\n",
      "Epoch 1970/3000, Train Loss: 0.7419, Time: 1.1655998229980469\n",
      "Epoch 1971/3000, Train Loss: 0.7196, Time: 1.142543077468872\n",
      "Epoch 1972/3000, Train Loss: 0.7285, Time: 1.1111271381378174\n",
      "Epoch 1973/3000, Train Loss: 0.7355, Time: 1.117140769958496\n",
      "Epoch 1974/3000, Train Loss: 0.7381, Time: 1.11903977394104\n",
      "Epoch 1975/3000, Train Loss: 0.7240, Time: 1.129368543624878\n",
      "Epoch 1976/3000, Train Loss: 0.7179, Time: 1.1190061569213867\n",
      "Epoch 1977/3000, Train Loss: 0.7324, Time: 1.0993118286132812\n",
      "Epoch 1978/3000, Train Loss: 0.7209, Time: 1.0972709655761719\n",
      "Epoch 1979/3000, Train Loss: 0.7358, Time: 1.095170021057129\n",
      "Epoch 1980/3000, Train Loss: 0.7753, Time: 1.1173865795135498\n",
      "Epoch 1981/3000, Train Loss: 0.7179, Time: 1.1580028533935547\n",
      "Epoch 1982/3000, Train Loss: 0.7253, Time: 1.2886571884155273\n",
      "Epoch 1983/3000, Train Loss: 0.7127, Time: 1.137749195098877\n",
      "Epoch 1984/3000, Train Loss: 0.7265, Time: 1.097093105316162\n",
      "Epoch 1985/3000, Train Loss: 0.7314, Time: 1.0938007831573486\n",
      "Epoch 1986/3000, Train Loss: 0.7198, Time: 1.1063292026519775\n",
      "Epoch 1987/3000, Train Loss: 0.7236, Time: 1.1058382987976074\n",
      "Epoch 1988/3000, Train Loss: 0.7531, Time: 1.1769142150878906\n",
      "Epoch 1989/3000, Train Loss: 0.7234, Time: 1.1336283683776855\n",
      "Epoch 1990/3000, Train Loss: 0.7190, Time: 1.152552604675293\n",
      "Epoch 1991/3000, Train Loss: 0.7423, Time: 1.1580114364624023\n",
      "Epoch 1992/3000, Train Loss: 0.7155, Time: 1.1110000610351562\n",
      "Epoch 1993/3000, Train Loss: 0.7267, Time: 1.1016027927398682\n",
      "Epoch 1994/3000, Train Loss: 0.7275, Time: 1.107161283493042\n",
      "Epoch 1995/3000, Train Loss: 0.7295, Time: 1.104027509689331\n",
      "Epoch 1996/3000, Train Loss: 0.7205, Time: 1.1223981380462646\n",
      "Epoch 1997/3000, Train Loss: 0.7182, Time: 1.1331570148468018\n",
      "Epoch 1998/3000, Train Loss: 0.7173, Time: 1.133958101272583\n",
      "Epoch 1999/3000, Train Loss: 0.7246, Time: 1.1229548454284668\n",
      "Epoch 2000/3000, Train Loss: 0.7257, Time: 1.0994720458984375\n",
      "Epoch 2001/3000, Train Loss: 0.7276, Time: 1.1089825630187988\n",
      "Epoch 2002/3000, Train Loss: 0.7247, Time: 1.107487440109253\n",
      "Epoch 2003/3000, Train Loss: 0.7155, Time: 1.1149928569793701\n",
      "Epoch 2004/3000, Train Loss: 0.7263, Time: 1.136033535003662\n",
      "Epoch 2005/3000, Train Loss: 0.7176, Time: 1.150432825088501\n",
      "Epoch 2006/3000, Train Loss: 0.7415, Time: 1.1091382503509521\n",
      "Epoch 2007/3000, Train Loss: 0.7178, Time: 1.1282949447631836\n",
      "Epoch 2008/3000, Train Loss: 0.7177, Time: 1.098083257675171\n",
      "Epoch 2009/3000, Train Loss: 0.7091, Time: 1.1084458827972412\n",
      "Epoch 2010/3000, Train Loss: 0.7228, Time: 1.127286672592163\n",
      "Epoch 2011/3000, Train Loss: 0.7297, Time: 1.1261372566223145\n",
      "Epoch 2012/3000, Train Loss: 0.7118, Time: 1.1149992942810059\n",
      "Epoch 2013/3000, Train Loss: 0.7223, Time: 1.0987646579742432\n",
      "Epoch 2014/3000, Train Loss: 0.7227, Time: 1.0993947982788086\n",
      "Epoch 2015/3000, Train Loss: 0.7224, Time: 1.0991792678833008\n",
      "Epoch 2016/3000, Train Loss: 0.7074, Time: 1.1180384159088135\n",
      "Epoch 2017/3000, Train Loss: 0.7090, Time: 1.1341478824615479\n",
      "Epoch 2018/3000, Train Loss: 0.7239, Time: 1.123957633972168\n",
      "Epoch 2019/3000, Train Loss: 0.7143, Time: 1.1003246307373047\n",
      "Epoch 2020/3000, Train Loss: 0.7092, Time: 1.1072323322296143\n",
      "Epoch 2021/3000, Train Loss: 0.7173, Time: 1.1255686283111572\n",
      "Epoch 2022/3000, Train Loss: 0.7135, Time: 1.1027557849884033\n",
      "Epoch 2023/3000, Train Loss: 0.7221, Time: 1.0971927642822266\n",
      "Epoch 2024/3000, Train Loss: 0.7174, Time: 1.1271286010742188\n",
      "Epoch 2025/3000, Train Loss: 0.7218, Time: 1.1294076442718506\n",
      "Epoch 2026/3000, Train Loss: 0.7118, Time: 1.10463285446167\n",
      "Epoch 2027/3000, Train Loss: 0.7122, Time: 1.0960183143615723\n",
      "Epoch 2028/3000, Train Loss: 0.7273, Time: 1.0972568988800049\n",
      "Epoch 2029/3000, Train Loss: 0.7135, Time: 1.0944485664367676\n",
      "Epoch 2030/3000, Train Loss: 0.7146, Time: 1.0954482555389404\n",
      "Epoch 2031/3000, Train Loss: 0.7166, Time: 1.1088714599609375\n",
      "Epoch 2032/3000, Train Loss: 0.7183, Time: 1.1656687259674072\n",
      "Epoch 2033/3000, Train Loss: 0.7186, Time: 1.1245479583740234\n",
      "Epoch 2034/3000, Train Loss: 0.7224, Time: 1.1206822395324707\n",
      "Epoch 2035/3000, Train Loss: 0.7152, Time: 1.0956227779388428\n",
      "Epoch 2036/3000, Train Loss: 0.7270, Time: 1.2116341590881348\n",
      "Epoch 2037/3000, Train Loss: 0.7131, Time: 1.1900155544281006\n",
      "Epoch 2038/3000, Train Loss: 0.7127, Time: 1.1223714351654053\n",
      "Epoch 2039/3000, Train Loss: 0.7180, Time: 1.1395676136016846\n",
      "Epoch 2040/3000, Train Loss: 0.7255, Time: 1.1391570568084717\n",
      "Epoch 2041/3000, Train Loss: 0.7294, Time: 1.1175215244293213\n",
      "Epoch 2042/3000, Train Loss: 0.7222, Time: 1.1218888759613037\n",
      "Epoch 2043/3000, Train Loss: 0.7135, Time: 1.1230223178863525\n",
      "Epoch 2044/3000, Train Loss: 0.7213, Time: 1.119020700454712\n",
      "Epoch 2045/3000, Train Loss: 0.7173, Time: 1.1353938579559326\n",
      "Epoch 2046/3000, Train Loss: 0.7186, Time: 1.1438539028167725\n",
      "Epoch 2047/3000, Train Loss: 0.7304, Time: 1.1457750797271729\n",
      "Epoch 2048/3000, Train Loss: 0.7127, Time: 1.1573307514190674\n",
      "Epoch 2049/3000, Train Loss: 0.7283, Time: 1.0848698616027832\n",
      "Epoch 2050/3000, Train Loss: 0.7216, Time: 1.0429928302764893\n",
      "Epoch 2051/3000, Train Loss: 0.7169, Time: 1.0215940475463867\n",
      "Epoch 2052/3000, Train Loss: 0.7147, Time: 1.1487212181091309\n",
      "Epoch 2053/3000, Train Loss: 0.7155, Time: 1.1462819576263428\n",
      "Epoch 2054/3000, Train Loss: 0.7157, Time: 1.1160743236541748\n",
      "Epoch 2055/3000, Train Loss: 0.7115, Time: 1.100545883178711\n",
      "Epoch 2056/3000, Train Loss: 0.7131, Time: 1.1019115447998047\n",
      "Epoch 2057/3000, Train Loss: 0.7158, Time: 1.1373569965362549\n",
      "Epoch 2058/3000, Train Loss: 0.7195, Time: 1.130802869796753\n",
      "Epoch 2059/3000, Train Loss: 0.7323, Time: 1.1253039836883545\n",
      "Epoch 2060/3000, Train Loss: 0.7124, Time: 1.1342713832855225\n",
      "Epoch 2061/3000, Train Loss: 0.7153, Time: 1.1344130039215088\n",
      "Epoch 2062/3000, Train Loss: 0.7120, Time: 1.1147470474243164\n",
      "Epoch 2063/3000, Train Loss: 0.7149, Time: 1.1028473377227783\n",
      "Epoch 2064/3000, Train Loss: 0.7171, Time: 1.0964081287384033\n",
      "Epoch 2065/3000, Train Loss: 0.7206, Time: 1.1254587173461914\n",
      "Epoch 2066/3000, Train Loss: 0.7130, Time: 1.13112473487854\n",
      "Epoch 2067/3000, Train Loss: 0.7161, Time: 1.1265790462493896\n",
      "Epoch 2068/3000, Train Loss: 0.7104, Time: 1.1071600914001465\n",
      "Epoch 2069/3000, Train Loss: 0.7114, Time: 1.1087346076965332\n",
      "Epoch 2070/3000, Train Loss: 0.7261, Time: 1.1084983348846436\n",
      "Epoch 2071/3000, Train Loss: 0.7142, Time: 1.1068477630615234\n",
      "Epoch 2072/3000, Train Loss: 0.7157, Time: 1.1171886920928955\n",
      "Epoch 2073/3000, Train Loss: 0.7152, Time: 1.1403474807739258\n",
      "Epoch 2074/3000, Train Loss: 0.7189, Time: 1.1526648998260498\n",
      "Epoch 2075/3000, Train Loss: 0.7127, Time: 1.116624116897583\n",
      "Epoch 2076/3000, Train Loss: 0.7071, Time: 1.1452672481536865\n",
      "Epoch 2077/3000, Train Loss: 0.7145, Time: 1.112959384918213\n",
      "Epoch 2078/3000, Train Loss: 0.7123, Time: 1.1109249591827393\n",
      "Epoch 2079/3000, Train Loss: 0.7148, Time: 1.121687650680542\n",
      "Epoch 2080/3000, Train Loss: 0.7167, Time: 1.136423110961914\n",
      "Epoch 2081/3000, Train Loss: 0.7197, Time: 1.1306235790252686\n",
      "Epoch 2082/3000, Train Loss: 0.7234, Time: 1.1662299633026123\n",
      "Epoch 2083/3000, Train Loss: 0.7111, Time: 1.1021862030029297\n",
      "Epoch 2084/3000, Train Loss: 0.7270, Time: 1.1108825206756592\n",
      "Epoch 2085/3000, Train Loss: 0.7176, Time: 1.1255826950073242\n",
      "Epoch 2086/3000, Train Loss: 0.7139, Time: 1.124544382095337\n",
      "Epoch 2087/3000, Train Loss: 0.7097, Time: 1.1354877948760986\n",
      "Epoch 2088/3000, Train Loss: 0.7161, Time: 1.1752839088439941\n",
      "Epoch 2089/3000, Train Loss: 0.7267, Time: 1.1325562000274658\n",
      "Epoch 2090/3000, Train Loss: 0.7135, Time: 1.308704137802124\n",
      "Epoch 2091/3000, Train Loss: 0.7115, Time: 1.1225829124450684\n",
      "Epoch 2092/3000, Train Loss: 0.7215, Time: 1.1262061595916748\n",
      "Epoch 2093/3000, Train Loss: 0.7135, Time: 1.1274912357330322\n",
      "Epoch 2094/3000, Train Loss: 0.7104, Time: 1.1313896179199219\n",
      "Epoch 2095/3000, Train Loss: 0.7142, Time: 1.1638362407684326\n",
      "Epoch 2096/3000, Train Loss: 0.7122, Time: 1.1895685195922852\n",
      "Epoch 2097/3000, Train Loss: 0.7140, Time: 1.1778268814086914\n",
      "Epoch 2098/3000, Train Loss: 0.7178, Time: 1.1594226360321045\n",
      "Epoch 2099/3000, Train Loss: 0.7081, Time: 1.1464428901672363\n",
      "Epoch 2100/3000, Train Loss: 0.7184, Time: 1.1817266941070557\n",
      "Epoch 2101/3000, Train Loss: 0.7110, Time: 1.184180498123169\n",
      "Epoch 2102/3000, Train Loss: 0.7175, Time: 1.1942167282104492\n",
      "Epoch 2103/3000, Train Loss: 0.7123, Time: 1.1967015266418457\n",
      "Epoch 2104/3000, Train Loss: 0.7176, Time: 1.163825273513794\n",
      "Epoch 2105/3000, Train Loss: 0.7150, Time: 1.1563668251037598\n",
      "Epoch 2106/3000, Train Loss: 0.7104, Time: 1.152785301208496\n",
      "Epoch 2107/3000, Train Loss: 0.7174, Time: 1.1726229190826416\n",
      "Epoch 2108/3000, Train Loss: 0.7180, Time: 1.1740481853485107\n",
      "Epoch 2109/3000, Train Loss: 0.7209, Time: 1.146780014038086\n",
      "Epoch 2110/3000, Train Loss: 0.7113, Time: 1.1937377452850342\n",
      "Epoch 2111/3000, Train Loss: 0.7102, Time: 1.1293737888336182\n",
      "Epoch 2112/3000, Train Loss: 0.7166, Time: 1.1021485328674316\n",
      "Epoch 2113/3000, Train Loss: 0.7183, Time: 1.0996332168579102\n",
      "Epoch 2114/3000, Train Loss: 0.7218, Time: 1.1243062019348145\n",
      "Epoch 2115/3000, Train Loss: 0.7179, Time: 1.0976543426513672\n",
      "Epoch 2116/3000, Train Loss: 0.7160, Time: 1.1268112659454346\n",
      "Epoch 2117/3000, Train Loss: 0.7231, Time: 1.1551220417022705\n",
      "Epoch 2118/3000, Train Loss: 0.7179, Time: 1.174393653869629\n",
      "Epoch 2119/3000, Train Loss: 0.7123, Time: 1.1413118839263916\n",
      "Epoch 2120/3000, Train Loss: 0.7204, Time: 1.114426612854004\n",
      "Epoch 2121/3000, Train Loss: 0.7036, Time: 1.1395151615142822\n",
      "Epoch 2122/3000, Train Loss: 0.7140, Time: 1.1230249404907227\n",
      "Epoch 2123/3000, Train Loss: 0.7151, Time: 1.1432998180389404\n",
      "Epoch 2124/3000, Train Loss: 0.7136, Time: 1.1500444412231445\n",
      "Epoch 2125/3000, Train Loss: 0.7142, Time: 1.1663055419921875\n",
      "Epoch 2126/3000, Train Loss: 0.7125, Time: 1.1746022701263428\n",
      "Epoch 2127/3000, Train Loss: 0.7165, Time: 1.1760003566741943\n",
      "Epoch 2128/3000, Train Loss: 0.7097, Time: 1.157571792602539\n",
      "Epoch 2129/3000, Train Loss: 0.7129, Time: 1.1770002841949463\n",
      "Epoch 2130/3000, Train Loss: 0.7105, Time: 1.1629328727722168\n",
      "Epoch 2131/3000, Train Loss: 0.7111, Time: 1.1618876457214355\n",
      "Epoch 2132/3000, Train Loss: 0.7140, Time: 1.1825380325317383\n",
      "Epoch 2133/3000, Train Loss: 0.7120, Time: 1.165790319442749\n",
      "Epoch 2134/3000, Train Loss: 0.7088, Time: 1.2040631771087646\n",
      "Epoch 2135/3000, Train Loss: 0.7095, Time: 1.1598868370056152\n",
      "Epoch 2136/3000, Train Loss: 0.7126, Time: 1.1501917839050293\n",
      "Epoch 2137/3000, Train Loss: 0.7174, Time: 1.1471526622772217\n",
      "Epoch 2138/3000, Train Loss: 0.7158, Time: 1.1530485153198242\n",
      "Epoch 2139/3000, Train Loss: 0.7052, Time: 1.132920503616333\n",
      "Epoch 2140/3000, Train Loss: 0.7296, Time: 1.1407132148742676\n",
      "Epoch 2141/3000, Train Loss: 0.7075, Time: 1.3768439292907715\n",
      "Epoch 2142/3000, Train Loss: 0.7119, Time: 1.164862871170044\n",
      "Epoch 2143/3000, Train Loss: 0.7014, Time: 1.1000709533691406\n",
      "Epoch 2144/3000, Train Loss: 0.7144, Time: 1.0830063819885254\n",
      "Epoch 2145/3000, Train Loss: 0.7182, Time: 1.1145730018615723\n",
      "Epoch 2146/3000, Train Loss: 0.7108, Time: 1.186692714691162\n",
      "Epoch 2147/3000, Train Loss: 0.7085, Time: 1.169264554977417\n",
      "Epoch 2148/3000, Train Loss: 0.7143, Time: 1.1286935806274414\n",
      "Epoch 2149/3000, Train Loss: 0.6973, Time: 1.1456644535064697\n",
      "Epoch 2150/3000, Train Loss: 0.7084, Time: 1.1603336334228516\n",
      "Epoch 2151/3000, Train Loss: 0.7133, Time: 1.184351921081543\n",
      "Epoch 2152/3000, Train Loss: 0.7084, Time: 1.1745197772979736\n",
      "Epoch 2153/3000, Train Loss: 0.7108, Time: 1.152895212173462\n",
      "Epoch 2154/3000, Train Loss: 0.7097, Time: 1.123798131942749\n",
      "Epoch 2155/3000, Train Loss: 0.7072, Time: 1.097440481185913\n",
      "Epoch 2156/3000, Train Loss: 0.6991, Time: 1.098998785018921\n",
      "Epoch 2157/3000, Train Loss: 0.7076, Time: 1.1216464042663574\n",
      "Epoch 2158/3000, Train Loss: 0.7065, Time: 1.1279170513153076\n",
      "Epoch 2159/3000, Train Loss: 0.7141, Time: 1.1323132514953613\n",
      "Epoch 2160/3000, Train Loss: 0.7093, Time: 1.1037170886993408\n",
      "Epoch 2161/3000, Train Loss: 0.7145, Time: 1.105224847793579\n",
      "Epoch 2162/3000, Train Loss: 0.7141, Time: 1.106105089187622\n",
      "Epoch 2163/3000, Train Loss: 0.7173, Time: 1.1206293106079102\n",
      "Epoch 2164/3000, Train Loss: 0.7097, Time: 1.1332008838653564\n",
      "Epoch 2165/3000, Train Loss: 0.7115, Time: 1.1491148471832275\n",
      "Epoch 2166/3000, Train Loss: 0.7063, Time: 1.1535708904266357\n",
      "Epoch 2167/3000, Train Loss: 0.7079, Time: 1.1635406017303467\n",
      "Epoch 2168/3000, Train Loss: 0.7104, Time: 1.17258882522583\n",
      "Epoch 2169/3000, Train Loss: 0.7135, Time: 1.2088780403137207\n",
      "Epoch 2170/3000, Train Loss: 0.7140, Time: 1.1492178440093994\n",
      "Epoch 2171/3000, Train Loss: 0.7071, Time: 1.154451608657837\n",
      "Epoch 2172/3000, Train Loss: 0.7206, Time: 1.152651071548462\n",
      "Epoch 2173/3000, Train Loss: 0.7125, Time: 1.1816301345825195\n",
      "Epoch 2174/3000, Train Loss: 0.7170, Time: 1.1384449005126953\n",
      "Epoch 2175/3000, Train Loss: 0.7025, Time: 1.1343750953674316\n",
      "Epoch 2176/3000, Train Loss: 0.7094, Time: 1.102522373199463\n",
      "Epoch 2177/3000, Train Loss: 0.7129, Time: 1.101008415222168\n",
      "Epoch 2178/3000, Train Loss: 0.7130, Time: 1.1847553253173828\n",
      "Epoch 2179/3000, Train Loss: 0.7050, Time: 1.1474862098693848\n",
      "Epoch 2180/3000, Train Loss: 0.7063, Time: 1.1437768936157227\n",
      "Epoch 2181/3000, Train Loss: 0.7102, Time: 1.1660733222961426\n",
      "Epoch 2182/3000, Train Loss: 0.7097, Time: 1.1722195148468018\n",
      "Epoch 2183/3000, Train Loss: 0.7097, Time: 1.1975150108337402\n",
      "Epoch 2184/3000, Train Loss: 0.7089, Time: 1.148792028427124\n",
      "Epoch 2185/3000, Train Loss: 0.7103, Time: 1.150202989578247\n",
      "Epoch 2186/3000, Train Loss: 0.7069, Time: 1.1763200759887695\n",
      "Epoch 2187/3000, Train Loss: 0.7111, Time: 1.1474111080169678\n",
      "Epoch 2188/3000, Train Loss: 0.7104, Time: 1.1499392986297607\n",
      "Epoch 2189/3000, Train Loss: 0.7075, Time: 1.1659245491027832\n",
      "Epoch 2190/3000, Train Loss: 0.7061, Time: 1.1925537586212158\n",
      "Epoch 2191/3000, Train Loss: 0.7028, Time: 1.160834789276123\n",
      "Epoch 2192/3000, Train Loss: 0.7164, Time: 1.1584887504577637\n",
      "Epoch 2193/3000, Train Loss: 0.7079, Time: 1.2726211547851562\n",
      "Epoch 2194/3000, Train Loss: 0.7076, Time: 1.2314085960388184\n",
      "Epoch 2195/3000, Train Loss: 0.7089, Time: 1.1302354335784912\n",
      "Epoch 2196/3000, Train Loss: 0.7042, Time: 1.1339986324310303\n",
      "Epoch 2197/3000, Train Loss: 0.7115, Time: 1.138929843902588\n",
      "Epoch 2198/3000, Train Loss: 0.7147, Time: 1.146535873413086\n",
      "Epoch 2199/3000, Train Loss: 0.7022, Time: 1.1608738899230957\n",
      "Epoch 2200/3000, Train Loss: 0.7019, Time: 1.1546754837036133\n",
      "Epoch 2201/3000, Train Loss: 0.7098, Time: 1.151888132095337\n",
      "Epoch 2202/3000, Train Loss: 0.7094, Time: 1.1527674198150635\n",
      "Epoch 2203/3000, Train Loss: 0.7061, Time: 1.2008514404296875\n",
      "Epoch 2204/3000, Train Loss: 0.7127, Time: 1.175990343093872\n",
      "Epoch 2205/3000, Train Loss: 0.7121, Time: 1.2066543102264404\n",
      "Epoch 2206/3000, Train Loss: 0.7109, Time: 1.1528704166412354\n",
      "Epoch 2207/3000, Train Loss: 0.7149, Time: 1.1456491947174072\n",
      "Epoch 2208/3000, Train Loss: 0.7062, Time: 1.1402056217193604\n",
      "Epoch 2209/3000, Train Loss: 0.7130, Time: 1.098684310913086\n",
      "Epoch 2210/3000, Train Loss: 0.7085, Time: 1.10968017578125\n",
      "Epoch 2211/3000, Train Loss: 0.7140, Time: 1.1388177871704102\n",
      "Epoch 2212/3000, Train Loss: 0.7084, Time: 1.1366024017333984\n",
      "Epoch 2213/3000, Train Loss: 0.7092, Time: 1.1337766647338867\n",
      "Epoch 2214/3000, Train Loss: 0.7054, Time: 1.1294972896575928\n",
      "Epoch 2215/3000, Train Loss: 0.7150, Time: 1.1039175987243652\n",
      "Epoch 2216/3000, Train Loss: 0.7075, Time: 1.1372809410095215\n",
      "Epoch 2217/3000, Train Loss: 0.7101, Time: 1.1105926036834717\n",
      "Epoch 2218/3000, Train Loss: 0.7100, Time: 1.1429541110992432\n",
      "Epoch 2219/3000, Train Loss: 0.7024, Time: 1.1299331188201904\n",
      "Epoch 2220/3000, Train Loss: 0.7001, Time: 1.1770555973052979\n",
      "Epoch 2221/3000, Train Loss: 0.7070, Time: 1.1446418762207031\n",
      "Epoch 2222/3000, Train Loss: 0.7088, Time: 1.1439521312713623\n",
      "Epoch 2223/3000, Train Loss: 0.7134, Time: 1.1145055294036865\n",
      "Epoch 2224/3000, Train Loss: 0.7245, Time: 1.1022734642028809\n",
      "Epoch 2225/3000, Train Loss: 0.7117, Time: 1.132370948791504\n",
      "Epoch 2226/3000, Train Loss: 0.7114, Time: 1.1351063251495361\n",
      "Epoch 2227/3000, Train Loss: 0.7176, Time: 1.128309965133667\n",
      "Epoch 2228/3000, Train Loss: 0.7102, Time: 1.1683845520019531\n",
      "Epoch 2229/3000, Train Loss: 0.7021, Time: 1.107567548751831\n",
      "Epoch 2230/3000, Train Loss: 0.7086, Time: 1.1037929058074951\n",
      "Epoch 2231/3000, Train Loss: 0.7036, Time: 1.1372005939483643\n",
      "Epoch 2232/3000, Train Loss: 0.7033, Time: 1.1661877632141113\n",
      "Epoch 2233/3000, Train Loss: 0.7152, Time: 1.1187069416046143\n",
      "Epoch 2234/3000, Train Loss: 0.7233, Time: 1.111783504486084\n",
      "Epoch 2235/3000, Train Loss: 0.7109, Time: 1.1044948101043701\n",
      "Epoch 2236/3000, Train Loss: 0.7084, Time: 1.099773645401001\n",
      "Epoch 2237/3000, Train Loss: 0.7071, Time: 1.1303379535675049\n",
      "Epoch 2238/3000, Train Loss: 0.7196, Time: 1.0485305786132812\n",
      "Epoch 2239/3000, Train Loss: 0.7095, Time: 1.0569689273834229\n",
      "Epoch 2240/3000, Train Loss: 0.7094, Time: 1.1058337688446045\n",
      "Epoch 2241/3000, Train Loss: 0.7132, Time: 1.1071536540985107\n",
      "Epoch 2242/3000, Train Loss: 0.7104, Time: 1.139296293258667\n",
      "Epoch 2243/3000, Train Loss: 0.7172, Time: 1.1832661628723145\n",
      "Epoch 2244/3000, Train Loss: 0.6999, Time: 1.146615743637085\n",
      "Epoch 2245/3000, Train Loss: 0.7103, Time: 1.1291656494140625\n",
      "Epoch 2246/3000, Train Loss: 0.7113, Time: 1.2765188217163086\n",
      "Epoch 2247/3000, Train Loss: 0.7084, Time: 1.169356346130371\n",
      "Epoch 2248/3000, Train Loss: 0.7109, Time: 1.1674931049346924\n",
      "Epoch 2249/3000, Train Loss: 0.7082, Time: 1.1840176582336426\n",
      "Epoch 2250/3000, Train Loss: 0.6972, Time: 1.1876678466796875\n",
      "Epoch 2251/3000, Train Loss: 0.7079, Time: 1.1572308540344238\n",
      "Epoch 2252/3000, Train Loss: 0.6957, Time: 1.1713597774505615\n",
      "Epoch 2253/3000, Train Loss: 0.7050, Time: 1.1509926319122314\n",
      "Epoch 2254/3000, Train Loss: 0.7093, Time: 1.1297478675842285\n",
      "Epoch 2255/3000, Train Loss: 0.7076, Time: 1.128497838973999\n",
      "Epoch 2256/3000, Train Loss: 0.7000, Time: 1.1127989292144775\n",
      "Epoch 2257/3000, Train Loss: 0.7143, Time: 1.104520559310913\n",
      "Epoch 2258/3000, Train Loss: 0.7142, Time: 1.1019372940063477\n",
      "Epoch 2259/3000, Train Loss: 0.7049, Time: 1.115063190460205\n",
      "Epoch 2260/3000, Train Loss: 0.7079, Time: 1.1584181785583496\n",
      "Epoch 2261/3000, Train Loss: 0.7073, Time: 1.128821611404419\n",
      "Epoch 2262/3000, Train Loss: 0.7017, Time: 1.1535890102386475\n",
      "Epoch 2263/3000, Train Loss: 0.7110, Time: 1.122042179107666\n",
      "Epoch 2264/3000, Train Loss: 0.7036, Time: 1.1083984375\n",
      "Epoch 2265/3000, Train Loss: 0.7106, Time: 1.1724722385406494\n",
      "Epoch 2266/3000, Train Loss: 0.7138, Time: 1.1115176677703857\n",
      "Epoch 2267/3000, Train Loss: 0.7141, Time: 1.1071531772613525\n",
      "Epoch 2268/3000, Train Loss: 0.7126, Time: 1.1082839965820312\n",
      "Epoch 2269/3000, Train Loss: 0.7043, Time: 1.1358139514923096\n",
      "Epoch 2270/3000, Train Loss: 0.7075, Time: 1.1356842517852783\n",
      "Epoch 2271/3000, Train Loss: 0.7092, Time: 1.1330296993255615\n",
      "Epoch 2272/3000, Train Loss: 0.7054, Time: 1.1024153232574463\n",
      "Epoch 2273/3000, Train Loss: 0.7109, Time: 1.1116361618041992\n",
      "Epoch 2274/3000, Train Loss: 0.7123, Time: 1.110863208770752\n",
      "Epoch 2275/3000, Train Loss: 0.7127, Time: 1.1334877014160156\n",
      "Epoch 2276/3000, Train Loss: 0.7059, Time: 1.1705641746520996\n",
      "Epoch 2277/3000, Train Loss: 0.7073, Time: 1.1148552894592285\n",
      "Epoch 2278/3000, Train Loss: 0.7052, Time: 1.112539529800415\n",
      "Epoch 2279/3000, Train Loss: 0.7024, Time: 1.1074957847595215\n",
      "Epoch 2280/3000, Train Loss: 0.7101, Time: 1.1136586666107178\n",
      "Epoch 2281/3000, Train Loss: 0.7113, Time: 1.136033535003662\n",
      "Epoch 2282/3000, Train Loss: 0.7037, Time: 1.1367244720458984\n",
      "Epoch 2283/3000, Train Loss: 0.7019, Time: 1.1141726970672607\n",
      "Epoch 2284/3000, Train Loss: 0.7082, Time: 1.110267162322998\n",
      "Epoch 2285/3000, Train Loss: 0.7103, Time: 1.1089611053466797\n",
      "Epoch 2286/3000, Train Loss: 0.7111, Time: 1.1107380390167236\n",
      "Epoch 2287/3000, Train Loss: 0.7036, Time: 1.131624698638916\n",
      "Epoch 2288/3000, Train Loss: 0.7065, Time: 1.1430094242095947\n",
      "Epoch 2289/3000, Train Loss: 0.7027, Time: 1.116811990737915\n",
      "Epoch 2290/3000, Train Loss: 0.7031, Time: 1.107558250427246\n",
      "Epoch 2291/3000, Train Loss: 0.7024, Time: 1.1067264080047607\n",
      "Epoch 2292/3000, Train Loss: 0.7077, Time: 1.1119863986968994\n",
      "Epoch 2293/3000, Train Loss: 0.7063, Time: 1.1155297756195068\n",
      "Epoch 2294/3000, Train Loss: 0.6994, Time: 1.1553332805633545\n",
      "Epoch 2295/3000, Train Loss: 0.7213, Time: 1.1099309921264648\n",
      "Epoch 2296/3000, Train Loss: 0.7055, Time: 1.1548099517822266\n",
      "Epoch 2297/3000, Train Loss: 0.7098, Time: 1.1088473796844482\n",
      "Epoch 2298/3000, Train Loss: 0.7064, Time: 1.1331257820129395\n",
      "Epoch 2299/3000, Train Loss: 0.7081, Time: 1.1638720035552979\n",
      "Epoch 2300/3000, Train Loss: 0.7048, Time: 1.1990735530853271\n",
      "Epoch 2301/3000, Train Loss: 0.7130, Time: 1.1748299598693848\n",
      "Epoch 2302/3000, Train Loss: 0.7117, Time: 1.1038377285003662\n",
      "Epoch 2303/3000, Train Loss: 0.7061, Time: 1.126791000366211\n",
      "Epoch 2304/3000, Train Loss: 0.7069, Time: 1.0998718738555908\n",
      "Epoch 2305/3000, Train Loss: 0.7033, Time: 1.1023099422454834\n",
      "Epoch 2306/3000, Train Loss: 0.7104, Time: 1.1100969314575195\n",
      "Epoch 2307/3000, Train Loss: 0.7168, Time: 1.1252002716064453\n",
      "Epoch 2308/3000, Train Loss: 0.7038, Time: 1.135584831237793\n",
      "Epoch 2309/3000, Train Loss: 0.7004, Time: 1.1400492191314697\n",
      "Epoch 2310/3000, Train Loss: 0.7013, Time: 1.1339995861053467\n",
      "Epoch 2311/3000, Train Loss: 0.7092, Time: 1.1121957302093506\n",
      "Epoch 2312/3000, Train Loss: 0.7093, Time: 1.1081900596618652\n",
      "Epoch 2313/3000, Train Loss: 0.7034, Time: 1.13547945022583\n",
      "Epoch 2314/3000, Train Loss: 0.7086, Time: 1.147526502609253\n",
      "Epoch 2315/3000, Train Loss: 0.7009, Time: 1.1424789428710938\n",
      "Epoch 2316/3000, Train Loss: 0.7095, Time: 1.135793685913086\n",
      "Epoch 2317/3000, Train Loss: 0.7096, Time: 1.1124036312103271\n",
      "Epoch 2318/3000, Train Loss: 0.7088, Time: 1.11458420753479\n",
      "Epoch 2319/3000, Train Loss: 0.7105, Time: 1.1099555492401123\n",
      "Epoch 2320/3000, Train Loss: 0.7066, Time: 1.1221091747283936\n",
      "Epoch 2321/3000, Train Loss: 0.7110, Time: 1.139620304107666\n",
      "Epoch 2322/3000, Train Loss: 0.7064, Time: 1.186699628829956\n",
      "Epoch 2323/3000, Train Loss: 0.7004, Time: 1.118943452835083\n",
      "Epoch 2324/3000, Train Loss: 0.7064, Time: 1.1094887256622314\n",
      "Epoch 2325/3000, Train Loss: 0.7056, Time: 1.1096949577331543\n",
      "Epoch 2326/3000, Train Loss: 0.7084, Time: 1.105780839920044\n",
      "Epoch 2327/3000, Train Loss: 0.7068, Time: 1.1317875385284424\n",
      "Epoch 2328/3000, Train Loss: 0.7207, Time: 1.1027772426605225\n",
      "Epoch 2329/3000, Train Loss: 0.7043, Time: 1.1360747814178467\n",
      "Epoch 2330/3000, Train Loss: 0.7194, Time: 1.1365811824798584\n",
      "Epoch 2331/3000, Train Loss: 0.7053, Time: 1.1565744876861572\n",
      "Epoch 2332/3000, Train Loss: 0.6997, Time: 1.1246066093444824\n",
      "Epoch 2333/3000, Train Loss: 0.7163, Time: 1.1387555599212646\n",
      "Epoch 2334/3000, Train Loss: 0.7056, Time: 1.0346150398254395\n",
      "Epoch 2335/3000, Train Loss: 0.7021, Time: 1.0685217380523682\n",
      "Epoch 2336/3000, Train Loss: 0.7028, Time: 1.1147408485412598\n",
      "Epoch 2337/3000, Train Loss: 0.7075, Time: 1.147857666015625\n",
      "Epoch 2338/3000, Train Loss: 0.7083, Time: 1.168959140777588\n",
      "Epoch 2339/3000, Train Loss: 0.7090, Time: 1.1095361709594727\n",
      "Epoch 2340/3000, Train Loss: 0.7052, Time: 1.1267180442810059\n",
      "Epoch 2341/3000, Train Loss: 0.7003, Time: 1.1183972358703613\n",
      "Epoch 2342/3000, Train Loss: 0.7057, Time: 1.123138189315796\n",
      "Epoch 2343/3000, Train Loss: 0.7092, Time: 1.1637239456176758\n",
      "Epoch 2344/3000, Train Loss: 0.7076, Time: 1.143537998199463\n",
      "Epoch 2345/3000, Train Loss: 0.6977, Time: 1.1788585186004639\n",
      "Epoch 2346/3000, Train Loss: 0.7118, Time: 1.1570944786071777\n",
      "Epoch 2347/3000, Train Loss: 0.7126, Time: 1.1545279026031494\n",
      "Epoch 2348/3000, Train Loss: 0.7091, Time: 1.150639295578003\n",
      "Epoch 2349/3000, Train Loss: 0.7040, Time: 1.151061773300171\n",
      "Epoch 2350/3000, Train Loss: 0.7059, Time: 1.15895414352417\n",
      "Epoch 2351/3000, Train Loss: 0.7052, Time: 1.1955649852752686\n",
      "Epoch 2352/3000, Train Loss: 0.7104, Time: 1.321338176727295\n",
      "Epoch 2353/3000, Train Loss: 0.7116, Time: 1.1691985130310059\n",
      "Epoch 2354/3000, Train Loss: 0.7081, Time: 1.1522037982940674\n",
      "Epoch 2355/3000, Train Loss: 0.7122, Time: 1.1482741832733154\n",
      "Epoch 2356/3000, Train Loss: 0.7029, Time: 1.181339979171753\n",
      "Epoch 2357/3000, Train Loss: 0.7104, Time: 1.2199983596801758\n",
      "Epoch 2358/3000, Train Loss: 0.7061, Time: 1.1670572757720947\n",
      "Epoch 2359/3000, Train Loss: 0.7002, Time: 1.172295331954956\n",
      "Epoch 2360/3000, Train Loss: 0.6999, Time: 1.1257822513580322\n",
      "Epoch 2361/3000, Train Loss: 0.7044, Time: 1.0816621780395508\n",
      "Epoch 2362/3000, Train Loss: 0.7175, Time: 1.0724892616271973\n",
      "Epoch 2363/3000, Train Loss: 0.7074, Time: 1.1138849258422852\n",
      "Epoch 2364/3000, Train Loss: 0.7059, Time: 1.093052625656128\n",
      "Epoch 2365/3000, Train Loss: 0.7053, Time: 1.1556370258331299\n",
      "Epoch 2366/3000, Train Loss: 0.6992, Time: 1.1521484851837158\n",
      "Epoch 2367/3000, Train Loss: 0.6984, Time: 1.1363131999969482\n",
      "Epoch 2368/3000, Train Loss: 0.7128, Time: 1.1270086765289307\n",
      "Epoch 2369/3000, Train Loss: 0.7031, Time: 1.1250793933868408\n",
      "Epoch 2370/3000, Train Loss: 0.7086, Time: 1.1405179500579834\n",
      "Epoch 2371/3000, Train Loss: 0.7151, Time: 1.1671686172485352\n",
      "Epoch 2372/3000, Train Loss: 0.7066, Time: 1.1449716091156006\n",
      "Epoch 2373/3000, Train Loss: 0.7127, Time: 1.162435531616211\n",
      "Epoch 2374/3000, Train Loss: 0.7003, Time: 1.1784639358520508\n",
      "Epoch 2375/3000, Train Loss: 0.7118, Time: 1.145904302597046\n",
      "Epoch 2376/3000, Train Loss: 0.7050, Time: 1.1627824306488037\n",
      "Epoch 2377/3000, Train Loss: 0.7105, Time: 1.1346089839935303\n",
      "Epoch 2378/3000, Train Loss: 0.7109, Time: 1.1772613525390625\n",
      "Epoch 2379/3000, Train Loss: 0.7087, Time: 1.1504673957824707\n",
      "Epoch 2380/3000, Train Loss: 0.7045, Time: 1.14530611038208\n",
      "Epoch 2381/3000, Train Loss: 0.7051, Time: 1.1353492736816406\n",
      "Epoch 2382/3000, Train Loss: 0.6954, Time: 1.1531651020050049\n",
      "Epoch 2383/3000, Train Loss: 0.7108, Time: 1.1396102905273438\n",
      "Epoch 2384/3000, Train Loss: 0.6999, Time: 1.1394307613372803\n",
      "Epoch 2385/3000, Train Loss: 0.7043, Time: 1.150083303451538\n",
      "Epoch 2386/3000, Train Loss: 0.7377, Time: 1.1449720859527588\n",
      "Epoch 2387/3000, Train Loss: 0.7136, Time: 1.148191213607788\n",
      "Epoch 2388/3000, Train Loss: 0.6985, Time: 1.1419150829315186\n",
      "Epoch 2389/3000, Train Loss: 0.6979, Time: 1.138091802597046\n",
      "Epoch 2390/3000, Train Loss: 0.6988, Time: 1.1447172164916992\n",
      "Epoch 2391/3000, Train Loss: 0.7022, Time: 1.1364448070526123\n",
      "Epoch 2392/3000, Train Loss: 0.7080, Time: 1.1419317722320557\n",
      "Epoch 2393/3000, Train Loss: 0.7085, Time: 1.1933844089508057\n",
      "Epoch 2394/3000, Train Loss: 0.7061, Time: 1.1462748050689697\n",
      "Epoch 2395/3000, Train Loss: 0.7011, Time: 1.1403436660766602\n",
      "Epoch 2396/3000, Train Loss: 0.7081, Time: 1.1333296298980713\n",
      "Epoch 2397/3000, Train Loss: 0.7073, Time: 1.128350019454956\n",
      "Epoch 2398/3000, Train Loss: 0.7059, Time: 1.169266700744629\n",
      "Epoch 2399/3000, Train Loss: 0.7058, Time: 1.137657642364502\n",
      "Epoch 2400/3000, Train Loss: 0.7075, Time: 1.2028923034667969\n",
      "Epoch 2401/3000, Train Loss: 0.7104, Time: 1.1697072982788086\n",
      "Epoch 2402/3000, Train Loss: 0.7049, Time: 1.1422922611236572\n",
      "Epoch 2403/3000, Train Loss: 0.7020, Time: 1.1399860382080078\n",
      "Epoch 2404/3000, Train Loss: 0.7119, Time: 1.2487785816192627\n",
      "Epoch 2405/3000, Train Loss: 0.7024, Time: 1.2232909202575684\n",
      "Epoch 2406/3000, Train Loss: 0.6991, Time: 1.1254818439483643\n",
      "Epoch 2407/3000, Train Loss: 0.7062, Time: 1.1202263832092285\n",
      "Epoch 2408/3000, Train Loss: 0.7039, Time: 1.1705811023712158\n",
      "Epoch 2409/3000, Train Loss: 0.7076, Time: 1.1563854217529297\n",
      "Epoch 2410/3000, Train Loss: 0.7104, Time: 1.1291289329528809\n",
      "Epoch 2411/3000, Train Loss: 0.6960, Time: 1.0893869400024414\n",
      "Epoch 2412/3000, Train Loss: 0.6994, Time: 1.0846190452575684\n",
      "Epoch 2413/3000, Train Loss: 0.7116, Time: 1.1538300514221191\n",
      "Epoch 2414/3000, Train Loss: 0.7159, Time: 1.177847146987915\n",
      "Epoch 2415/3000, Train Loss: 0.7022, Time: 1.1744961738586426\n",
      "Epoch 2416/3000, Train Loss: 0.7100, Time: 1.1445481777191162\n",
      "Epoch 2417/3000, Train Loss: 0.7036, Time: 1.1250731945037842\n",
      "Epoch 2418/3000, Train Loss: 0.7005, Time: 1.1485264301300049\n",
      "Epoch 2419/3000, Train Loss: 0.7095, Time: 1.1634738445281982\n",
      "Epoch 2420/3000, Train Loss: 0.7077, Time: 1.161078929901123\n",
      "Epoch 2421/3000, Train Loss: 0.7097, Time: 1.2015655040740967\n",
      "Epoch 2422/3000, Train Loss: 0.7050, Time: 1.1784594058990479\n",
      "Epoch 2423/3000, Train Loss: 0.7022, Time: 1.1708779335021973\n",
      "Epoch 2424/3000, Train Loss: 0.7057, Time: 1.1534137725830078\n",
      "Epoch 2425/3000, Train Loss: 0.6955, Time: 1.1926281452178955\n",
      "Epoch 2426/3000, Train Loss: 0.7021, Time: 1.1670739650726318\n",
      "Epoch 2427/3000, Train Loss: 0.7027, Time: 1.1921520233154297\n",
      "Epoch 2428/3000, Train Loss: 0.7035, Time: 1.1990981101989746\n",
      "Epoch 2429/3000, Train Loss: 0.7081, Time: 1.1213228702545166\n",
      "Epoch 2430/3000, Train Loss: 0.7039, Time: 1.1441395282745361\n",
      "Epoch 2431/3000, Train Loss: 0.7047, Time: 1.1638097763061523\n",
      "Epoch 2432/3000, Train Loss: 0.7072, Time: 1.166107177734375\n",
      "Epoch 2433/3000, Train Loss: 0.7027, Time: 1.1629579067230225\n",
      "Epoch 2434/3000, Train Loss: 0.7066, Time: 1.1686789989471436\n",
      "Epoch 2435/3000, Train Loss: 0.7072, Time: 1.1871936321258545\n",
      "Epoch 2436/3000, Train Loss: 0.7025, Time: 1.1891875267028809\n",
      "Epoch 2437/3000, Train Loss: 0.7045, Time: 1.1655707359313965\n",
      "Epoch 2438/3000, Train Loss: 0.7081, Time: 1.1581511497497559\n",
      "Epoch 2439/3000, Train Loss: 0.7011, Time: 1.1407866477966309\n",
      "Epoch 2440/3000, Train Loss: 0.7057, Time: 1.126654863357544\n",
      "Epoch 2441/3000, Train Loss: 0.7110, Time: 1.1792821884155273\n",
      "Epoch 2442/3000, Train Loss: 0.7006, Time: 1.140979528427124\n",
      "Epoch 2443/3000, Train Loss: 0.7056, Time: 1.139958381652832\n",
      "Epoch 2444/3000, Train Loss: 0.6996, Time: 1.113269329071045\n",
      "Epoch 2445/3000, Train Loss: 0.7048, Time: 1.1066386699676514\n",
      "Epoch 2446/3000, Train Loss: 0.7057, Time: 1.1064975261688232\n",
      "Epoch 2447/3000, Train Loss: 0.7176, Time: 1.1140882968902588\n",
      "Epoch 2448/3000, Train Loss: 0.6975, Time: 1.1494324207305908\n",
      "Epoch 2449/3000, Train Loss: 0.7105, Time: 1.1360478401184082\n",
      "Epoch 2450/3000, Train Loss: 0.7037, Time: 1.1430201530456543\n",
      "Epoch 2451/3000, Train Loss: 0.7058, Time: 1.1376142501831055\n",
      "Epoch 2452/3000, Train Loss: 0.7048, Time: 1.1183099746704102\n",
      "Epoch 2453/3000, Train Loss: 0.7012, Time: 1.106555461883545\n",
      "Epoch 2454/3000, Train Loss: 0.7082, Time: 1.1106178760528564\n",
      "Epoch 2455/3000, Train Loss: 0.7090, Time: 1.131659746170044\n",
      "Epoch 2456/3000, Train Loss: 0.7008, Time: 1.257145643234253\n",
      "Epoch 2457/3000, Train Loss: 0.7042, Time: 1.2105035781860352\n",
      "Epoch 2458/3000, Train Loss: 0.7102, Time: 1.1102590560913086\n",
      "Epoch 2459/3000, Train Loss: 0.6959, Time: 1.111680269241333\n",
      "Epoch 2460/3000, Train Loss: 0.7047, Time: 1.1148335933685303\n",
      "Epoch 2461/3000, Train Loss: 0.7002, Time: 1.1307520866394043\n",
      "Epoch 2462/3000, Train Loss: 0.7068, Time: 1.117708444595337\n",
      "Epoch 2463/3000, Train Loss: 0.7001, Time: 1.1083505153656006\n",
      "Epoch 2464/3000, Train Loss: 0.7052, Time: 1.101491928100586\n",
      "Epoch 2465/3000, Train Loss: 0.7014, Time: 1.1493840217590332\n",
      "Epoch 2466/3000, Train Loss: 0.7053, Time: 1.1227765083312988\n",
      "Epoch 2467/3000, Train Loss: 0.6965, Time: 1.1451349258422852\n",
      "Epoch 2468/3000, Train Loss: 0.7053, Time: 1.1355535984039307\n",
      "Epoch 2469/3000, Train Loss: 0.7020, Time: 1.129258155822754\n",
      "Epoch 2470/3000, Train Loss: 0.7063, Time: 1.1334855556488037\n",
      "Epoch 2471/3000, Train Loss: 0.7036, Time: 1.1602892875671387\n",
      "Epoch 2472/3000, Train Loss: 0.7124, Time: 1.1316041946411133\n",
      "Epoch 2473/3000, Train Loss: 0.7106, Time: 1.100924015045166\n",
      "Epoch 2474/3000, Train Loss: 0.7058, Time: 1.1227469444274902\n",
      "Epoch 2475/3000, Train Loss: 0.7021, Time: 1.1405069828033447\n",
      "Epoch 2476/3000, Train Loss: 0.7021, Time: 1.1213288307189941\n",
      "Epoch 2477/3000, Train Loss: 0.7061, Time: 1.101043701171875\n",
      "Epoch 2478/3000, Train Loss: 0.7035, Time: 1.1041686534881592\n",
      "Epoch 2479/3000, Train Loss: 0.7043, Time: 1.1040880680084229\n",
      "Epoch 2480/3000, Train Loss: 0.7006, Time: 1.1304113864898682\n",
      "Epoch 2481/3000, Train Loss: 0.7065, Time: 1.1457855701446533\n",
      "Epoch 2482/3000, Train Loss: 0.6941, Time: 1.1698315143585205\n",
      "Epoch 2483/3000, Train Loss: 0.7029, Time: 1.164050817489624\n",
      "Epoch 2484/3000, Train Loss: 0.7021, Time: 1.1380832195281982\n",
      "Epoch 2485/3000, Train Loss: 0.7022, Time: 1.102945327758789\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from modules.finetune_hyperparameters import KDD_Finetune_Hyperparameters\n",
    "from modules.kdd_model import kdd_model4finetune_test, kdd_model4pretrain, kdd_model4finetune, kdd_model4pretrain_test\n",
    "from modules.pretrain_hyperparameters import KDD_Pretrain_Hyperparameters, KDD_NoMask_Pretrain_Hyperparameters\n",
    "from utils.finetune import finetune_kdd_model, eval_finetune_kdd_model\n",
    "from utils.load_data_from_file import load_mixed_data, prepare_mixed_data_loader, load_one_out_data, \\\n",
    "    prepare_one_out_data_loader, prepare_no_mask_one_out_data_loader\n",
    "from utils.pretrain import pretrain_kdd_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the config from JSON file first\n",
    "    with open(\"utils/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "    print(config)\n",
    "\n",
    "    # config[\"general\"][\"pretrain_model\"] = \"results/Desktop/kdd_model/One_out/convolution/pretrain/window_size_30sec/feat_dim_2/kernelsize_30_stride_15_dilation_1_padding_0/freeze_False_epoch_2000_lr_0.001_d_hidden_16_d_ff_128_n_heads_8_n_layer_8_pos_encode_learnable_activation_gelu_norm_LayerNorm\"\n",
    "\n",
    "    config[\"general\"][\"test_set\"] = \"Desktop\" # Reading or Desktop or CosSin\n",
    "\n",
    "    config[\"general\"][\"window_size\"] = 600\n",
    "    config[\"general\"][\"overlap\"] = 0.8\n",
    "    config[\"general\"][\"batch_size\"] = 128\n",
    "    config[\"kdd_pretrain\"][\"epoch\"] = 3000\n",
    "    config[\"kdd_finetune\"][\"epoch\"] = 6000\n",
    "\n",
    "    config[\"kdd_model\"][\"d_hidden\"] = 16\n",
    "    config[\"kdd_model\"][\"d_ff\"] = 128\n",
    "    config[\"kdd_model\"][\"n_heads\"] = 8\n",
    "    config[\"kdd_model\"][\"n_layers\"] = 8\n",
    "    config[\"kdd_model\"][\"dropout\"] = 0.1\n",
    "    \n",
    "    config[\"kdd_model\"][\"pos_encoding\"] = \"fixed\"\n",
    "    config[\"kdd_model\"][\"activation\"] = \"relu\"\n",
    "    config[\"kdd_model\"][\"norm\"] = \"LayerNorm\"\n",
    "    config[\"kdd_model\"][\"projection\"] = \"convolution\"\n",
    "    config[\"general\"][\"stack_conv\"] = False\n",
    "    # config[\"general\"][\"freeze\"] = True\n",
    "\n",
    "    # First load the data into dataloader according to chosen test_mode: Mixed or One_out\n",
    "    if config[\"general\"][\"test_mode\"] == \"Mixed\":\n",
    "        data, labels, encoder = load_mixed_data(window_size=config[\"general\"][\"window_size\"],\n",
    "                                                overlap=config[\"general\"][\"overlap\"],\n",
    "                                                data_set=config[\"general\"][\"test_set\"])\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        labels_dim = labels.shape\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}, the labels_dim is {labels_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_mixed_data_loader\n",
    "                               (data, labels, batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"]))\n",
    "\n",
    "    elif config[\"general\"][\"test_mode\"] == \"One_out\":\n",
    "        train_data, train_labels, test_data, test_labels, encoder = (load_one_out_data\n",
    "                                                                     (window_size=config[\"general\"][\"window_size\"],\n",
    "                                                                      overlap=config[\"general\"][\"overlap\"],\n",
    "                                                                      data_set=config[\"general\"][\"test_set\"]))\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = train_data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_no_mask_one_out_data_loader\n",
    "                               (train_data, train_labels, test_data, test_labels,\n",
    "                                batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"],\n",
    "                                labeled_percentage=0.2))\n",
    "    else:\n",
    "        print(\"Either Mixed / One_out\")\n",
    "        sys.exit()\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # If the pretrain_model path is not provided, start with pretraining the model\n",
    "    if config[\"general\"][\"pretrain_model\"] is None:\n",
    "        hyperparameters = KDD_NoMask_Pretrain_Hyperparameters(config)\n",
    "        model = kdd_model4pretrain_test(config, feat_dim)\n",
    "        loss = hyperparameters.loss\n",
    "        optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                              weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "        pretrain_kdd_model(model, loss, optimizer, eyegaze_data_loader[0], config)\n",
    "\n",
    "    # If the pretrain_model path is provided, meaning that there is already a pretrained model, then directly finetune\n",
    "    # After pretrain, finetune will be performed automatically, because the pretrain_model will be filled\n",
    "    hyperparameters = KDD_Finetune_Hyperparameters(config)\n",
    "    model = kdd_model4finetune_test(config, feat_dim, num_classes)\n",
    "    loss = hyperparameters.loss\n",
    "    optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                          weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "    # eyegaze_data_loader[1] is the training set, and eyegaze_data_loader[2] is the validation set\n",
    "    finetune_kdd_model(model, loss, optimizer, eyegaze_data_loader[1], eyegaze_data_loader[2], config)\n",
    "\n",
    "    eval_finetune_kdd_model(model, eyegaze_data_loader[3], config, encoder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
