{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from modules.finetune_hyperparameters import KDD_Finetune_Hyperparameters\n",
    "from modules.kdd_model import kdd_model4finetune_test, kdd_model4pretrain, kdd_model4finetune, kdd_model4pretrain_test\n",
    "from modules.pretrain_hyperparameters import KDD_Pretrain_Hyperparameters, KDD_NoMask_Pretrain_Hyperparameters\n",
    "from utils.finetune import finetune_kdd_model, eval_finetune_kdd_model\n",
    "from utils.load_data_from_file import load_mixed_data, prepare_mixed_data_loader, load_one_out_data, \\\n",
    "    prepare_one_out_data_loader, prepare_no_mask_one_out_data_loader\n",
    "from utils.pretrain import pretrain_kdd_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the config from JSON file first\n",
    "    with open(\"utils/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "    print(config)\n",
    "\n",
    "    # config[\"general\"][\"pretrain_model\"] = \"results/Desktop/kdd_model/One_out/convolution/pretrain/window_size_30sec/feat_dim_2/kernelsize_30_stride_15_dilation_1_padding_0/freeze_False_epoch_2000_lr_0.001_d_hidden_16_d_ff_128_n_heads_8_n_layer_8_pos_encode_learnable_activation_gelu_norm_LayerNorm\"\n",
    "\n",
    "    config[\"general\"][\"test_set\"] = \"Desktop\" # Reading or Desktop or CosSin\n",
    "\n",
    "    config[\"general\"][\"window_size\"] = 300\n",
    "    config[\"general\"][\"overlap\"] = 0.8\n",
    "    config[\"general\"][\"batch_size\"] = 128\n",
    "    config[\"kdd_pretrain\"][\"epoch\"] = 3000\n",
    "    config[\"kdd_finetune\"][\"epoch\"] = 6000\n",
    "\n",
    "    config[\"kdd_model\"][\"d_hidden\"] = 16\n",
    "    config[\"kdd_model\"][\"d_ff\"] = 128\n",
    "    config[\"kdd_model\"][\"n_heads\"] = 8\n",
    "    config[\"kdd_model\"][\"n_layers\"] = 8\n",
    "    config[\"kdd_model\"][\"dropout\"] = 0.1\n",
    "    \n",
    "    config[\"kdd_model\"][\"pos_encoding\"] = \"learnable\"\n",
    "    config[\"kdd_model\"][\"activation\"] = \"gelu\"\n",
    "    config[\"kdd_model\"][\"norm\"] = \"LayerNorm\"\n",
    "    config[\"kdd_model\"][\"projection\"] = \"convolution\"\n",
    "    config[\"general\"][\"stack_conv\"] = False\n",
    "    # config[\"general\"][\"freeze\"] = True\n",
    "\n",
    "    # First load the data into dataloader according to chosen test_mode: Mixed or One_out\n",
    "    if config[\"general\"][\"test_mode\"] == \"Mixed\":\n",
    "        data, labels, encoder = load_mixed_data(window_size=config[\"general\"][\"window_size\"],\n",
    "                                                overlap=config[\"general\"][\"overlap\"],\n",
    "                                                data_set=config[\"general\"][\"test_set\"])\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        labels_dim = labels.shape\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}, the labels_dim is {labels_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_mixed_data_loader\n",
    "                               (data, labels, batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"]))\n",
    "\n",
    "    elif config[\"general\"][\"test_mode\"] == \"One_out\":\n",
    "        train_data, train_labels, test_data, test_labels, encoder = (load_one_out_data\n",
    "                                                                     (window_size=config[\"general\"][\"window_size\"],\n",
    "                                                                      overlap=config[\"general\"][\"overlap\"],\n",
    "                                                                      data_set=config[\"general\"][\"test_set\"]))\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = train_data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_no_mask_one_out_data_loader\n",
    "                               (train_data, train_labels, test_data, test_labels,\n",
    "                                batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"],\n",
    "                                labeled_percentage=0.2))\n",
    "    else:\n",
    "        print(\"Either Mixed / One_out\")\n",
    "        sys.exit()\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # If the pretrain_model path is not provided, start with pretraining the model\n",
    "    if config[\"general\"][\"pretrain_model\"] is None:\n",
    "        hyperparameters = KDD_NoMask_Pretrain_Hyperparameters(config)\n",
    "        model = kdd_model4pretrain_test(config, feat_dim)\n",
    "        loss = hyperparameters.loss\n",
    "        optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                              weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "        pretrain_kdd_model(model, loss, optimizer, eyegaze_data_loader[0], config)\n",
    "\n",
    "    # If the pretrain_model path is provided, meaning that there is already a pretrained model, then directly finetune\n",
    "    # After pretrain, finetune will be performed automatically, because the pretrain_model will be filled\n",
    "    hyperparameters = KDD_Finetune_Hyperparameters(config)\n",
    "    model = kdd_model4finetune_test(config, feat_dim, num_classes)\n",
    "    loss = hyperparameters.loss\n",
    "    optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                          weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "    # eyegaze_data_loader[1] is the training set, and eyegaze_data_loader[2] is the validation set\n",
    "    finetune_kdd_model(model, loss, optimizer, eyegaze_data_loader[1], eyegaze_data_loader[2], config)\n",
    "\n",
    "    eval_finetune_kdd_model(model, eyegaze_data_loader[3], config, encoder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
