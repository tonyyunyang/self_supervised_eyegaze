{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': {'test_set': 'Reading', 'test_mode': 'One_out', 'window_size': 900, 'overlap': 0.944, 'feat_dim': None, 'pretrain_model': None, 'finetune_model': None, 'batch_size': 128, 'freeze': False, 'stack_conv': False}, 'kdd_pretrain': {'epoch': 5, 'lr': 0.001, 'optimizer': 'RAdam', 'weight_decay': None, 'harden': False}, 'kdd_finetune': {'epoch': 5, 'lr': 0.0002, 'optimizer': 'RAdam', 'weight_decay': None}, 'limu_pretrain': {'epoch': 10, 'lr': 0.001, 'optimizer': 'Adam', 'weight_decay': None, 'harden': False}, 'limu_finetune': {'epoch': 10, 'lr': 0.001, 'optimizer': 'Adam', 'weight_decay': None, 'classifier': 'gru'}, 'limu_mask': {'mask_ratio': 0.15, 'mask_alpha': 6, 'max_gram': 10, 'mask_prob': 0.8, 'replace_prob': 0.0}, 'kdd_model': {'d_hidden': 64, 'd_ff': 256, 'n_heads': 8, 'n_layers': 3, 'dropout': 0.1, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'projection': 'convolution'}, 'limu_model': {'d_hidden': 24, 'd_ff': 72, 'n_heads': 4, 'n_layers': 4, 'emb_norm': False}, 'limu_classifier': {'gru_v1': {'rnn_layers': [2, 1], 'rnn_io': [[72, 20], [20, 10]], 'linear_io': [[10, 6]], 'activ': False, 'dropout': False}}, 'conv1d_5sec': {'first': {'kernel_size': 20, 'stride': 10, 'dilation': 1, 'padding': 0}}, 'conv1d_10sec': {'first': {'kernel_size': 30, 'stride': 15, 'dilation': 1, 'padding': 0}}, 'conv1d_15sec': {'first': {'kernel_size': 40, 'stride': 20, 'dilation': 1, 'padding': 0}}, 'conv1d_20sec': {'first': {'kernel_size': 30, 'stride': 15, 'dilation': 1, 'padding': 0}}, 'conv1d_30sec': {'first': {'kernel_size': 30, 'stride': 15, 'dilation': 1, 'padding': 0}}, 'conv1d_5sec_stack': {'first': {'kernel_size': 10, 'stride': 1, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 20, 'stride': 1, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 20, 'stride': 10, 'dilation': 1, 'padding': 0}}, 'conv1d_10sec_stack': {'first': {'kernel_size': 30, 'stride': 15, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 5, 'stride': 1, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 5, 'stride': 1, 'dilation': 1, 'padding': 0}}, 'conv1d_15sec_stack': {'first': {'kernel_size': 10, 'stride': 2, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 10, 'stride': 4, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 10, 'stride': 6, 'dilation': 1, 'padding': 0}}, 'conv1d_30sec_stack': {'first': {'kernel_size': 20, 'stride': 1, 'dilation': 1, 'padding': 0}, 'second': {'kernel_size': 40, 'stride': 1, 'dilation': 1, 'padding': 0}, 'thrid': {'kernel_size': 60, 'stride': 30, 'dilation': 1, 'padding': 0}}}\n",
      "The step size of each sample is 119, this is determined via the overlap\n",
      "Class: BROWSE -> Encoded Value: 0\n",
      "Class: PLAY -> Encoded Value: 1\n",
      "Class: READ -> Encoded Value: 2\n",
      "Class: SEARCH -> Encoded Value: 3\n",
      "Class: WATCH -> Encoded Value: 4\n",
      "Class: WRITE -> Encoded Value: 5\n",
      "The number of classes is 6, the feat_dim is 2\n",
      "Pretrain samples amount: 2982\n",
      "Finetune training samples amount: 89\n",
      "Finetune validation samples amount: 39\n",
      "Final testing samples amount: 298\n",
      "Label 4: 8 samples\n",
      "Label 0: 13 samples\n",
      "Label 2: 16 samples\n",
      "Label 1: 15 samples\n",
      "Label 3: 14 samples\n",
      "Label 5: 23 samples\n",
      "Model:\n",
      "TSTransformerEncoderTest(\n",
      "  (project_inp): Conv1d(2, 16, kernel_size=(30,), stride=(15,))\n",
      "  (pos_enc): LearnablePositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=16, bias=True)\n",
      "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): ConvTransposeLinear(\n",
      "    (conv_transpose): ConvTranspose1d(16, 2, kernel_size=(30,), stride=(15,))\n",
      "    (linear): Linear(in_features=1200, out_features=1200, bias=True)\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Total number of parameters: 1486898\n",
      "Trainable parameters: 1486898\n",
      "=============================================================\n",
      "=====================Training via cuda===================\n",
      "=============================================================\n",
      "Epoch 1/3000, Train Loss: 254.3157, Time: 1.2614765167236328\n",
      "Epoch 2/3000, Train Loss: 50.9443, Time: 1.088773488998413\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb#W0sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     eval_finetune_kdd_model(model, eyegaze_data_loader[\u001b[39m3\u001b[39m], config, encoder)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb#W0sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb#W0sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     main()\n",
      "\u001b[1;32m/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb#W0sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     loss \u001b[39m=\u001b[39m hyperparameters\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb#W0sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     optimizer \u001b[39m=\u001b[39m hyperparameters\u001b[39m.\u001b[39moptimizer(model\u001b[39m.\u001b[39mparameters(), hyperparameters\u001b[39m.\u001b[39mlr,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb#W0sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m                                           weight_decay\u001b[39m=\u001b[39mhyperparameters\u001b[39m.\u001b[39mweight_decay)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb#W0sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     pretrain_kdd_model(model, loss, optimizer, eyegaze_data_loader[\u001b[39m0\u001b[39;49m], config)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb#W0sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m# If the pretrain_model path is provided, meaning that there is already a pretrained model, then directly finetune\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb#W0sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# After pretrain, finetune will be performed automatically, because the pretrain_model will be filled\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tonyyang/Desktop/self_supervised_eyegaze/kdd_no_mask_convTest_main.ipynb#W0sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m hyperparameters \u001b[39m=\u001b[39m KDD_Finetune_Hyperparameters(config)\n",
      "File \u001b[0;32m~/Desktop/self_supervised_eyegaze/utils/pretrain.py:108\u001b[0m, in \u001b[0;36mpretrain_kdd_model\u001b[0;34m(model, loss, optimizer, pretrain_data, config)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, config[\u001b[39m\"\u001b[39m\u001b[39mkdd_pretrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    106\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 108\u001b[0m     train_loss \u001b[39m=\u001b[39m pretrain_kdd_epoch(model, loss, optimizer, pretrain_data, config, device, epoch, l2_reg\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    109\u001b[0m     train_loss_list\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m    111\u001b[0m     \u001b[39m# Save the model if it has the best loss so far\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/self_supervised_eyegaze/utils/pretrain.py:170\u001b[0m, in \u001b[0;36mpretrain_kdd_epoch\u001b[0;34m(model, loss, optimizer, pretrain_data, config, device, epoch, l2_reg)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    169\u001b[0m     active_elements \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(compute_loss)\n\u001b[0;32m--> 170\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39;49mitem()  \u001b[39m# add total loss of batch\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m==\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mkdd_pretrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    173\u001b[0m     np\u001b[39m.\u001b[39msavetxt(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mconfig[\u001b[39m'\u001b[39m\u001b[39mgeneral\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mpretrain_model\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m/pred.txt\u001b[39m\u001b[39m\"\u001b[39m, predictions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from modules.finetune_hyperparameters import KDD_Finetune_Hyperparameters\n",
    "from modules.kdd_model import kdd_model4finetune_test, kdd_model4pretrain, kdd_model4finetune, kdd_model4pretrain_test\n",
    "from modules.pretrain_hyperparameters import KDD_Pretrain_Hyperparameters, KDD_NoMask_Pretrain_Hyperparameters\n",
    "from utils.finetune import finetune_kdd_model, eval_finetune_kdd_model\n",
    "from utils.load_data_from_file import load_mixed_data, prepare_mixed_data_loader, load_one_out_data, \\\n",
    "    prepare_one_out_data_loader, prepare_no_mask_one_out_data_loader\n",
    "from utils.pretrain import pretrain_kdd_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the config from JSON file first\n",
    "    with open(\"utils/config.json\", \"r\") as file:\n",
    "        config = json.load(file)\n",
    "    print(config)\n",
    "\n",
    "    # config[\"general\"][\"pretrain_model\"] = \"results/Desktop/kdd_model/One_out/convolution/pretrain/window_size_20sec/feat_dim_2/kernelsize_30_stride_15_dilation_1_padding_0/freeze_False_epoch_5000_lr_0.001_d_hidden_16_d_ff_128_n_heads_8_n_layer_8_pos_encode_fixed_activation_relu_norm_LayerNorm\"\n",
    "\n",
    "    config[\"general\"][\"test_set\"] = \"Desktop\" # Reading or Desktop or CosSin\n",
    "\n",
    "    config[\"general\"][\"window_size\"] = 600\n",
    "    config[\"general\"][\"overlap\"] = 0.8\n",
    "    config[\"general\"][\"batch_size\"] = 128\n",
    "    config[\"kdd_pretrain\"][\"epoch\"] = 3000\n",
    "    config[\"kdd_finetune\"][\"epoch\"] = 6000\n",
    "\n",
    "    config[\"kdd_model\"][\"d_hidden\"] = 16\n",
    "    config[\"kdd_model\"][\"d_ff\"] = 128\n",
    "    config[\"kdd_model\"][\"n_heads\"] = 8\n",
    "    config[\"kdd_model\"][\"n_layers\"] = 8\n",
    "    config[\"kdd_model\"][\"dropout\"] = 0.1\n",
    "    \n",
    "    config[\"kdd_model\"][\"pos_encoding\"] = \"learnable\"\n",
    "    config[\"kdd_model\"][\"activation\"] = \"gelu\"\n",
    "    config[\"kdd_model\"][\"norm\"] = \"LayerNorm\"\n",
    "    config[\"kdd_model\"][\"projection\"] = \"convolution\"\n",
    "    config[\"general\"][\"stack_conv\"] = False\n",
    "    # config[\"general\"][\"freeze\"] = True\n",
    "\n",
    "    # First load the data into dataloader according to chosen test_mode: Mixed or One_out\n",
    "    if config[\"general\"][\"test_mode\"] == \"Mixed\":\n",
    "        data, labels, encoder = load_mixed_data(window_size=config[\"general\"][\"window_size\"],\n",
    "                                                overlap=config[\"general\"][\"overlap\"],\n",
    "                                                data_set=config[\"general\"][\"test_set\"])\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        labels_dim = labels.shape\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}, the labels_dim is {labels_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_mixed_data_loader\n",
    "                               (data, labels, batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"]))\n",
    "\n",
    "    elif config[\"general\"][\"test_mode\"] == \"One_out\":\n",
    "        train_data, train_labels, test_data, test_labels, encoder = (load_one_out_data\n",
    "                                                                     (window_size=config[\"general\"][\"window_size\"],\n",
    "                                                                      overlap=config[\"general\"][\"overlap\"],\n",
    "                                                                      data_set=config[\"general\"][\"test_set\"]))\n",
    "\n",
    "        num_classes = len(encoder.classes_)\n",
    "        feat_dim = train_data[0].shape[1]\n",
    "        config[\"general\"][\"feat_dim\"] = feat_dim\n",
    "        print(f\"The number of classes is {num_classes}, the feat_dim is {feat_dim}\")\n",
    "\n",
    "        eyegaze_data_loader = (prepare_no_mask_one_out_data_loader\n",
    "                               (train_data, train_labels, test_data, test_labels,\n",
    "                                batch_size=config[\"general\"][\"batch_size\"],\n",
    "                                max_len=config[\"general\"][\"window_size\"],\n",
    "                                labeled_percentage=0.3))\n",
    "    else:\n",
    "        print(\"Either Mixed / One_out\")\n",
    "        sys.exit()\n",
    "\n",
    "    # ==================================================================================================================\n",
    "    # If the pretrain_model path is not provided, start with pretraining the model\n",
    "    if config[\"general\"][\"pretrain_model\"] is None:\n",
    "        hyperparameters = KDD_NoMask_Pretrain_Hyperparameters(config)\n",
    "        model = kdd_model4pretrain_test(config, feat_dim)\n",
    "        loss = hyperparameters.loss\n",
    "        optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                              weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "        pretrain_kdd_model(model, loss, optimizer, eyegaze_data_loader[0], config)\n",
    "\n",
    "    # If the pretrain_model path is provided, meaning that there is already a pretrained model, then directly finetune\n",
    "    # After pretrain, finetune will be performed automatically, because the pretrain_model will be filled\n",
    "    hyperparameters = KDD_Finetune_Hyperparameters(config)\n",
    "    model = kdd_model4finetune_test(config, feat_dim, num_classes)\n",
    "    loss = hyperparameters.loss\n",
    "    optimizer = hyperparameters.optimizer(model.parameters(), hyperparameters.lr,\n",
    "                                          weight_decay=hyperparameters.weight_decay)\n",
    "\n",
    "    # eyegaze_data_loader[1] is the training set, and eyegaze_data_loader[2] is the validation set\n",
    "    finetune_kdd_model(model, loss, optimizer, eyegaze_data_loader[1], eyegaze_data_loader[2], config)\n",
    "\n",
    "    eval_finetune_kdd_model(model, eyegaze_data_loader[3], config, encoder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
